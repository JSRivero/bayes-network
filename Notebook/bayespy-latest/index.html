

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>BayesPy v0.5.10+6.g1d2e2c2 Documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  

  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="None" href="index.html#document-index"/> 

  
  <script src="_static/js/modernizr.min.js"></script>


<!-- RTD Extra Head -->

<!-- 
Always link to the latest version, as canonical.
http://docs.readthedocs.org/en/latest/canonical.html
-->
<link rel="canonical" href="http://www.bayespy.org/" />

<link rel="stylesheet" href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" type="text/css" />

<script type="text/javascript" src="_static/readthedocs-data.js"></script>

<!-- Add page-specific data, which must exist in the page js, not global -->
<script type="text/javascript">
READTHEDOCS_DATA['page'] = 'index'
</script>

<script type="text/javascript" src="_static/readthedocs-dynamic-include.js"></script>

<!-- end RTD <extrahead> --></head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html#document-index" class="icon icon-home"> BayesPy
          

          
          </a>

          
            
            
            
              <div class="version">
                latest
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#project-information">Project information</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#similar-projects">Similar projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#contributors">Contributors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#version-history">Version history</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-user_guide/user_guide">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_guide/install">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_guide/quickstart">Quick start guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_guide/modelconstruct">Constructing the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_guide/inference">Performing inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_guide/plot">Examining the results</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_guide/advanced">Advanced topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-examples/examples">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/multinomial">Multinomial distribution: bags of marbles</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/regression">Linear regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/gmm">Gaussian mixture model</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/bmm">Bernoulli mixture model</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/hmm">Hidden Markov model</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/pca">Principal component analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/lssm">Linear state-space model</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-examples/lda">Latent Dirichlet allocation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-dev_guide/dev_guide">Developer guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_guide/workflow">Workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_guide/vmp">Variational message passing</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_guide/engine">Implementing inference engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_guide/writingnodes">Implementing nodes</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-user_api/user_api">User API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_api/generated/bayespy.nodes">bayespy.nodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_api/generated/bayespy.inference">bayespy.inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-user_api/generated/bayespy.plot">bayespy.plot</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-dev_api/dev_api">Developer API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_api/nodes">Developer nodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_api/moments">Moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_api/distributions">Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#document-dev_api/utils">Utility functions</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html#document-index">BayesPy</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html#document-index">Docs</a> &raquo;</li>
        
      <li>BayesPy v0.5.10+6.g1d2e2c2 Documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/bayespy/bayespy/blob/develop/doc/source/index.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="bayespy-bayesian-python">
<h1>BayesPy &#8211; Bayesian Python<a class="headerlink" href="#bayespy-bayesian-python" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
<span id="document-intro"></span><div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>BayesPy provides tools for Bayesian inference with Python.  The user
constructs a model as a Bayesian network, observes data and runs
posterior inference.  The goal is to provide a tool which is
efficient, flexible and extendable enough for expert use but also
accessible for more casual users.</p>
<p>Currently, only variational Bayesian inference for
conjugate-exponential family (variational message passing) has been
implemented.  Future work includes variational approximations for
other types of distributions and possibly other approximate inference
methods such as expectation propagation, Laplace approximations,
Markov chain Monte Carlo (MCMC) and other methods. Contributions are
welcome.</p>
<div class="section" id="project-information">
<h3>Project information<a class="headerlink" href="#project-information" title="Permalink to this headline">¶</a></h3>
<p>Copyright (C) 2011-2017 Jaakko Luttinen and other contributors (see below)</p>
<p>BayesPy including the documentation is licensed under the MIT License. See
LICENSE file for a text of the license or visit
<a class="reference external" href="http://opensource.org/licenses/MIT">http://opensource.org/licenses/MIT</a>.</p>
<table border="1" class="docutils">
<colgroup>
<col width="24%" />
<col width="76%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Latest release</td>
<td><a class="reference external" href="https://pypi.python.org/pypi/bayespy"><img alt="release" src="https://badge.fury.io/py/bayespy.svg" /></a></td>
</tr>
<tr class="row-even"><td>Documentation</td>
<td><a class="reference external" href="http://bayespy.org">http://bayespy.org</a></td>
</tr>
<tr class="row-odd"><td>Repository</td>
<td><a class="reference external" href="https://github.com/bayespy/bayespy.git">https://github.com/bayespy/bayespy.git</a></td>
</tr>
<tr class="row-even"><td>Bug reports</td>
<td><a class="reference external" href="https://github.com/bayespy/bayespy/issues">https://github.com/bayespy/bayespy/issues</a></td>
</tr>
<tr class="row-odd"><td>Author</td>
<td>Jaakko Luttinen <a class="reference external" href="mailto:jaakko&#46;luttinen&#37;&#52;&#48;iki&#46;fi">jaakko<span>&#46;</span>luttinen<span>&#64;</span>iki<span>&#46;</span>fi</a></td>
</tr>
<tr class="row-even"><td>Chat</td>
<td><a class="reference external" href="https://gitter.im/bayespy/bayespy?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge"><img alt="chat" src="https://badges.gitter.im/Join%20Chat.svg" /></a></td>
</tr>
<tr class="row-odd"><td>Mailing list</td>
<td><a class="reference external" href="mailto:bayespy&#37;&#52;&#48;googlegroups&#46;com">bayespy<span>&#64;</span>googlegroups<span>&#46;</span>com</a></td>
</tr>
</tbody>
</table>
<div class="section" id="continuous-integration">
<h4>Continuous integration<a class="headerlink" href="#continuous-integration" title="Permalink to this headline">¶</a></h4>
<table border="1" class="docutils">
<colgroup>
<col width="32%" />
<col width="24%" />
<col width="23%" />
<col width="21%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Branch</th>
<th class="head">Test status</th>
<th class="head">Test coverage</th>
<th class="head">Documentation</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><strong>master (stable)</strong></td>
<td><a class="reference external" href="https://travis-ci.org/bayespy/bayespy/"><img alt="travismaster" class="align-middle" src="https://travis-ci.org/bayespy/bayespy.svg?branch=master" /></a></td>
<td><a class="reference external" href="https://coveralls.io/r/bayespy/bayespy?branch=master"><img alt="covermaster" class="align-middle" src="https://coveralls.io/repos/bayespy/bayespy/badge.svg?branch=master" /></a></td>
<td><a class="reference external" href="http://www.bayespy.org/en/stable/"><img alt="docsmaster" class="align-middle" src="https://img.shields.io/badge/docs-master-blue.svg?style=flat" /></a></td>
</tr>
<tr class="row-odd"><td><strong>develop (latest)</strong></td>
<td><a class="reference external" href="https://travis-ci.org/bayespy/bayespy/"><img alt="travisdevelop" class="align-middle" src="https://travis-ci.org/bayespy/bayespy.svg?branch=develop" /></a></td>
<td><a class="reference external" href="https://coveralls.io/r/bayespy/bayespy?branch=develop"><img alt="coverdevelop" class="align-middle" src="https://coveralls.io/repos/bayespy/bayespy/badge.svg?branch=develop" /></a></td>
<td><a class="reference external" href="http://www.bayespy.org/en/latest/"><img alt="docsdevelop" class="align-middle" src="https://img.shields.io/badge/docs-develop-blue.svg?style=flat" /></a></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="similar-projects">
<h3>Similar projects<a class="headerlink" href="#similar-projects" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://vibes.sourceforge.net/">VIBES</a>
(<a class="reference external" href="http://vibes.sourceforge.net/">http://vibes.sourceforge.net/</a>) allows variational inference to be
performed automatically on a Bayesian network.  It is implemented in
Java and released under revised BSD license.</p>
<p><a class="reference external" href="http://research.ics.aalto.fi/bayes/software/">Bayes Blocks</a>
(<a class="reference external" href="http://research.ics.aalto.fi/bayes/software/">http://research.ics.aalto.fi/bayes/software/</a>) is a C++/Python
implementation of the variational building block framework.  The
framework allows easy learning of a wide variety of models using
variational Bayesian learning.  It is available as free software under
the GNU General Public License.</p>
<p><a class="reference external" href="http://research.microsoft.com/infernet/">Infer.NET</a>
(<a class="reference external" href="http://research.microsoft.com/infernet/">http://research.microsoft.com/infernet/</a>) is a .NET framework for
machine learning.  It provides message-passing algorithms and
statistical routines for performing Bayesian inference.  It is partly
closed source and licensed for non-commercial use only.</p>
<p><a class="reference external" href="https://github.com/pymc-devs/pymc">PyMC</a>
(<a class="reference external" href="https://github.com/pymc-devs/pymc">https://github.com/pymc-devs/pymc</a>) provides MCMC methods in Python.
It is released under the Academic Free License.</p>
<p><a class="reference external" href="http://www.openbugs.info">OpenBUGS</a> (<a class="reference external" href="http://www.openbugs.info">http://www.openbugs.info</a>) is a
software package for performing Bayesian inference using Gibbs
sampling.  It is released under the GNU General Public License.</p>
<p><a class="reference external" href="http://dimple.probprog.org/">Dimple</a> (<a class="reference external" href="http://dimple.probprog.org/">http://dimple.probprog.org/</a>) provides
Gibbs sampling, belief propagation and a few other inference algorithms for
Matlab and Java.  It is released under the Apache License.</p>
<p><a class="reference external" href="http://mc-stan.org/">Stan</a> (<a class="reference external" href="http://mc-stan.org/">http://mc-stan.org/</a>) provides inference using
MCMC with an interface for R and Python.  It is released under the New BSD
License.</p>
<p><a class="reference external" href="http://pbnt.berlios.de/">PBNT - Python Bayesian Network Toolbox</a>
(<a class="reference external" href="http://pbnt.berlios.de/">http://pbnt.berlios.de/</a>) is Bayesian network library in Python supporting
static networks with discrete variables.  There was no information about the
license.</p>
</div>
<div class="section" id="contributors">
<h3>Contributors<a class="headerlink" href="#contributors" title="Permalink to this headline">¶</a></h3>
<p>The list of contributors:</p>
<ul class="simple">
<li>Jaakko Luttinen</li>
<li>Hannu Hartikainen</li>
<li>Deebul Nair</li>
<li>Christopher Cramer</li>
<li>Till Hoffmann</li>
</ul>
<p>Each file or the git log can be used for more detailed information.</p>
</div>
<div class="section" id="version-history">
<h3>Version history<a class="headerlink" href="#version-history" title="Permalink to this headline">¶</a></h3>
<div class="section" id="version-0-5-11-2017-09-26">
<h4>Version 0.5.11 (2017-09-26)<a class="headerlink" href="#version-0-5-11-2017-09-26" title="Permalink to this headline">¶</a></h4>
<div class="section" id="fixed">
<h5>Fixed<a class="headerlink" href="#fixed" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Handle scalar moments of the innovation vector properly in Gaussian Markov
chain.</li>
<li>Skip some failing image comparison unit tests. Image comparison tests will be
deprecated at some point.</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-10-2017-09-02">
<h4>Version 0.5.10 (2017-09-02)<a class="headerlink" href="#version-0-5-10-2017-09-02" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id1">
<h5>Fixed<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix release</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-9-2017-09-02">
<h4>Version 0.5.9 (2017-09-02)<a class="headerlink" href="#version-0-5-9-2017-09-02" title="Permalink to this headline">¶</a></h4>
<div class="section" id="added">
<h5>Added<a class="headerlink" href="#added" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Support tqdm for monitoring the iteration progress (#105).</li>
<li>Allow VB iteration without maximum number of iteration steps (#104).</li>
<li>Add ellipse patch creation from covariance or precision (#103).</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-8-2017-05-13">
<h4>Version 0.5.8 (2017-05-13)<a class="headerlink" href="#version-0-5-8-2017-05-13" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id2">
<h5>Fixed<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Implement random sampling for Poisson</li>
<li>Update some old licensing information</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-7-2016-11-15">
<h4>Version 0.5.7 (2016-11-15)<a class="headerlink" href="#version-0-5-7-2016-11-15" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id3">
<h5>Fixed<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix deterministic mappings in Mixture, which caused NaNs in results</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-6-2016-11-08">
<h4>Version 0.5.6 (2016-11-08)<a class="headerlink" href="#version-0-5-6-2016-11-08" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id4">
<h5>Fixed<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Remove significant reshaping overhead in Cholesky computations in linalg
module</li>
<li>Fix minor plate multiplier issues</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-5-2016-11-04">
<h4>Version 0.5.5 (2016-11-04)<a class="headerlink" href="#version-0-5-5-2016-11-04" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id5">
<h5>Fixed<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix critical plate multiplier bug in Take node. The bug caused basically all
models with Take node to be incorrect.</li>
<li>Fix ndim handling in GaussianGamma and Wishart</li>
<li>Support lists and other array-convertible formats in several nodes</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-4-2016-10-27">
<h4>Version 0.5.4 (2016-10-27)<a class="headerlink" href="#version-0-5-4-2016-10-27" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id6">
<h5>Added<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Add conversion from Gamma to scalar Wishart</li>
<li>Implement message from GaussianMarkovChain to its input parent node</li>
<li>Add generic unit test functions for messages and moments</li>
</ul>
</div>
<div class="section" id="changed">
<h5>Changed<a class="headerlink" href="#changed" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Require NumPy 1.10 or greater</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-3-2016-08-17">
<h4>Version 0.5.3 (2016-08-17)<a class="headerlink" href="#version-0-5-3-2016-08-17" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id7">
<h5>Fixed<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix package metadata handling</li>
<li>Fix Travis test errors</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-2-2016-08-17">
<h4>Version 0.5.2 (2016-08-17)<a class="headerlink" href="#version-0-5-2-2016-08-17" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id8">
<h5>Added<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Add a node method to obtain the VB lower bound terms that contain the node</li>
</ul>
</div>
<div class="section" id="id9">
<h5>Fixed<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Handle empty CLI argument lists in CLI argument parsing</li>
<li>Fix handling of the two variables (Gaussian and Gamma) in GaussianGamma
methods</li>
<li>Fix minor bugs, including CGF in GaussianMarkovChain with inputs</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-1-2016-05-17">
<h4>Version 0.5.1 (2016-05-17)<a class="headerlink" href="#version-0-5-1-2016-05-17" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id10">
<h5>Fixed<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Accept lists as number of multinomial trials</li>
<li>Fix typo in handling concentration regularization shape</li>
</ul>
</div>
</div>
<div class="section" id="version-0-5-0-2016-05-04">
<h4>Version 0.5.0 (2016-05-04)<a class="headerlink" href="#version-0-5-0-2016-05-04" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id11">
<h5>Added<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Implement the following new nodes:<ul>
<li>Take</li>
<li>MultiMixture</li>
<li>ConcatGaussian</li>
<li>GaussianWishart</li>
<li>GaussianGamma</li>
<li>Choose</li>
<li>Concentration</li>
<li>MaximumLikelihood</li>
<li>Function</li>
</ul>
</li>
<li>Add preliminary support for maximum likelihood estimation (implemented only
for Wishart moments now)</li>
<li>Support multiplying Wishart variable by a gamma variable (scale method in
Wishart class)</li>
<li>Support GaussianWishart and GaussianGamma in GaussianMarkovChain</li>
<li>Support 1-p operation (complement) for beta variables</li>
<li>Implement random sampling for Multinomial node</li>
<li>Support ndim in many linalg functions and Gaussian-related nodes</li>
<li>Add conjugate gradient support for Multinomial and Mixture</li>
<li>Support monitoring of only some nodes when learning</li>
<li>Add diag() method to Gamma node</li>
<li>Add some examples as Jupyter notebooks</li>
</ul>
</div>
<div class="section" id="id12">
<h5>Changed<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Simplify GaussianARD mean parent handling</li>
<li>Move documentation to Read the Docs</li>
</ul>
</div>
<div class="section" id="id13">
<h5>Fixed<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix an axis mapping bug in Mixture (#39)</li>
<li>Fix NaN issue in Mixture with deterministic mappings (#66)</li>
<li>Fix Dirichlet node parent validation</li>
<li>Fix VB iteration when no data given (#67)</li>
<li>Fix axis label support in Hinton plots (#64)</li>
<li>Fix recursive node deletion</li>
</ul>
</div>
</div>
<div class="section" id="version-0-4-1-2015-11-02">
<h4>Version 0.4.1 (2015-11-02)<a class="headerlink" href="#version-0-4-1-2015-11-02" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Define extra dependencies needed to build the documentation</li>
</ul>
</div>
<div class="section" id="version-0-4-0-2015-11-02">
<h4>Version 0.4.0 (2015-11-02)<a class="headerlink" href="#version-0-4-0-2015-11-02" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Implement Add node for Gaussian nodes</li>
<li>Raise error if attempting to install on Python 2</li>
<li>Return both relative and absolute errors from numerical gradient checking</li>
<li>Add nose plugin to filter unit test warnings appropriately</li>
</ul>
</div>
<div class="section" id="version-0-3-9-2015-10-16">
<h4>Version 0.3.9 (2015-10-16)<a class="headerlink" href="#version-0-3-9-2015-10-16" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix Gaussian ARD node sampling</li>
</ul>
</div>
<div class="section" id="version-0-3-8-2015-10-16">
<h4>Version 0.3.8 (2015-10-16)<a class="headerlink" href="#version-0-3-8-2015-10-16" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix Gaussian node sampling</li>
</ul>
</div>
<div class="section" id="version-0-3-7-2015-09-23">
<h4>Version 0.3.7 (2015-09-23)<a class="headerlink" href="#version-0-3-7-2015-09-23" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Enable keyword arguments when plotting via the inference engine</li>
<li>Add initial support for logging</li>
</ul>
</div>
<div class="section" id="version-0-3-6-2015-08-12">
<h4>Version 0.3.6 (2015-08-12)<a class="headerlink" href="#version-0-3-6-2015-08-12" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Add maximum likelihood node for the shape parameter of Gamma</li>
<li>Fix Hinton diagrams for 1-D and 0-D Gaussians</li>
<li>Fix autosave interval counter</li>
<li>Fix bugs in constant nodes</li>
</ul>
</div>
<div class="section" id="version-0-3-5-2015-06-09">
<h4>Version 0.3.5 (2015-06-09)<a class="headerlink" href="#version-0-3-5-2015-06-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix indexing bug in VB optimization (not VB-EM)</li>
<li>Fix demos</li>
</ul>
</div>
<div class="section" id="version-0-3-4-2015-06-09">
<h4>Version 0.3.4 (2015-06-09)<a class="headerlink" href="#version-0-3-4-2015-06-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix computation of probability density of Dirichlet nodes</li>
<li>Use unit tests for all code snippets in docstrings and documentation</li>
</ul>
</div>
<div class="section" id="version-0-3-3-2015-06-05">
<h4>Version 0.3.3 (2015-06-05)<a class="headerlink" href="#version-0-3-3-2015-06-05" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Change license to the MIT license</li>
<li>Improve SumMultiply efficiency</li>
<li>Hinton diagrams for gamma variables</li>
<li>Possible to load only nodes from HDF5 results</li>
</ul>
</div>
<div class="section" id="version-0-3-2-2015-03-16">
<h4>Version 0.3.2 (2015-03-16)<a class="headerlink" href="#version-0-3-2-2015-03-16" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Concatenate node added</li>
<li>Unit tests for plotting fixed</li>
</ul>
</div>
<div class="section" id="version-0-3-1-2015-03-12">
<h4>Version 0.3.1 (2015-03-12)<a class="headerlink" href="#version-0-3-1-2015-03-12" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Gaussian mixture 2D plotting improvements</li>
<li>Covariance matrix sampling improvements</li>
<li>Minor documentation fixes</li>
</ul>
</div>
<div class="section" id="version-0-3-2015-03-05">
<h4>Version 0.3 (2015-03-05)<a class="headerlink" href="#version-0-3-2015-03-05" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Add gradient-based optimization methods (Riemannian/natural gradient or normal)</li>
<li>Add collapsed inference</li>
<li>Add the pattern search method</li>
<li>Add deterministic annealing</li>
<li>Add stochastic variational inference</li>
<li>Add optional input signals to Gaussian Markov chains</li>
<li>Add unit tests for plotting functions (by Hannu Hartikainen)</li>
<li>Add printing support to nodes</li>
<li>Drop Python 3.2 support</li>
</ul>
</div>
<div class="section" id="version-0-2-3-2014-12-03">
<h4>Version 0.2.3 (2014-12-03)<a class="headerlink" href="#version-0-2-3-2014-12-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix matplotlib compatibility broken by recent changes in matplotlib</li>
<li>Add random sampling for Binomial and Bernoulli nodes</li>
<li>Fix minor bugs, for instance, in plot module</li>
</ul>
</div>
<div class="section" id="version-0-2-2-2014-11-01">
<h4>Version 0.2.2 (2014-11-01)<a class="headerlink" href="#version-0-2-2-2014-11-01" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix normalization of categorical Markov chain probabilities (fixes HMM demo)</li>
<li>Fix initialization from parameter values</li>
</ul>
</div>
<div class="section" id="version-0-2-1-2014-09-30">
<h4>Version 0.2.1 (2014-09-30)<a class="headerlink" href="#version-0-2-1-2014-09-30" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Add workaround for matplotlib 1.4.0 bug related to interactive mode which
affected monitoring</li>
<li>Fix bugs in Hinton diagrams for Gaussian variables</li>
</ul>
</div>
<div class="section" id="version-0-2-2014-08-06">
<h4>Version 0.2 (2014-08-06)<a class="headerlink" href="#version-0-2-2014-08-06" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Added all remaining common distributions: Bernoulli, binomial, multinomial,
Poisson, beta, exponential.</li>
<li>Added Gaussian arrays (not just scalars or vectors).</li>
<li>Added Gaussian Markov chains with time-varying or swithing dynamics.</li>
<li>Added discrete Markov chains (enabling hidden Markov models).</li>
<li>Added joint Gaussian-Wishart and Gaussian-gamma nodes.</li>
<li>Added deterministic gating node.</li>
<li>Added deterministic general sum-product node.</li>
<li>Added parameter expansion for Gaussian arrays and time-varying/switching
Gaussian Markov chains.</li>
<li>Added new plotting functions: pdf, Hinton diagram.</li>
<li>Added monitoring of posterior distributions during iteration.</li>
<li>Finished documentation and added API.</li>
</ul>
</div>
<div class="section" id="version-0-1-2013-07-25">
<h4>Version 0.1 (2013-07-25)<a class="headerlink" href="#version-0-1-2013-07-25" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Added variational message passing inference engine.</li>
<li>Added the following common distributions: Gaussian vector, gamma, Wishart,
Dirichlet, categorical.</li>
<li>Added Gaussian Markov chain.</li>
<li>Added parameter expansion for Gaussian vectors and Gaussian Markov chain.</li>
<li>Added stochastic mixture node.</li>
<li>Added deterministic dot product node.</li>
<li>Created preliminary version of the documentation.</li>
</ul>
</div>
</div>
</div>
<span id="document-user_guide/user_guide"></span><div class="section" id="user-guide">
<span id="sec-user-guide"></span><h2>User guide<a class="headerlink" href="#user-guide" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-user_guide/install"></span><div class="section" id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h3>
<p>BayesPy is a Python 3 package and it can be installed from PyPI or the latest
development version from GitHub.  The instructions below explain how to set up
the system by installing required packages, how to install BayesPy and how to
compile this documentation yourself.  However, if these instructions contain
errors or some relevant details are missing, please file a bug report at
<a class="reference external" href="https://github.com/bayespy/bayespy/issues">https://github.com/bayespy/bayespy/issues</a>.</p>
<div class="section" id="installing-bayespy">
<h4>Installing BayesPy<a class="headerlink" href="#installing-bayespy" title="Permalink to this headline">¶</a></h4>
<p>BayesPy can be installed easily by using Pip if the system has been properly set
up.  If you have problems with the following methods, see the following section
for some help on installing the requirements.  For instance, a bug in recent
versions of h5py and pip may require you to install some of the requirements
manually.</p>
<div class="section" id="for-users">
<h5>For users<a class="headerlink" href="#for-users" title="Permalink to this headline">¶</a></h5>
<p>First, you may want to set up a virtual environment.  Using virtual environment
is optional but recommended.  To create and activate a new virtual environment,
run (in the folder in which you want to create the environment):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">virtualenv -p python3 --system-site-packages ENV</span>
<span class="go">source ENV/bin/activate</span>
</pre></div>
</div>
<p>The latest release of BayesPy can be installed from PyPI simply as</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install bayespy</span>
</pre></div>
</div>
<p>If you want to install the latest development version of BayesPy, use GitHub
instead:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install git+https://github.com/bayespy/bayespy.git@develop</span>
</pre></div>
</div>
</div>
<div class="section" id="for-developers">
<h5>For developers<a class="headerlink" href="#for-developers" title="Permalink to this headline">¶</a></h5>
<p>If you want to install the development version of BayesPy in such a way that you
can easily edit the package, follow these instructions.  Get the git repository:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">git clone https://github.com/bayespy/bayespy.git</span>
<span class="go">cd bayespy</span>
</pre></div>
</div>
<p>Create and activate a new virtual environment (optional but recommended):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">virtualenv -p python3 --system-site-packages ENV</span>
<span class="go">source ENV/bin/activate</span>
</pre></div>
</div>
<p>Install BayesPy in editable mode:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install -e .</span>
</pre></div>
</div>
</div>
<div class="section" id="checking-installation">
<h5>Checking installation<a class="headerlink" href="#checking-installation" title="Permalink to this headline">¶</a></h5>
<p>If you have problems installing BayesPy, read the next section for more details.
It is recommended to run the unit tests in order to check that BayesPy is
working properly.  Thus, install Nose and run the unit tests:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install nose</span>
<span class="go">nosetests bayespy</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="installing-requirements">
<h4>Installing requirements<a class="headerlink" href="#installing-requirements" title="Permalink to this headline">¶</a></h4>
<p>BayesPy requires Python 3.3 (or later) and the following packages:</p>
<ul class="simple">
<li>NumPy (&gt;=1.10.0),</li>
<li>SciPy (&gt;=0.13.0)</li>
<li>matplotlib (&gt;=1.2)</li>
<li>h5py</li>
</ul>
<p>Ideally, Pip should install the necessary requirements and a manual installation
of these dependencies is not required.  However, there are several reasons why
the installation of these dependencies needs to be done manually in some cases.
Thus, this section tries to give some details on how to set up your system.  A
proper installation of the dependencies for Python 3 can be a bit tricky and you
may refer to <a class="reference external" href="http://www.scipy.org/install.html">http://www.scipy.org/install.html</a> for more detailed instructions
about the SciPy stack.  Detailed instructions on installing recent SciPy stack
for various platforms is out of the scope of these instructions, but we provide
some general guidance here.  There are basically three ways to install the
dependencies:</p>
<blockquote>
<div><ol class="arabic">
<li><p class="first">Install a Python distribution which includes the packages.  For Windows,
Mac and Linux, there are several Python distributions which include all the
necessary packages:
<a class="reference external" href="http://www.scipy.org/install.html#scientific-python-distributions">http://www.scipy.org/install.html#scientific-python-distributions</a>.  For
instance, you may try <a class="reference external" href="http://continuum.io/downloads">Anaconda</a> or
<a class="reference external" href="https://www.enthought.com/products/canopy/">Enthought</a>.</p>
</li>
<li><p class="first">Install the packages using the system package manager.  On Linux, the
packages might be called something like <code class="docutils literal"><span class="pre">python-scipy</span></code> or <code class="docutils literal"><span class="pre">scipy</span></code>.
However, it is possible that these system packages are not recent enough
for BayesPy.</p>
</li>
<li><p class="first">Install the packages using Pip:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install &quot;distribute&gt;=0.6.28&quot;</span>
<span class="go">pip install &quot;numpy&gt;=1.10.0&quot; &quot;scipy&gt;=0.13.0&quot; &quot;matplotlib&gt;=1.2&quot; h5py</span>
</pre></div>
</div>
<p>This also makes sure you have recent enough version of Distribute (required
by Matplotlib).  However, this installation method may require that the
system has some libraries needed for compiling (e.g., C compiler, Python
development files, BLAS/LAPACK).  For instance, on Ubuntu (&gt;= 12.10), you
may install the required system libraries for each package as:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">sudo apt-get build-dep python3-numpy</span>
<span class="go">sudo apt-get build-dep python3-scipy</span>
<span class="go">sudo apt-get build-dep python3-matplotlib</span>
<span class="go">sudo apt-get build-dep python-h5py</span>
</pre></div>
</div>
<p>Then installation using Pip should work.</p>
</li>
</ol>
</div></blockquote>
</div>
<div class="section" id="compiling-documentation">
<h4>Compiling documentation<a class="headerlink" href="#compiling-documentation" title="Permalink to this headline">¶</a></h4>
<p>This documentation can be found at <a class="reference external" href="http://bayespy.org/">http://bayespy.org/</a> in HTML and PDF formats.
The documentation source files are also readable as such in reStructuredText
format in <code class="docutils literal"><span class="pre">doc/source/</span></code> directory.  It is possible to compile the
documentation into HTML or PDF yourself.  In order to compile the documentation,
Sphinx is required and a few extensions for it. Those can be installed as:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install &quot;sphinx&gt;=1.2.3&quot; sphinxcontrib-tikz sphinxcontrib-bayesnet sphinxcontrib-bibtex &quot;numpydoc&gt;=0.5&quot;</span>
</pre></div>
</div>
<p>Or you can simply install BayesPy with <code class="docutils literal"><span class="pre">doc</span></code> extra, which will take care of
installing the required dependencies:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">pip install bayespy[doc]</span>
</pre></div>
</div>
<p>In order to visualize graphical models in HTML, you need to have <code class="docutils literal"><span class="pre">ImageMagick</span></code>
or <code class="docutils literal"><span class="pre">Netpbm</span></code> installed.  The documentation can be compiled to HTML and PDF by
running the following commands in the <code class="docutils literal"><span class="pre">doc</span></code> directory:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">make html</span>
<span class="go">make latexpdf</span>
</pre></div>
</div>
<p>You can also run doctest to test code snippets in the documentation:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">make doctest</span>
</pre></div>
</div>
<p>or in the docstrings:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">nosetests --with-doctest --doctest-options=&quot;+ELLIPSIS&quot; bayespy</span>
</pre></div>
</div>
</div>
</div>
<span id="document-user_guide/quickstart"></span><div class="section" id="quick-start-guide">
<h3>Quick start guide<a class="headerlink" href="#quick-start-guide" title="Permalink to this headline">¶</a></h3>
<p>This short guide shows the key steps in using BayesPy for variational
Bayesian inference by applying BayesPy to a simple problem. The key
steps in using BayesPy are the following:</p>
<ul class="simple">
<li>Construct the model</li>
<li>Observe some of the variables by providing the data in a proper
format</li>
<li>Run variational Bayesian inference</li>
<li>Examine the resulting posterior approximation</li>
</ul>
<p>To demonstrate BayesPy, we&#8217;ll consider a very simple problem: we have a
set of observations from a Gaussian distribution with unknown mean and
variance, and we want to learn these parameters. In this case, we do not
use any real-world data but generate some artificial data. The dataset
consists of ten samples from a Gaussian distribution with mean 5 and
standard deviation 10. This dataset can be generated with NumPy as
follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
</pre></div>
</div>
<div class="section" id="constructing-the-model">
<h4>Constructing the model<a class="headerlink" href="#constructing-the-model" title="Permalink to this headline">¶</a></h4>
<p>Now, given this data we would like to estimate the mean and the standard
deviation as if we didn&#8217;t know their values. The model can be defined as
follows:</p>
<div class="math">
<p><img src="_images/math/e3dcf770acd86b6898596159468ab1d6cc4cd1a5.svg" alt="\begin{split}
p(\mathbf{y}|\mu,\tau) &amp;= \prod^{9}_{n=0} \mathcal{N}(y_n|\mu,\tau) \\
p(\mu) &amp;= \mathcal{N}(\mu|0,10^{-6}) \\
p(\tau) &amp;= \mathcal{G}(\tau|10^{-6},10^{-6})
\end{split}"/></p>
</div><p>where <img class="math" src="_images/math/5af756a4f800353849702d0b41faf3d3af5b22f4.svg" alt="\mathcal{N}"/> is the Gaussian distribution parameterized by
its mean and precision (i.e., inverse variance), and <img class="math" src="_images/math/40025f8c7391a7af5ad6d6e9bfa98755610ffe73.svg" alt="\mathcal{G}"/>
is the gamma distribution parameterized by its shape and rate
parameters. Note that we have given quite uninformative priors for the
variables <img class="math" src="_images/math/d79e8a2c7ce54906c2b25549da38bdbe02cf40d6.svg" alt="\mu"/> and <img class="math" src="_images/math/557d5fad862c9046d26e1a930f45a550c146d592.svg" alt="\tau"/>. This simple model can also be
shown as a directed factor graph:</p>
<div class="figure">
<p><img src="_images/tikz-57bc0c88a2974f4c1e2335fe9edb88ff2efdf970.png" alt="% tikzlibrary.code.tex
%
% Copyright 2010-2011 by Laura Dietz
% Copyright 2012 by Jaakko Luttinen
%
% This file may be distributed and/or modified
%
% 1. under the LaTeX Project Public License and/or
% 2. under the GNU General Public License.
%
% See the files LICENSE_LPPL and LICENSE_GPL for more details.

% Load other libraries
\usetikzlibrary{shapes}
\usetikzlibrary{fit}
\usetikzlibrary{chains}
\usetikzlibrary{arrows}

% Latent node
\tikzstyle{latent} = [circle,fill=white,draw=black,inner sep=1pt,
minimum size=20pt, font=\fontsize{10}{10}\selectfont, node distance=1]
% Observed node
\tikzstyle{obs} = [latent,fill=gray!25]
% Constant node
\tikzstyle{const} = [rectangle, inner sep=0pt, node distance=1]
% Factor node
\tikzstyle{factor} = [rectangle, fill=black,minimum size=5pt, inner
sep=0pt, node distance=0.4]
% Deterministic node
\tikzstyle{det} = [latent, diamond]

% Plate node
\tikzstyle{plate} = [draw, rectangle, rounded corners, fit=#1]
% Invisible wrapper node
\tikzstyle{wrap} = [inner sep=0pt, fit=#1]
% Gate
\tikzstyle{gate} = [draw, rectangle, dashed, fit=#1]

% Caption node
\tikzstyle{caption} = [font=\footnotesize, node distance=0] %
\tikzstyle{plate caption} = [caption, node distance=0, inner sep=0pt,
below left=5pt and 0pt of #1.south east] %
\tikzstyle{factor caption} = [caption] %
\tikzstyle{every label} += [caption] %

\tikzset{&gt;={triangle 45}}

%\pgfdeclarelayer{b}
%\pgfdeclarelayer{f}
%\pgfsetlayers{b,main,f}

% \factoredge [options] {inputs} {factors} {outputs}
\newcommand{\factoredge}[4][]{ %
  % Connect all nodes #2 to all nodes #4 via all factors #3.
  \foreach \f in {#3} { %
    \foreach \x in {#2} { %
      \draw[-,#1] (\x) edge[-] (\f) ; %
    } ;
    \foreach \y in {#4} { %
      \draw[-&gt;,#1] (\f) -- (\y) ; %
    } ;
  } ;
}

% \edge [options] {inputs} {outputs}
\newcommand{\edge}[3][]{ %
  % Connect all nodes #2 to all nodes #3.
  \foreach \x in {#2} { %
    \foreach \y in {#3} { %
      \draw[-&gt;,#1] (\x) -- (\y) ;%
    } ;
  } ;
}

% \factor [options] {name} {caption} {inputs} {outputs}
\newcommand{\factor}[5][]{ %
  % Draw the factor node. Use alias to allow empty names.
  \node[factor, label={[name=#2-caption]#3}, name=#2, #1,
  alias=#2-alias] {} ; %
  % Connect all inputs to outputs via this factor
  \factoredge {#4} {#2-alias} {#5} ; %
}

% \plate [options] {name} {fitlist} {caption}
\newcommand{\plate}[4][]{ %
  \node[wrap=#3] (#2-wrap) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
  \node[plate=(#2-wrap)(#2-caption), #1] (#2) {}; %
}

% \gate [options] {name} {fitlist} {inputs}
\newcommand{\gate}[4][]{ %
  \node[gate=#3, name=#2, #1, alias=#2-alias] {}; %
  \foreach \x in {#4} { %
    \draw [-*,thick] (\x) -- (#2-alias); %
  } ;%
}

% \vgate {name} {fitlist-left} {caption-left} {fitlist-right}
% {caption-right} {inputs}
\newcommand{\vgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-left) {}; %
  \node[wrap=#4] (#1-right) {}; %
  % Draw the gate
  \node[gate=(#1-left)(#1-right)] (#1) {}; %
  % Add captions
  \node[caption, below left=of #1.north ] (#1-left-caption)
  {#3}; %
  \node[caption, below right=of #1.north ] (#1-right-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.north) -- (#1.south); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

% \hgate {name} {fitlist-top} {caption-top} {fitlist-bottom}
% {caption-bottom} {inputs}
\newcommand{\hgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-top) {}; %
  \node[wrap=#4] (#1-bottom) {}; %
  % Draw the gate
  \node[gate=(#1-top)(#1-bottom)] (#1) {}; %
  % Add captions
  \node[caption, above right=of #1.west ] (#1-top-caption)
  {#3}; %
  \node[caption, below right=of #1.west ] (#1-bottom-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.west) -- (#1.east); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

\node[obs]                                  (y)     {$y_n$} ;
\node[latent, above left=1.5 and 0.5 of y]  (mu)    {$\mu$} ;
\node[latent, above right=1.5 and 0.5 of y] (tau)   {$\tau$} ;
\factor[above=of mu] {mu-f} {left:$\mathcal{N}$} {} {mu} ;
\factor[above=of tau] {tau-f} {left:$\mathcal{G}$} {} {tau} ;

\factor[above=of y] {y-f} {left:$\mathcal{N}$} {mu,tau}     {y};

\plate {} {(y)(y-f)(y-f-caption)} {$n=0,\ldots,9$} ;" /></p>
<p class="caption">Directed factor graph of the example model.</p></div><p>This model can be constructed in BayesPy as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">GaussianARD</span><span class="p">,</span> <span class="n">Gamma</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
</pre></div>
</div>
<p>This is quite self-explanatory given the model definitions above. We have used
two types of nodes <a class="reference internal" href="index.html#bayespy.nodes.GaussianARD" title="bayespy.nodes.GaussianARD"><code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code></a> and <a class="reference internal" href="index.html#bayespy.nodes.Gamma" title="bayespy.nodes.Gamma"><code class="xref py py-class docutils literal"><span class="pre">Gamma</span></code></a> to represent Gaussian
and gamma distributions, respectively. There are much more distributions in
<a class="reference internal" href="index.html#module-bayespy.nodes" title="bayespy.nodes"><code class="xref py py-mod docutils literal"><span class="pre">bayespy.nodes</span></code></a> so you can construct quite complex conjugate exponential
family models. The node <code class="code docutils literal"><span class="pre">y</span></code> uses keyword argument <code class="code docutils literal"><span class="pre">plates</span></code> to define
the plates <img class="math" src="_images/math/3b25346337c96ffd54c46bee587339db3176179a.svg" alt="n=0,\ldots,9"/>.</p>
</div>
<div class="section" id="performing-inference">
<h4>Performing inference<a class="headerlink" href="#performing-inference" title="Permalink to this headline">¶</a></h4>
<p>Now that we have created the model, we can provide our data by setting
<code class="docutils literal"><span class="pre">y</span></code> as observed:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>Next we want to estimate the posterior distribution. In principle, we could use
different inference engines (e.g., MCMC or EP) but currently only variational
Bayesian (VB) engine is implemented. The engine is initialized by giving all the
nodes of the model:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>The inference algorithm can be run as long as wanted (max. 20 iterations
in this case):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-6.020956e+01 (... seconds)</span>
<span class="go">Iteration 2: loglike=-5.820527e+01 (... seconds)</span>
<span class="go">Iteration 3: loglike=-5.820290e+01 (... seconds)</span>
<span class="go">Iteration 4: loglike=-5.820288e+01 (... seconds)</span>
<span class="go">Converged at iteration 4.</span>
</pre></div>
</div>
<p>Now the algorithm converged after four iterations, before the requested 20
iterations. VB approximates the true posterior <img class="math" src="_images/math/a5030679301a41f886c8b53fb82ce8fb164b4e39.svg" alt="p(\mu,\tau|\mathbf{y})"/>
with a distribution which factorizes with respect to the nodes:
<img class="math" src="_images/math/4f55f8807f438c852b87c3b1c85f092e5b5cb9c0.svg" alt="q(\mu)q(\tau)"/>.</p>
</div>
<div class="section" id="examining-posterior-approximation">
<h4>Examining posterior approximation<a class="headerlink" href="#examining-posterior-approximation" title="Permalink to this headline">¶</a></h4>
<p>The resulting approximate posterior distributions <img class="math" src="_images/math/c22a4f38cc55a2b7f092cea76361669c982ead5b.svg" alt="q(\mu)"/> and
<img class="math" src="_images/math/37e96d65f9c9338be4b18c77e54dd7b8e534184c.svg" alt="q(\tau)"/> can be examined, for instance, by plotting the marginal
probability density functions:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">&lt;matplotlib.axes...AxesSubplot object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;\mu&#39;</span><span class="p">)</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">&lt;matplotlib.axes...AxesSubplot object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;\tau&#39;</span><span class="p">)</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../user_guide/quickstart-1.py">Source code</a>, <a class="reference external" href="../user_guide/quickstart-1.png">png</a>, <a class="reference external" href="../user_guide/quickstart-1.hires.png">hires.png</a>, <a class="reference external" href="../user_guide/quickstart-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/quickstart-1.png" src="_images/quickstart-1.png" />
</div>
<p>This example was a very simple introduction to using BayesPy. The model
can be much more complex and each phase contains more options to give
the user more control over the inference. The following sections give
more details about the phases.</p>
</div>
</div>
<span id="document-user_guide/modelconstruct"></span><div class="section" id="constructing-the-model">
<h3>Constructing the model<a class="headerlink" href="#constructing-the-model" title="Permalink to this headline">¶</a></h3>
<p>In BayesPy, the model is constructed by creating nodes which form a directed
network.  There are two types of nodes: stochastic and deterministic.  A
stochastic node corresponds to a random variable (or a set of random variables)
from a specific probability distribution.  A deterministic node corresponds to a
deterministic function of its parents. For a list of built-in nodes, see the
<a class="reference internal" href="index.html#sec-user-api"><span class="std std-ref">User API</span></a>.</p>
<div class="section" id="creating-nodes">
<h4>Creating nodes<a class="headerlink" href="#creating-nodes" title="Permalink to this headline">¶</a></h4>
<p>Creating a node is basically like writing the conditional prior distribution of
the variable in Python.  The node is constructed by giving the parent nodes,
that is, the conditioning variables as arguments.  The number of parents and
their meaning depend on the node.  For instance, a <a class="reference internal" href="index.html#bayespy.nodes.Gaussian" title="bayespy.nodes.Gaussian"><code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code></a> node is
created by giving the mean vector and the precision matrix. These parents can be
constant numerical arrays if they are known:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Gaussian</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
</pre></div>
</div>
<p>or other nodes if they are unknown and given prior distributions:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">Wishart</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">Wishart</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">)</span>
</pre></div>
</div>
<p>Nodes can also be named by providing <code class="docutils literal"><span class="pre">name</span></code> keyword argument:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The name may be useful when referring to the node using an inference engine.</p>
<p>For the parent nodes, there are two main restrictions: non-constant parent nodes
must be conjugate and the parent nodes must be mutually independent in the
posterior approximation.</p>
<div class="section" id="conjugacy-of-the-parents">
<h5>Conjugacy of the parents<a class="headerlink" href="#conjugacy-of-the-parents" title="Permalink to this headline">¶</a></h5>
<p>In Bayesian framework in general, one can give quite arbitrary probability
distributions for variables. However, one often uses distributions that are easy
to handle in practice. Quite often this means that the parents are given
conjugate priors. This is also one of the limitations in BayesPy: only conjugate
family prior distributions are accepted currently. Thus, although in principle
one could give, for instance, gamma prior for the mean parameter <code class="docutils literal"><span class="pre">mu</span></code>, only
Gaussian-family distributions are accepted because of the conjugacy. If the
parent is not of a proper type, an error is raised. This conjugacy is checked
automatically by BayesPy and <code class="docutils literal"><span class="pre">NoConverterError</span></code> is raised if a parent cannot
be interpreted as being from a conjugate distribution.</p>
</div>
<div class="section" id="independence-of-the-parents">
<h5>Independence of the parents<a class="headerlink" href="#independence-of-the-parents" title="Permalink to this headline">¶</a></h5>
<p>Another a bit rarely encountered limitation is that the parents must be mutually
independent (in the posterior factorization). Thus, a node cannot have the same
stochastic node as several parents without intermediate stochastic nodes. For
instance, the following leads to an error:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Dot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">ValueError</span>: <span class="n">Parent nodes are not independent</span>
</pre></div>
</div>
<p>The error is raised because <code class="docutils literal"><span class="pre">X</span></code> is given as two parents for <code class="docutils literal"><span class="pre">Y</span></code>, and
obviously <code class="docutils literal"><span class="pre">X</span></code> is not independent of <code class="docutils literal"><span class="pre">X</span></code> in the posterior approximation. Even
if <code class="docutils literal"><span class="pre">X</span></code> is not given several times directly but there are some intermediate
deterministic nodes, an error is raised because the deterministic nodes depend
on their parents and thus the parents of <code class="docutils literal"><span class="pre">Y</span></code> would not be independent.
However, it is valid that a node is a parent of another node via several paths
if all the paths or all except one path has intermediate stochastic nodes. This
is valid because the intermediate stochastic nodes have independent posterior
approximations. Thus, for instance, the following construction does not raise
errors:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Dot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>This works because there is now an intermediate stochastic node <code class="docutils literal"><span class="pre">Z</span></code> on the
other path from <code class="docutils literal"><span class="pre">X</span></code> node to <code class="docutils literal"><span class="pre">Y</span></code> node.</p>
</div>
</div>
<div class="section" id="effects-of-the-nodes-on-inference">
<h4>Effects of the nodes on inference<a class="headerlink" href="#effects-of-the-nodes-on-inference" title="Permalink to this headline">¶</a></h4>
<p>When constructing the network with nodes, the stochastic nodes actually define
three important aspects:</p>
<ol class="arabic simple">
<li>the prior probability distribution for the variables,</li>
<li>the factorization of the posterior approximation,</li>
<li>the functional form of the posterior approximation for the variables.</li>
</ol>
<div class="section" id="prior-probability-distribution">
<h5>Prior probability distribution<a class="headerlink" href="#prior-probability-distribution" title="Permalink to this headline">¶</a></h5>
<p>First, the most intuitive feature of the nodes is that they define the prior
distribution. In the previous example, <code class="docutils literal"><span class="pre">mu</span></code> was a stochastic
<a class="reference internal" href="index.html#bayespy.nodes.GaussianARD" title="bayespy.nodes.GaussianARD"><code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code></a> node corresponding to <img class="math" src="_images/math/d79e8a2c7ce54906c2b25549da38bdbe02cf40d6.svg" alt="\mu"/> from the normal
distribution, <code class="docutils literal"><span class="pre">tau</span></code> was a stochastic <a class="reference internal" href="index.html#bayespy.nodes.Gamma" title="bayespy.nodes.Gamma"><code class="xref py py-class docutils literal"><span class="pre">Gamma</span></code></a> node corresponding to
<img class="math" src="_images/math/557d5fad862c9046d26e1a930f45a550c146d592.svg" alt="\tau"/> from the gamma distribution, and <code class="docutils literal"><span class="pre">y</span></code> was a stochastic
<a class="reference internal" href="index.html#bayespy.nodes.GaussianARD" title="bayespy.nodes.GaussianARD"><code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code></a> node corresponding to <img class="math" src="_images/math/276f7e256cbddeb81eee42e1efc348f3cb4ab5f8.svg" alt="y"/> from the normal
distribution with mean <img class="math" src="_images/math/d79e8a2c7ce54906c2b25549da38bdbe02cf40d6.svg" alt="\mu"/> and precision <img class="math" src="_images/math/557d5fad862c9046d26e1a930f45a550c146d592.svg" alt="\tau"/>.  If we denote the
set of all stochastic nodes by <img class="math" src="_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.svg" alt="\Omega"/>, and by <img class="math" src="_images/math/fda2ee58e90949f860b19692e886b08f90b1e572.svg" alt="\pi_X"/> the set of
parents of a node <img class="math" src="_images/math/7a7bb470119808e2db2879fc2b2526f467b7a40b.svg" alt="X"/>, the model is defined as</p>
<div class="math">
<p><img src="_images/math/ceaaff9fd92d98b48f288c52af14682ec748c531.svg" alt="p(\Omega) = \prod_{X \in \Omega} p(X|\pi_X),"/></p>
</div><p>where nodes correspond to the terms <img class="math" src="_images/math/afa459ffd219f85f65bed1df6702201d7d592951.svg" alt="p(X|\pi_X)"/>.</p>
</div>
<div class="section" id="posterior-factorization">
<h5>Posterior factorization<a class="headerlink" href="#posterior-factorization" title="Permalink to this headline">¶</a></h5>
<p>Second, the nodes define the structure of the posterior approximation.  The
variational Bayesian approximation factorizes with respect to nodes, that is,
each node corresponds to an independent probability distribution in the
posterior approximation. In the previous example, <code class="docutils literal"><span class="pre">mu</span></code> and <code class="docutils literal"><span class="pre">tau</span></code> were
separate nodes, thus the posterior approximation factorizes with respect to
them: <img class="math" src="_images/math/4f55f8807f438c852b87c3b1c85f092e5b5cb9c0.svg" alt="q(\mu)q(\tau)"/>. Thus, the posterior approximation can be written
as:</p>
<div class="math">
<p><img src="_images/math/7707631d430a72b42399788ff8ff76a3cdf5febb.svg" alt="p(\tilde{\Omega}|\hat{\Omega}) \approx \prod_{X \in \tilde{\Omega}} q(X),"/></p>
</div><p>where <img class="math" src="_images/math/abf86dd4f493a1c1de351bac4cd9f365e3b4870c.svg" alt="\tilde{\Omega}"/> is the set of latent stochastic nodes and
<img class="math" src="_images/math/49757d9a2f2b8599d7de7086c8aad6c338c9ea44.svg" alt="\hat{\Omega}"/> is the set of observed stochastic nodes.  Sometimes one may
want to avoid the factorization between some variables.  For this purpose, there
are some nodes which model several variables jointly without factorization.  For
instance, <code class="xref py py-class docutils literal"><span class="pre">GaussianGammaISO</span></code> is a joint node for <img class="math" src="_images/math/d79e8a2c7ce54906c2b25549da38bdbe02cf40d6.svg" alt="\mu"/> and
<img class="math" src="_images/math/557d5fad862c9046d26e1a930f45a550c146d592.svg" alt="\tau"/> variables from the normal-gamma distribution and the posterior
approximation does not factorize between <img class="math" src="_images/math/d79e8a2c7ce54906c2b25549da38bdbe02cf40d6.svg" alt="\mu"/> and <img class="math" src="_images/math/557d5fad862c9046d26e1a930f45a550c146d592.svg" alt="\tau"/>, that is,
the posterior approximation is <img class="math" src="_images/math/ffe53846dd71900503e57df6b5223417c410b85d.svg" alt="q(\mu,\tau)"/>.</p>
</div>
<div class="section" id="functional-form-of-the-posterior">
<h5>Functional form of the posterior<a class="headerlink" href="#functional-form-of-the-posterior" title="Permalink to this headline">¶</a></h5>
<p>Last, the nodes define the functional form of the posterior approximation.
Usually, the posterior approximation has the same or similar functional form as
the prior.  For instance, <a class="reference internal" href="index.html#bayespy.nodes.Gamma" title="bayespy.nodes.Gamma"><code class="xref py py-class docutils literal"><span class="pre">Gamma</span></code></a> uses gamma distribution to also
approximate the posterior distribution.  Similarly, <a class="reference internal" href="index.html#bayespy.nodes.GaussianARD" title="bayespy.nodes.GaussianARD"><code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code></a> uses
Gaussian distribution for the posterior.  However, the posterior approximation
of <a class="reference internal" href="index.html#bayespy.nodes.GaussianARD" title="bayespy.nodes.GaussianARD"><code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code></a> uses a full covariance matrix although the prior assumes
a diagonal covariance matrix.  Thus, there can be slight differences in the
exact functional form of the posterior approximation but the rule of thumb is
that the functional form of the posterior approximation is the same as or more
general than the functional form of the prior.</p>
</div>
</div>
<div class="section" id="using-plate-notation">
<h4>Using plate notation<a class="headerlink" href="#using-plate-notation" title="Permalink to this headline">¶</a></h4>
<div class="section" id="defining-plates">
<h5>Defining plates<a class="headerlink" href="#defining-plates" title="Permalink to this headline">¶</a></h5>
<p>Stochastic nodes take the optional parameter <code class="docutils literal"><span class="pre">plates</span></code>, which can be used to
define plates of the variable. A plate defines the number of repetitions of a
set of variables. For instance, a set of random variables
<img class="math" src="_images/math/0b8d24e47b97db5308e62d423ea87a372308b73e.svg" alt="\mathbf{y}_{mn}"/> could be defined as</p>
<div class="math">
<p><img src="_images/math/db5105aacfef52e21fbdeebce55a846e20302c57.svg" alt="\mathbf{y}_{mn} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Lambda}),\qquad m=0,\ldots,9, \quad n=0,\ldots,29."/></p>
</div><p>This can also be visualized as a graphical model:</p>
<div class="figure">
<p><img src="_images/tikz-80a1db369be1f25b61ceacfff551dae2bdd331c3.png" alt="% tikzlibrary.code.tex
%
% Copyright 2010-2011 by Laura Dietz
% Copyright 2012 by Jaakko Luttinen
%
% This file may be distributed and/or modified
%
% 1. under the LaTeX Project Public License and/or
% 2. under the GNU General Public License.
%
% See the files LICENSE_LPPL and LICENSE_GPL for more details.

% Load other libraries
\usetikzlibrary{shapes}
\usetikzlibrary{fit}
\usetikzlibrary{chains}
\usetikzlibrary{arrows}

% Latent node
\tikzstyle{latent} = [circle,fill=white,draw=black,inner sep=1pt,
minimum size=20pt, font=\fontsize{10}{10}\selectfont, node distance=1]
% Observed node
\tikzstyle{obs} = [latent,fill=gray!25]
% Constant node
\tikzstyle{const} = [rectangle, inner sep=0pt, node distance=1]
% Factor node
\tikzstyle{factor} = [rectangle, fill=black,minimum size=5pt, inner
sep=0pt, node distance=0.4]
% Deterministic node
\tikzstyle{det} = [latent, diamond]

% Plate node
\tikzstyle{plate} = [draw, rectangle, rounded corners, fit=#1]
% Invisible wrapper node
\tikzstyle{wrap} = [inner sep=0pt, fit=#1]
% Gate
\tikzstyle{gate} = [draw, rectangle, dashed, fit=#1]

% Caption node
\tikzstyle{caption} = [font=\footnotesize, node distance=0] %
\tikzstyle{plate caption} = [caption, node distance=0, inner sep=0pt,
below left=5pt and 0pt of #1.south east] %
\tikzstyle{factor caption} = [caption] %
\tikzstyle{every label} += [caption] %

\tikzset{&gt;={triangle 45}}

%\pgfdeclarelayer{b}
%\pgfdeclarelayer{f}
%\pgfsetlayers{b,main,f}

% \factoredge [options] {inputs} {factors} {outputs}
\newcommand{\factoredge}[4][]{ %
  % Connect all nodes #2 to all nodes #4 via all factors #3.
  \foreach \f in {#3} { %
    \foreach \x in {#2} { %
      \draw[-,#1] (\x) edge[-] (\f) ; %
    } ;
    \foreach \y in {#4} { %
      \draw[-&gt;,#1] (\f) -- (\y) ; %
    } ;
  } ;
}

% \edge [options] {inputs} {outputs}
\newcommand{\edge}[3][]{ %
  % Connect all nodes #2 to all nodes #3.
  \foreach \x in {#2} { %
    \foreach \y in {#3} { %
      \draw[-&gt;,#1] (\x) -- (\y) ;%
    } ;
  } ;
}

% \factor [options] {name} {caption} {inputs} {outputs}
\newcommand{\factor}[5][]{ %
  % Draw the factor node. Use alias to allow empty names.
  \node[factor, label={[name=#2-caption]#3}, name=#2, #1,
  alias=#2-alias] {} ; %
  % Connect all inputs to outputs via this factor
  \factoredge {#4} {#2-alias} {#5} ; %
}

% \plate [options] {name} {fitlist} {caption}
\newcommand{\plate}[4][]{ %
  \node[wrap=#3] (#2-wrap) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
  \node[plate=(#2-wrap)(#2-caption), #1] (#2) {}; %
}

% \gate [options] {name} {fitlist} {inputs}
\newcommand{\gate}[4][]{ %
  \node[gate=#3, name=#2, #1, alias=#2-alias] {}; %
  \foreach \x in {#4} { %
    \draw [-*,thick] (\x) -- (#2-alias); %
  } ;%
}

% \vgate {name} {fitlist-left} {caption-left} {fitlist-right}
% {caption-right} {inputs}
\newcommand{\vgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-left) {}; %
  \node[wrap=#4] (#1-right) {}; %
  % Draw the gate
  \node[gate=(#1-left)(#1-right)] (#1) {}; %
  % Add captions
  \node[caption, below left=of #1.north ] (#1-left-caption)
  {#3}; %
  \node[caption, below right=of #1.north ] (#1-right-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.north) -- (#1.south); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

% \hgate {name} {fitlist-top} {caption-top} {fitlist-bottom}
% {caption-bottom} {inputs}
\newcommand{\hgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-top) {}; %
  \node[wrap=#4] (#1-bottom) {}; %
  % Draw the gate
  \node[gate=(#1-top)(#1-bottom)] (#1) {}; %
  % Add captions
  \node[caption, above right=of #1.west ] (#1-top-caption)
  {#3}; %
  \node[caption, below right=of #1.west ] (#1-bottom-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.west) -- (#1.east); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

\node[latent] (y) {$\mathbf{y}_{mn}$} ;
\node[latent, above left=1.8 and 0.4 of y] (mu) {$\boldsymbol{\mu}$} ;
\node[latent, above right=1.8 and 0.4 of y] (Lambda) {$\mathbf{\Lambda}$} ;
\factor[above=of y] {y-f} {left:$\mathcal{N}$} {mu,Lambda}     {y};
\plate {m-plate} {(y)(y-f)(y-f-caption)} {$m=0,\ldots,9$} ;
\plate {n-plate} {(m-plate)(m-plate-caption)} {$n=0,\ldots,29$} ;" /></p>
</div><p>The variable has two plates: one for the index <img class="math" src="_images/math/edba97b4c0d864d26e92ea7ea73767fa38eef3f7.svg" alt="m"/> and one for the
index <img class="math" src="_images/math/e11f2701c4a39c7fe543a6c4150b421d50f1c159.svg" alt="n"/>. In BayesPy, this random variable can be constructed
as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">30</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The plates are always given as a tuple of positive integers.</p>
</div>
<p>Plates also define indexing for the nodes, thus you can use simple NumPy-style
slice indexing to obtain a subset of the plates:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_0</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(30,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_even</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,::</span><span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_even</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(10, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_complex</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">:</span><span class="mi">20</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_complex</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(5, 2)</span>
</pre></div>
</div>
<p>Note that this indexing is for the plates only, not for the random variable
dimensions.</p>
</div>
<div class="section" id="sharing-and-broadcasting-plates">
<h5>Sharing and broadcasting plates<a class="headerlink" href="#sharing-and-broadcasting-plates" title="Permalink to this headline">¶</a></h5>
<p>Instead of having a common mean and precision matrix for all
<img class="math" src="_images/math/0b8d24e47b97db5308e62d423ea87a372308b73e.svg" alt="\mathbf{y}_{mn}"/>, it is also possible to share plates with parents. For
instance, the mean could be different for each index <img class="math" src="_images/math/edba97b4c0d864d26e92ea7ea73767fa38eef3f7.svg" alt="m"/> and the precision
for each index <img class="math" src="_images/math/e11f2701c4a39c7fe543a6c4150b421d50f1c159.svg" alt="n"/>:</p>
<div class="math">
<p><img src="_images/math/b2c2769c207cf489db01aa78e3b256c8b7facf32.svg" alt="\mathbf{y}_{mn} \sim \mathcal{N}(\boldsymbol{\mu}_m,
\mathbf{\Lambda}_n),\qquad m=0,\ldots,9, \quad n=0,\ldots,29."/></p>
</div><p>which has the following graphical representation:</p>
<div class="figure">
<p><img src="_images/tikz-97236981a2be663d10ade1ad85caa727621615db.png" alt="% tikzlibrary.code.tex
%
% Copyright 2010-2011 by Laura Dietz
% Copyright 2012 by Jaakko Luttinen
%
% This file may be distributed and/or modified
%
% 1. under the LaTeX Project Public License and/or
% 2. under the GNU General Public License.
%
% See the files LICENSE_LPPL and LICENSE_GPL for more details.

% Load other libraries
\usetikzlibrary{shapes}
\usetikzlibrary{fit}
\usetikzlibrary{chains}
\usetikzlibrary{arrows}

% Latent node
\tikzstyle{latent} = [circle,fill=white,draw=black,inner sep=1pt,
minimum size=20pt, font=\fontsize{10}{10}\selectfont, node distance=1]
% Observed node
\tikzstyle{obs} = [latent,fill=gray!25]
% Constant node
\tikzstyle{const} = [rectangle, inner sep=0pt, node distance=1]
% Factor node
\tikzstyle{factor} = [rectangle, fill=black,minimum size=5pt, inner
sep=0pt, node distance=0.4]
% Deterministic node
\tikzstyle{det} = [latent, diamond]

% Plate node
\tikzstyle{plate} = [draw, rectangle, rounded corners, fit=#1]
% Invisible wrapper node
\tikzstyle{wrap} = [inner sep=0pt, fit=#1]
% Gate
\tikzstyle{gate} = [draw, rectangle, dashed, fit=#1]

% Caption node
\tikzstyle{caption} = [font=\footnotesize, node distance=0] %
\tikzstyle{plate caption} = [caption, node distance=0, inner sep=0pt,
below left=5pt and 0pt of #1.south east] %
\tikzstyle{factor caption} = [caption] %
\tikzstyle{every label} += [caption] %

\tikzset{&gt;={triangle 45}}

%\pgfdeclarelayer{b}
%\pgfdeclarelayer{f}
%\pgfsetlayers{b,main,f}

% \factoredge [options] {inputs} {factors} {outputs}
\newcommand{\factoredge}[4][]{ %
  % Connect all nodes #2 to all nodes #4 via all factors #3.
  \foreach \f in {#3} { %
    \foreach \x in {#2} { %
      \draw[-,#1] (\x) edge[-] (\f) ; %
    } ;
    \foreach \y in {#4} { %
      \draw[-&gt;,#1] (\f) -- (\y) ; %
    } ;
  } ;
}

% \edge [options] {inputs} {outputs}
\newcommand{\edge}[3][]{ %
  % Connect all nodes #2 to all nodes #3.
  \foreach \x in {#2} { %
    \foreach \y in {#3} { %
      \draw[-&gt;,#1] (\x) -- (\y) ;%
    } ;
  } ;
}

% \factor [options] {name} {caption} {inputs} {outputs}
\newcommand{\factor}[5][]{ %
  % Draw the factor node. Use alias to allow empty names.
  \node[factor, label={[name=#2-caption]#3}, name=#2, #1,
  alias=#2-alias] {} ; %
  % Connect all inputs to outputs via this factor
  \factoredge {#4} {#2-alias} {#5} ; %
}

% \plate [options] {name} {fitlist} {caption}
\newcommand{\plate}[4][]{ %
  \node[wrap=#3] (#2-wrap) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
  \node[plate=(#2-wrap)(#2-caption), #1] (#2) {}; %
}

% \gate [options] {name} {fitlist} {inputs}
\newcommand{\gate}[4][]{ %
  \node[gate=#3, name=#2, #1, alias=#2-alias] {}; %
  \foreach \x in {#4} { %
    \draw [-*,thick] (\x) -- (#2-alias); %
  } ;%
}

% \vgate {name} {fitlist-left} {caption-left} {fitlist-right}
% {caption-right} {inputs}
\newcommand{\vgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-left) {}; %
  \node[wrap=#4] (#1-right) {}; %
  % Draw the gate
  \node[gate=(#1-left)(#1-right)] (#1) {}; %
  % Add captions
  \node[caption, below left=of #1.north ] (#1-left-caption)
  {#3}; %
  \node[caption, below right=of #1.north ] (#1-right-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.north) -- (#1.south); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

% \hgate {name} {fitlist-top} {caption-top} {fitlist-bottom}
% {caption-bottom} {inputs}
\newcommand{\hgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-top) {}; %
  \node[wrap=#4] (#1-bottom) {}; %
  % Draw the gate
  \node[gate=(#1-top)(#1-bottom)] (#1) {}; %
  % Add captions
  \node[caption, above right=of #1.west ] (#1-top-caption)
  {#3}; %
  \node[caption, below right=of #1.west ] (#1-bottom-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.west) -- (#1.east); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

\node[latent] (y) {$\mathbf{y}_{mn}$} ;
\node[latent, above left=1 and 2 of y] (mu) {$\boldsymbol{\mu}_m$} ;
\node[latent, above right=1 and 1 of y] (Lambda) {$\mathbf{\Lambda}_n$} ;
\factor[above=of y] {y-f} {above:$\mathcal{N}$} {mu,Lambda}     {y};
\plate {m-plate} {(mu)(y)(y-f)(y-f-caption)} {$m=0,\ldots,9$} ;
\plate {n-plate} {(Lambda)(y)(y-f)(y-f-caption)(m-plate-caption)(m-plate.north east)} {$n=0,\ldots,29$} ;" /></p>
</div><p>This can be constructed in BayesPy, for instance, as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">Wishart</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">1e-6</span><span class="p">,</span> <span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">]],</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">Wishart</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">30</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">)</span>
</pre></div>
</div>
<p>There are a few things to notice here. First, the plates are defined similarly
as shapes in NumPy, that is, they use similar broadcasting rules. For instance,
the plates <code class="docutils literal"><span class="pre">(10,1)</span></code> and <code class="docutils literal"><span class="pre">(1,30)</span></code> broadcast to <code class="docutils literal"><span class="pre">(10,30)</span></code>. In fact, one
could use plates <code class="docutils literal"><span class="pre">(10,1)</span></code> and <code class="docutils literal"><span class="pre">(30,)</span></code> to get the broadcasted plates
<code class="docutils literal"><span class="pre">(10,30)</span></code> because broadcasting compares the plates from right to left starting
from the last axis. Second, <code class="docutils literal"><span class="pre">X</span></code> is not given <code class="docutils literal"><span class="pre">plates</span></code> keyword argument
because the default plates are the plates broadcasted from the parents and that
was what we wanted so it was not necessary to provide the keyword argument. If
we wanted, for instance, plates <code class="docutils literal"><span class="pre">(20,10,30)</span></code> for <code class="docutils literal"><span class="pre">X</span></code>, then we would have
needed to provide <code class="docutils literal"><span class="pre">plates=(20,10,30)</span></code>.</p>
<p>The validity of the plates between a child and its parents is checked as
follows. The plates are compared plate-wise starting from the last axis and
working the way forward. A plate of the child is compatible with a plate of the
parent if either of the following conditions is met:</p>
<ol class="arabic simple">
<li>The two plates have equal size</li>
<li>The parent has size 1 (or no plate)</li>
</ol>
<p>Table below shows an example of compatible plates for a child node and
its two parent nodes:</p>
<table border="1" class="docutils">
<colgroup>
<col width="29%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="13%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">node</th>
<th class="head" colspan="7">plates</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>parent1</td>
<td>&#160;</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>8</td>
<td>10</td>
</tr>
<tr class="row-odd"><td>parent2</td>
<td>&#160;</td>
<td>&#160;</td>
<td>1</td>
<td>1</td>
<td>5</td>
<td>1</td>
<td>10</td>
</tr>
<tr class="row-even"><td>child</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>7</td>
<td>5</td>
<td>8</td>
<td>10</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="plates-in-deterministic-nodes">
<h5>Plates in deterministic nodes<a class="headerlink" href="#plates-in-deterministic-nodes" title="Permalink to this headline">¶</a></h5>
<p>Note that plates can be defined explicitly only for stochastic nodes.  For
deterministic nodes, the plates are defined implicitly by the plate broadcasting
rules from the parents. Deterministic nodes do not need more plates than this
because there is no randomness. The deterministic node would just have the same
value over the extra plates, but it is not necessary to do this explicitly
because the child nodes of the deterministic node can utilize broadcasting
anyway. Thus, there is no point in having extra plates in deterministic nodes,
and for this reason, deterministic nodes do not use <code class="docutils literal"><span class="pre">plates</span></code> keyword argument.</p>
</div>
<div class="section" id="plates-in-constants">
<h5>Plates in constants<a class="headerlink" href="#plates-in-constants" title="Permalink to this headline">¶</a></h5>
<p>It is useful to understand how the plates and the shape of a random variable are
connected. The shape of an array which contains all the plates of a random
variable is the concatenation of the plates and the shape of the variable. For
instance, consider a 2-dimensional Gaussian variable with plates <code class="docutils literal"><span class="pre">(3,)</span></code>. If
you want the value of the constant mean vector and constant precision matrix to
vary between plates, they are given as <code class="docutils literal"><span class="pre">(3,2)</span></code>-shape and <code class="docutils literal"><span class="pre">(3,2,2)</span></code>-shape
arrays, respectively:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
<span class="gp">... </span>            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span>
<span class="gp">... </span>           <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
<span class="gp">... </span>            <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span>
<span class="gp">... </span>           <span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>            <span class="p">[</span><span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span> <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
<span class="go">(3, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Lambda</span><span class="p">)</span>
<span class="go">(3, 2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(3,)</span>
</pre></div>
</div>
<p>Thus, the leading axes of an array are the plate axes and the trailing axes are
the random variable axes. In the example above, the mean vector has plates
<code class="docutils literal"><span class="pre">(3,)</span></code> and shape <code class="docutils literal"><span class="pre">(2,)</span></code>, and the precision matrix has plates <code class="docutils literal"><span class="pre">(3,)</span></code> and
shape <code class="docutils literal"><span class="pre">(2,2)</span></code>.</p>
</div>
<div class="section" id="factorization-of-plates">
<h5>Factorization of plates<a class="headerlink" href="#factorization-of-plates" title="Permalink to this headline">¶</a></h5>
<p>It is important to undestand the independency structure the plates induce for
the model. First, the repetitions defined by a plate are independent a priori
given the parents. Second, the repetitions are independent in the posterior
approximation, that is, the posterior approximation factorizes with respect to
plates. Thus, the plates also have an effect on the independence structure of
the posterior approximation, not only prior. If dependencies between a set of
variables need to be handled, that set must be handled as a some kind of
multi-dimensional variable.</p>
</div>
<div class="section" id="irregular-plates">
<span id="sec-irregular-plates"></span><h5>Irregular plates<a class="headerlink" href="#irregular-plates" title="Permalink to this headline">¶</a></h5>
<p>The handling of plates is not always as simple as described above. There are
cases in which the plates of the parents do not map directly to the plates of
the child node. The user API should mention such irregularities.</p>
<p>For instance, the parents of a mixture distribution have a plate which contains
the different parameters for each cluster, but the variable from the mixture
distribution does not have that plate:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">Wishart</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">Mixture</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">Wishart</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(3,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(3,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(100,)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(100,)</span>
</pre></div>
</div>
<p>The plates <code class="docutils literal"><span class="pre">(3,)</span></code> and <code class="docutils literal"><span class="pre">(100,)</span></code> should not broadcast according to the rules
mentioned above. However, when validating the plates, <a class="reference internal" href="index.html#bayespy.nodes.Mixture" title="bayespy.nodes.Mixture"><code class="xref py py-class docutils literal"><span class="pre">Mixture</span></code></a> removes
the plate which corresponds to the clusters in <code class="docutils literal"><span class="pre">mu</span></code> and <code class="docutils literal"><span class="pre">Lambda</span></code>. Thus,
<code class="docutils literal"><span class="pre">X</span></code> has plates which are the result of broadcasting plates <code class="docutils literal"><span class="pre">()</span></code> and
<code class="docutils literal"><span class="pre">(100,)</span></code> which equals <code class="docutils literal"><span class="pre">(100,)</span></code>.</p>
<p>Also, sometimes the plates of the parents may be mapped to the variable
axes. For instance, an automatic relevance determination (ARD) prior for a
Gaussian variable is constructed by giving the diagonal elements of the
precision matrix (or tensor). The Gaussian variable itself can be a scalar, a
vector, a matrix or a tensor. A set of five <img class="math" src="_images/math/2cdfa26d1bb24b4b1dea4c028b73771c0443b5a9.svg" alt="4 \times 3"/>
-dimensional Gaussian matrices with ARD prior is constructed as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">GaussianARD</span><span class="p">,</span> <span class="n">Gamma</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(5, 4, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(5,)</span>
</pre></div>
</div>
<p>Note how the last two plate axes of <code class="docutils literal"><span class="pre">tau</span></code> are mapped to the variable axes of
<code class="docutils literal"><span class="pre">X</span></code> with shape <code class="docutils literal"><span class="pre">(4,3)</span></code> and the plates of <code class="docutils literal"><span class="pre">X</span></code> are obtained by taking the
remaining leading plate axes of <code class="docutils literal"><span class="pre">tau</span></code>.</p>
</div>
</div>
<div class="section" id="example-model-principal-component-analysis">
<h4>Example model: Principal component analysis<a class="headerlink" href="#example-model-principal-component-analysis" title="Permalink to this headline">¶</a></h4>
<p>Now, we&#8217;ll construct a bit more complex model which will be used in the
following sections.  The model is a probabilistic version of principal component
analysis (PCA):</p>
<div class="math">
<p><img src="_images/math/17ac2f5833891a9af83786b551d1f8c421c271e6.svg" alt="\mathbf{Y} = \mathbf{C}\mathbf{X}^T + \mathrm{noise}"/></p>
</div><p>where <img class="math" src="_images/math/882e6721a5df0caf9a878f7d49deec991c5aacae.svg" alt="\mathbf{Y}"/> is <img class="math" src="_images/math/3a627f65b957a24dc0781c4deaf8304ee8d34b48.svg" alt="M\times N"/> data matrix, <img class="math" src="_images/math/4b332c34a61a965d9977bc36f6619b062c4e239f.svg" alt="\mathbf{C}"/> is
<img class="math" src="_images/math/a5f6bcf6f09bf05ae9d5c5a1ad520e4b26d93368.svg" alt="M\times D"/> loading matrix, <img class="math" src="_images/math/c20f7f9210a91b8aeba2e85d2bcb23e29bcab3f4.svg" alt="\mathbf{X}"/> is <img class="math" src="_images/math/a2aae0c59716b3a501ef0ce4c0d686af08dc5fbe.svg" alt="N\times D"/> state
matrix, and noise is isotropic Gaussian.  The dimensionality <img class="math" src="_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.svg" alt="D"/> is
usually assumed to be much smaller than <img class="math" src="_images/math/450a8e2c2320d77181e0d4fc68c947e9a5de8ecb.svg" alt="M"/> and <img class="math" src="_images/math/f4170ed8938b79490d8923857962695514a8e4cb.svg" alt="N"/>.</p>
<p>A probabilistic formulation can be written as:</p>
<div class="math">
<p><img src="_images/math/6fbe330ee6fb365cab57abc6f3ceb12bf7d36ef3.svg" alt="p(\mathbf{Y}) &amp;= \prod^{M-1}_{m=0} \prod^{N-1}_{n=0} \mathcal{N}(y_{mn} |
\mathbf{c}_m^T \mathbf{x}_n, \tau)
\\
p(\mathbf{X}) &amp;= \prod^{N-1}_{n=0} \prod^{D-1}_{d=0} \mathcal{N}(x_{nd} |
0, 1)
\\
p(\mathbf{C}) &amp;= \prod^{M-1}_{m=0} \prod^{D-1}_{d=0} \mathcal{N}(c_{md} |
0, \alpha_d)
\\
p(\boldsymbol{\alpha}) &amp;= \prod^{D-1}_{d=0} \mathcal{G} (\alpha_d | 10^{-3},
10^{-3})
\\
p(\tau) &amp;= \mathcal{G} (\tau | 10^{-3}, 10^{-3})"/></p>
</div><p>where we have given automatic relevance determination (ARD) prior for
<img class="math" src="_images/math/4b332c34a61a965d9977bc36f6619b062c4e239f.svg" alt="\mathbf{C}"/>.  This can be visualized as a graphical model:</p>
<div class="figure">
<p><img src="_images/tikz-7c950fa428df90525eaecdf9647b397a4a3426b1.png" alt="% tikzlibrary.code.tex
%
% Copyright 2010-2011 by Laura Dietz
% Copyright 2012 by Jaakko Luttinen
%
% This file may be distributed and/or modified
%
% 1. under the LaTeX Project Public License and/or
% 2. under the GNU General Public License.
%
% See the files LICENSE_LPPL and LICENSE_GPL for more details.

% Load other libraries
\usetikzlibrary{shapes}
\usetikzlibrary{fit}
\usetikzlibrary{chains}
\usetikzlibrary{arrows}

% Latent node
\tikzstyle{latent} = [circle,fill=white,draw=black,inner sep=1pt,
minimum size=20pt, font=\fontsize{10}{10}\selectfont, node distance=1]
% Observed node
\tikzstyle{obs} = [latent,fill=gray!25]
% Constant node
\tikzstyle{const} = [rectangle, inner sep=0pt, node distance=1]
% Factor node
\tikzstyle{factor} = [rectangle, fill=black,minimum size=5pt, inner
sep=0pt, node distance=0.4]
% Deterministic node
\tikzstyle{det} = [latent, diamond]

% Plate node
\tikzstyle{plate} = [draw, rectangle, rounded corners, fit=#1]
% Invisible wrapper node
\tikzstyle{wrap} = [inner sep=0pt, fit=#1]
% Gate
\tikzstyle{gate} = [draw, rectangle, dashed, fit=#1]

% Caption node
\tikzstyle{caption} = [font=\footnotesize, node distance=0] %
\tikzstyle{plate caption} = [caption, node distance=0, inner sep=0pt,
below left=5pt and 0pt of #1.south east] %
\tikzstyle{factor caption} = [caption] %
\tikzstyle{every label} += [caption] %

\tikzset{&gt;={triangle 45}}

%\pgfdeclarelayer{b}
%\pgfdeclarelayer{f}
%\pgfsetlayers{b,main,f}

% \factoredge [options] {inputs} {factors} {outputs}
\newcommand{\factoredge}[4][]{ %
  % Connect all nodes #2 to all nodes #4 via all factors #3.
  \foreach \f in {#3} { %
    \foreach \x in {#2} { %
      \draw[-,#1] (\x) edge[-] (\f) ; %
    } ;
    \foreach \y in {#4} { %
      \draw[-&gt;,#1] (\f) -- (\y) ; %
    } ;
  } ;
}

% \edge [options] {inputs} {outputs}
\newcommand{\edge}[3][]{ %
  % Connect all nodes #2 to all nodes #3.
  \foreach \x in {#2} { %
    \foreach \y in {#3} { %
      \draw[-&gt;,#1] (\x) -- (\y) ;%
    } ;
  } ;
}

% \factor [options] {name} {caption} {inputs} {outputs}
\newcommand{\factor}[5][]{ %
  % Draw the factor node. Use alias to allow empty names.
  \node[factor, label={[name=#2-caption]#3}, name=#2, #1,
  alias=#2-alias] {} ; %
  % Connect all inputs to outputs via this factor
  \factoredge {#4} {#2-alias} {#5} ; %
}

% \plate [options] {name} {fitlist} {caption}
\newcommand{\plate}[4][]{ %
  \node[wrap=#3] (#2-wrap) {}; %
  \node[plate caption=#2-wrap] (#2-caption) {#4}; %
  \node[plate=(#2-wrap)(#2-caption), #1] (#2) {}; %
}

% \gate [options] {name} {fitlist} {inputs}
\newcommand{\gate}[4][]{ %
  \node[gate=#3, name=#2, #1, alias=#2-alias] {}; %
  \foreach \x in {#4} { %
    \draw [-*,thick] (\x) -- (#2-alias); %
  } ;%
}

% \vgate {name} {fitlist-left} {caption-left} {fitlist-right}
% {caption-right} {inputs}
\newcommand{\vgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-left) {}; %
  \node[wrap=#4] (#1-right) {}; %
  % Draw the gate
  \node[gate=(#1-left)(#1-right)] (#1) {}; %
  % Add captions
  \node[caption, below left=of #1.north ] (#1-left-caption)
  {#3}; %
  \node[caption, below right=of #1.north ] (#1-right-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.north) -- (#1.south); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

% \hgate {name} {fitlist-top} {caption-top} {fitlist-bottom}
% {caption-bottom} {inputs}
\newcommand{\hgate}[6]{ %
  % Wrap the left and right parts
  \node[wrap=#2] (#1-top) {}; %
  \node[wrap=#4] (#1-bottom) {}; %
  % Draw the gate
  \node[gate=(#1-top)(#1-bottom)] (#1) {}; %
  % Add captions
  \node[caption, above right=of #1.west ] (#1-top-caption)
  {#3}; %
  \node[caption, below right=of #1.west ] (#1-bottom-caption)
  {#5}; %
  % Draw middle separation
  \draw [-, dashed] (#1.west) -- (#1.east); %
  % Draw inputs
  \foreach \x in {#6} { %
    \draw [-*,thick] (\x) -- (#1); %
  } ;%
}

\node[latent] (y) {$\mathbf{y}_{mn}$} ;
\node[det, above=of y] (dot) {dot} ;
\node[latent, right=2 of dot] (tau) {$\tau$} ;
\node[latent, above left=1 and 2 of dot] (C) {$c_{md}$} ;
\node[latent, above=of C] (alpha) {$\alpha_d$} ;
\node[latent, above right=1 and 1 of dot] (X) {$x_{nd}$} ;

\factor[above=of y] {y-f} {left:$\mathcal{N}$} {dot,tau} {y};
\factor[above=of C] {C-f} {left:$\mathcal{N}$} {alpha} {C};
\factor[above=of X] {X-f} {above:$\mathcal{N}$} {} {X};
\factor[above=of alpha] {alpha-f} {above:$\mathcal{G}$} {} {alpha};
\factor[above=of tau] {tau-f} {above:$\mathcal{G}$} {} {tau};
\edge {C,X} {dot};

\tikzstyle{plate caption} += [below left=0pt and 0pt of #1.north east] ;
\plate {d-plate} {(X)(X-f)(X-f-caption)(C)(C-f)(C-f-caption)(alpha)(alpha-f)(alpha-f-caption)} {$d=0,\ldots,2$} ;
\tikzstyle{plate caption} += [below left=5pt and 0pt of #1.south east] ;
\plate {m-plate} {(y)(y-f)(y-f-caption)(C)(C-f)(C-f-caption)(d-plate.south west)} {$m=0,\ldots,9$} ;
\plate {n-plate} {(y)(y-f)(y-f-caption)(X)(X-f)(X-f-caption)(m-plate-caption)(m-plate.north east)(d-plate.south east)} {$n=0,\ldots,99$} ;" /></p>
</div><p>Now, let us construct this model in BayesPy.  First, we&#8217;ll define the
dimensionality of the latent space in our model:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
<p>Then the prior for the latent states <img class="math" src="_images/math/c20f7f9210a91b8aeba2e85d2bcb23e29bcab3f4.svg" alt="\mathbf{X}"/>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the shape of <code class="docutils literal"><span class="pre">X</span></code> is <code class="docutils literal"><span class="pre">(D,)</span></code>, although the latent dimensions are
marked with a plate in the graphical model and they are conditionally
independent in the prior.  However, we want to (and need to) model the posterior
dependency of the latent dimensions, thus we cannot factorize them, which would
happen if we used <code class="docutils literal"><span class="pre">plates=(1,100,D)</span></code> and <code class="docutils literal"><span class="pre">shape=()</span></code>.  The first plate axis
with size 1 is given just for clarity.</p>
<p>The prior for the ARD parameters <img class="math" src="_images/math/90fce749e46a02b9b85d743473fd32d7f691b529.svg" alt="\boldsymbol{\alpha}"/> of the loading
matrix:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The prior for the loading matrix <img class="math" src="_images/math/4b332c34a61a965d9977bc36f6619b062c4e239f.svg" alt="\mathbf{C}"/>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Again, note that the shape is the same as for <code class="docutils literal"><span class="pre">X</span></code> for the same reason.  Also,
the plates of <code class="docutils literal"><span class="pre">alpha</span></code>, <code class="docutils literal"><span class="pre">(D,)</span></code>, are mapped to the full shape of the node
<code class="docutils literal"><span class="pre">C</span></code>, <code class="docutils literal"><span class="pre">(10,1,D)</span></code>, using standard broadcasting rules.</p>
<p>The dot product is just a deterministic node:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>However, note that <code class="docutils literal"><span class="pre">Dot</span></code> requires that the input Gaussian nodes have the same
shape and that this shape has exactly one axis, that is, the variables are
vectors.  This the reason why we used shape <code class="docutils literal"><span class="pre">(D,)</span></code> for <code class="docutils literal"><span class="pre">X</span></code> and <code class="docutils literal"><span class="pre">C</span></code> but
from a bit different perspective.  The node computes the inner product of
<img class="math" src="_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.svg" alt="D"/>-dimensional vectors resulting in plates <code class="docutils literal"><span class="pre">(10,100)</span></code> broadcasted from
the plates <code class="docutils literal"><span class="pre">(1,100)</span></code> and <code class="docutils literal"><span class="pre">(10,1)</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(10, 100)</span>
</pre></div>
</div>
<p>The prior for the observation noise <img class="math" src="_images/math/557d5fad862c9046d26e1a930f45a550c146d592.svg" alt="\tau"/>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tau&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, the observations are conditionally independent Gaussian scalars:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we have defined our model and the next step is to observe some data and to
perform inference.</p>
</div>
</div>
<span id="document-user_guide/inference"></span><div class="section" id="performing-inference">
<h3>Performing inference<a class="headerlink" href="#performing-inference" title="Permalink to this headline">¶</a></h3>
<p>Approximation of the posterior distribution can be divided into several steps:</p>
<ul class="simple">
<li>Observe some nodes</li>
<li>Choose the inference engine</li>
<li>Initialize the posterior approximation</li>
<li>Run the inference algorithm</li>
</ul>
<p>In order to illustrate these steps, we&#8217;ll be using the PCA model constructed in
the previous section.</p>
<div class="section" id="observing-nodes">
<h4>Observing nodes<a class="headerlink" href="#observing-nodes" title="Permalink to this headline">¶</a></h4>
<p>First, let us generate some toy data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>The data is provided by simply calling <code class="docutils literal"><span class="pre">observe</span></code> method of a stochastic node:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>It is important that the shape of the <code class="docutils literal"><span class="pre">data</span></code> array matches the plates and
shape of the node <code class="docutils literal"><span class="pre">Y</span></code>.  For instance, if <code class="docutils literal"><span class="pre">Y</span></code> was <code class="xref py py-class docutils literal"><span class="pre">Wishart</span></code> node for
<img class="math" src="_images/math/5e3cfcce6f9932d7aa9d734e989a3976ad13cbff.svg" alt="3\times 3"/> matrices with plates <code class="docutils literal"><span class="pre">(5,1,10)</span></code>, the full shape of <code class="docutils literal"><span class="pre">Y</span></code>
would be <code class="docutils literal"><span class="pre">(5,1,10,3,3)</span></code>.  The <code class="docutils literal"><span class="pre">data</span></code> array should have this shape exactly,
that is, no broadcasting rules are applied.</p>
<div class="section" id="missing-values">
<h5>Missing values<a class="headerlink" href="#missing-values" title="Permalink to this headline">¶</a></h5>
<p>It is possible to mark missing values by providing a mask which is a boolean
array:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="p">[[</span><span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span>
<span class="gp">... </span>                      <span class="p">[</span><span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">],</span> <span class="p">[</span><span class="kc">False</span><span class="p">]])</span>
</pre></div>
</div>
<p><code class="docutils literal"><span class="pre">True</span></code> means that the value is observed and <code class="docutils literal"><span class="pre">False</span></code> means that the value is
missing.  The shape of the above mask is <code class="docutils literal"><span class="pre">(10,1)</span></code>, which broadcasts to the
plates of Y, <code class="docutils literal"><span class="pre">(10,100)</span></code>.  Thus, the above mask means that the second, third,
sixth and tenth rows of the <img class="math" src="_images/math/34e6cefd6c5ba42ab86ac364a5b0e60c8946f75c.svg" alt="10\times 100"/> data matrix are missing.</p>
<p>The mask is applied to the <em>plates</em>, not to the data array directly.  This means
that it is not possible to observe a random variable partially, each repetition
defined by the plates is either fully observed or fully missing.  Thus, the mask
is applied to the plates.  It is often possible to circumvent this seemingly
tight restriction by adding an observable child node which factorizes more.</p>
<p>The shape of the mask is broadcasted to plates using standard NumPy broadcasting
rules. So, if the variable has plates <code class="docutils literal"><span class="pre">(5,1,10)</span></code>, the mask could have a shape
<code class="docutils literal"><span class="pre">()</span></code>, <code class="docutils literal"><span class="pre">(1,)</span></code>, <code class="docutils literal"><span class="pre">(1,1)</span></code>, <code class="docutils literal"><span class="pre">(1,1,1)</span></code>, <code class="docutils literal"><span class="pre">(10,)</span></code>, <code class="docutils literal"><span class="pre">(1,10)</span></code>, <code class="docutils literal"><span class="pre">(1,1,10)</span></code>,
<code class="docutils literal"><span class="pre">(5,1,1)</span></code> or <code class="docutils literal"><span class="pre">(5,1,10)</span></code>.  In order to speed up the inference, missing values
are automatically integrated out if they are not needed as latent variables to
child nodes.  This leads to faster convergence and more accurate approximations.</p>
</div>
</div>
<div class="section" id="choosing-the-inference-method">
<h4>Choosing the inference method<a class="headerlink" href="#choosing-the-inference-method" title="Permalink to this headline">¶</a></h4>
<p>Inference methods can be found in <a class="reference internal" href="index.html#module-bayespy.inference" title="bayespy.inference"><code class="xref py py-mod docutils literal"><span class="pre">bayespy.inference</span></code></a> package.  Currently,
only variational Bayesian approximation is implemented
(<a class="reference internal" href="index.html#bayespy.inference.VB" title="bayespy.inference.VB"><code class="xref py py-class docutils literal"><span class="pre">bayespy.inference.VB</span></code></a>).  The inference engine is constructed by giving
the stochastic nodes of the model.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>There is no need to give any deterministic nodes.  Currently, the inference
engine does not automatically search for stochastic parents and children, thus
it is important that all stochastic nodes of the model are given.  This should
be made more robust in future versions.</p>
<p>A node of the model can be obtained by using the name of the node as a key:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span>
<span class="go">&lt;bayespy.inference.vmp.nodes.gaussian.GaussianARD object at 0x...&gt;</span>
</pre></div>
</div>
<p>Note that the returned object is the same as the node object itself:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="n">X</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Thus, one may use the object <code class="docutils literal"><span class="pre">X</span></code> when it is available.  However, if the model
and the inference engine are constructed in another function or module, the node
object may not be available directly and this feature becomes useful.</p>
</div>
<div class="section" id="initializing-the-posterior-approximation">
<h4>Initializing the posterior approximation<a class="headerlink" href="#initializing-the-posterior-approximation" title="Permalink to this headline">¶</a></h4>
<p>The inference engines give some initialization to the stochastic nodes by
default.  However, the inference algorithms can be sensitive to the
initialization, thus it is sometimes necessary to have better control over the
initialization.  For VB, the following initialization methods are available:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">initialize_from_prior</span></code>: Use the current states of the parent nodes to
update the node. This is the default initialization.</li>
<li><code class="docutils literal"><span class="pre">initialize_from_parameters</span></code>: Use the given parameter values for the
distribution.</li>
<li><code class="docutils literal"><span class="pre">initialize_from_value</span></code>: Use the given value for the variable.</li>
<li><code class="docutils literal"><span class="pre">initialize_from_random</span></code>: Draw a random value for the variable.  The random
sample is drawn from the current state of the node&#8217;s distribution.</li>
</ul>
<p>Note that <code class="docutils literal"><span class="pre">initialize_from_value</span></code> and <code class="docutils literal"><span class="pre">initialize_from_random</span></code> initialize
the distribution with a value of the variable instead of parameters of the
distribution.  Thus, the distribution is actually a delta distribution with a
peak on the value after the initialization.  This state of the distribution does
not have proper natural parameter values nor normalization, thus the VB lower
bound terms are <code class="docutils literal"><span class="pre">np.nan</span></code> for this initial state.</p>
<p>These initialization methods can be used to perform even a bit more complex
initializations.  For instance, a Gaussian distribution could be initialized
with a random mean and variance 0.1.  In our PCA model, this can be obtained by</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">initialize_from_parameters</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the shape of the random mean is the sum of the plates <code class="docutils literal"><span class="pre">(1,</span> <span class="pre">100)</span></code> and
the variable shape <code class="docutils literal"><span class="pre">(D,)</span></code>.  In addition, instead of variance,
<code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code> uses precision as the second parameter, thus we initialized
the variance to <img class="math" src="_images/math/de9b5660ca8686a252af43a89ebec581b6f4e1cd.svg" alt="\frac{1}{10}"/>.  This random initialization is important
in our PCA model because the default initialization gives <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">X</span></code> zero
mean.  If the mean of the other variable was zero when the other is updated, the
other variable gets zero mean too.  This would lead to an update algorithm where
both means remain zeros and effectively no latent space is found.  Thus, it is
important to give non-zero random initialization for <code class="docutils literal"><span class="pre">X</span></code> if <code class="docutils literal"><span class="pre">C</span></code> is updated
before <code class="docutils literal"><span class="pre">X</span></code> the first time.  It is typical that at least some nodes need be
initialized with some randomness.</p>
<p>By default, nodes are initialized with the method <code class="docutils literal"><span class="pre">initialize_from_prior</span></code>.
The method is not very time consuming but if for any reason you want to avoid
that default initialization computation, you can provide <code class="docutils literal"><span class="pre">initialize=False</span></code>
when creating the stochastic node.  However, the node does not have a proper
state in that case, which leads to errors in VB learning unless the distribution
is initialized using the above methods.</p>
</div>
<div class="section" id="running-the-inference-algorithm">
<h4>Running the inference algorithm<a class="headerlink" href="#running-the-inference-algorithm" title="Permalink to this headline">¶</a></h4>
<p>The approximation methods are based on iterative algorithms, which can
be run using <code class="docutils literal"><span class="pre">update</span></code> method. By default, it takes one iteration step
updating all nodes once:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
<span class="go">Iteration 1: loglike=-9.305259e+02 (... seconds)</span>
</pre></div>
</div>
<p>The <code class="docutils literal"><span class="pre">loglike</span></code> tells the VB lower bound.  The order in which the nodes are
updated is the same as the order in which the nodes were given when creating
<code class="docutils literal"><span class="pre">Q</span></code>.  If you want to change the order or update only some of the nodes, you
can give as arguments the nodes you want to update and they are updated in the
given order:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="go">Iteration 2: loglike=-8.818976e+02 (... seconds)</span>
</pre></div>
</div>
<p>It is also possible to give the same node several times:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="go">Iteration 3: loglike=-8.071222e+02 (... seconds)</span>
</pre></div>
</div>
<p>Note that each call to <code class="docutils literal"><span class="pre">update</span></code> is counted as one iteration step although not
variables are necessarily updated.  Instead of doing one iteration step,
<code class="docutils literal"><span class="pre">repeat</span></code> keyword argument can be used to perform several iteration steps:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="go">Iteration 4: loglike=-7.167588e+02 (... seconds)</span>
<span class="go">Iteration 5: loglike=-6.827873e+02 (... seconds)</span>
<span class="go">Iteration 6: loglike=-6.259477e+02 (... seconds)</span>
<span class="go">Iteration 7: loglike=-4.725400e+02 (... seconds)</span>
<span class="go">Iteration 8: loglike=-3.270816e+02 (... seconds)</span>
<span class="go">Iteration 9: loglike=-2.208865e+02 (... seconds)</span>
<span class="go">Iteration 10: loglike=-1.658761e+02 (... seconds)</span>
<span class="go">Iteration 11: loglike=-1.469468e+02 (... seconds)</span>
<span class="go">Iteration 12: loglike=-1.420311e+02 (... seconds)</span>
<span class="go">Iteration 13: loglike=-1.405139e+02 (... seconds)</span>
</pre></div>
</div>
<p>The VB algorithm stops automatically if it converges, that is, the relative
change in the lower bound is below some threshold:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 14: loglike=-1.396481e+02 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 488: loglike=-1.224106e+02 (... seconds)</span>
<span class="go">Converged at iteration 488.</span>
</pre></div>
</div>
<p>Now the algorithm stopped before taking 1000 iteration steps because it
converged.  The relative tolerance can be adjusted by providing <code class="docutils literal"><span class="pre">tol</span></code> keyword
argument to the <code class="docutils literal"><span class="pre">update</span></code> method:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="go">Iteration 489: loglike=-1.224094e+02 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 847: loglike=-1.222506e+02 (... seconds)</span>
<span class="go">Converged at iteration 847.</span>
</pre></div>
</div>
<p>Making the tolerance smaller, may improve the result but it may also
significantly increase the iteration steps until convergence.</p>
<p>Instead of using <code class="docutils literal"><span class="pre">update</span></code> method of the inference engine <code class="docutils literal"><span class="pre">VB</span></code>, it is
possible to use the <code class="docutils literal"><span class="pre">update</span></code> methods of the nodes directly as</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
<p>However, this is not recommended, because the <code class="docutils literal"><span class="pre">update</span></code> method of the inference
engine <code class="docutils literal"><span class="pre">VB</span></code> is a wrapper which, in addition to calling the nodes&#8217; <code class="docutils literal"><span class="pre">update</span></code>
methods, checks for convergence and does a few other useful minor things.  But
if for any reason these direct update methods are needed, they can be used.</p>
<div class="section" id="parameter-expansion">
<span id="sec-parameter-expansion"></span><h5>Parameter expansion<a class="headerlink" href="#parameter-expansion" title="Permalink to this headline">¶</a></h5>
<p>Sometimes the VB algorithm converges very slowly.  This may happen when the
variables are strongly coupled in the true posterior but factorized in the
approximate posterior.  This coupling leads to zigzagging of the variational
parameters which progresses slowly.  One solution to this problem is to use
parameter expansion.  The idea is to add an auxiliary variable which
parameterizes the posterior approximation of several variables.  Then optimizing
this auxiliary variable actually optimizes several posterior approximations
jointly leading to faster convergence.</p>
<p>The parameter expansion is model specific.  Currently in BayesPy, only
state-space models have built-in parameter expansions available.  These
state-space models contain a variable which is a dot product of two variables
(plus some noise):</p>
<div class="math">
<p><img src="_images/math/f6cb0d904f7213a17a1b9a53f15c5a32322b9fc5.svg" alt="y = \mathbf{c}^T\mathbf{x} + \mathrm{noise}"/></p>
</div><p>The parameter expansion can be motivated by noticing that we can add an
auxiliary variable which rotates the variables <img class="math" src="_images/math/86c5269f5d1ecdd2c651815e55e2d19febcb5ad4.svg" alt="\mathbf{c}"/> and
<img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/> so that the dot product is unaffected:</p>
<div class="math">
<p><img src="_images/math/503a5cd2f9a7539c469d321266c6f6e961139d67.svg" alt="y &amp;= \mathbf{c}^T\mathbf{x} + \mathrm{noise}
= \mathbf{c}^T \mathbf{R} \mathbf{R}^{-1}\mathbf{x} + \mathrm{noise}
= (\mathbf{R}^T\mathbf{c})^T(\mathbf{R}^{-1}\mathbf{x}) + \mathrm{noise}"/></p>
</div><p>Now, applying this rotation to the posterior approximations
<img class="math" src="_images/math/f53056e7f5405455c846c57eee4673a7b0464e1b.svg" alt="q(\mathbf{c})"/> and <img class="math" src="_images/math/20d88f57622ea4838c6d437a74878999bfc9cb38.svg" alt="q(\mathbf{x})"/>, and optimizing the VB lower
bound with respect to the rotation leads to parameterized joint optimization of
<img class="math" src="_images/math/86c5269f5d1ecdd2c651815e55e2d19febcb5ad4.svg" alt="\mathbf{c}"/> and <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>.</p>
<p>The available parameter expansion methods are in module <code class="docutils literal"><span class="pre">transformations</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference.vmp</span> <span class="k">import</span> <span class="n">transformations</span>
</pre></div>
</div>
<p>First, you create the rotation transformations for the two variables:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rotX</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rotC</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, the rotation for <code class="docutils literal"><span class="pre">C</span></code> provides the ARD parameters <code class="docutils literal"><span class="pre">alpha</span></code> so they are
updated simultaneously.  In addition to <a class="reference internal" href="index.html#bayespy.inference.vmp.transformations.RotateGaussianARD" title="bayespy.inference.vmp.transformations.RotateGaussianARD"><code class="xref py py-class docutils literal"><span class="pre">RotateGaussianARD</span></code></a>, there are a
few other built-in rotations defined, for instance, <a class="reference internal" href="index.html#bayespy.inference.vmp.transformations.RotateGaussian" title="bayespy.inference.vmp.transformations.RotateGaussian"><code class="xref py py-class docutils literal"><span class="pre">RotateGaussian</span></code></a> and
<a class="reference internal" href="index.html#bayespy.inference.vmp.transformations.RotateGaussianMarkovChain" title="bayespy.inference.vmp.transformations.RotateGaussianMarkovChain"><code class="xref py py-class docutils literal"><span class="pre">RotateGaussianMarkovChain</span></code></a>.  It is extremely important that the model
satisfies the assumptions made by the rotation class and the user is mostly
responsible for this.  The optimizer for the rotations is constructed by giving
the two rotations and the dimensionality of the rotated space:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">R</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotationOptimizer</span><span class="p">(</span><span class="n">rotC</span><span class="p">,</span> <span class="n">rotX</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, calling <code class="docutils literal"><span class="pre">rotate</span></code> method will find optimal rotation and update the
relevant nodes (<code class="docutils literal"><span class="pre">X</span></code>, <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">alpha</span></code>) accordingly:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">R</span><span class="o">.</span><span class="n">rotate</span><span class="p">()</span>
</pre></div>
</div>
<p>Let us see how our iteration would have gone if we had used this parameter
expansion.  First, let us re-initialize our nodes and VB algorithm:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span><span class="o">.</span><span class="n">initialize_from_prior</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">initialize_from_prior</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">initialize_from_parameters</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">D</span><span class="p">),</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span><span class="o">.</span><span class="n">initialize_from_prior</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, the rotation is set to run after each iteration step:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">rotate</span>
</pre></div>
</div>
<p>Now the iteration converges to the relative tolerance <img class="math" src="_images/math/a14afca4b427a51a91a3e8ce6ae0f710c314ab29.svg" alt="10^{-6}"/> much
faster:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-9.363...e+02 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 18: loglike=-1.221354e+02 (... seconds)</span>
<span class="go">Converged at iteration 18.</span>
</pre></div>
</div>
<p>The convergence took 18 iterations with rotations and 488 or 847 iterations
without the parameter expansion.  In addition, the lower bound is improved
slightly.  One can compare the number of iteration steps in this case because
the cost per iteration step with or without parameter expansion is approximately
the same.  Sometimes the parameter expansion can have the drawback that it
converges to a bad local optimum.  Usually, this can be solved by updating the
nodes near the observations a few times before starting to update the
hyperparameters and to use parameter expansion.  In any case, the parameter
expansion is practically necessary when using state-space models in order to
converge to a proper solution in a reasonable time.</p>
</div>
</div>
</div>
<span id="document-user_guide/plot"></span><div class="section" id="examining-the-results">
<h3>Examining the results<a class="headerlink" href="#examining-the-results" title="Permalink to this headline">¶</a></h3>
<p>After the results have been obtained, it is important to be able to examine the
results easily.  The results can be examined either numerically by inspecting
numerical arrays or visually by plotting distributions of the nodes.  In
addition, the posterior distributions can be visualized during the learning
algorithm and the results can saved into a file.</p>
<div class="section" id="plotting-the-results">
<h4>Plotting the results<a class="headerlink" href="#plotting-the-results" title="Permalink to this headline">¶</a></h4>
<p>The module <code class="xref py py-mod docutils literal"><span class="pre">plot</span></code> offers some plotting basic functionality:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
</pre></div>
</div>
<p>The module contains <code class="docutils literal"><span class="pre">matplotlib.pyplot</span></code> module if the user needs that.  For
instance, interactive plotting can be enabled as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">ion</span><span class="p">()</span>
</pre></div>
</div>
<p>The <code class="xref py py-mod docutils literal"><span class="pre">plot</span></code> module contains some functions but it is not a very
comprehensive collection, thus the user may need to write some problem- or
model-specific plotting functions.  The current collection is:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#bayespy.plot.pdf" title="bayespy.plot.pdf"><code class="xref py py-func docutils literal"><span class="pre">pdf()</span></code></a>: show probability density function of a scalar</li>
<li><a class="reference internal" href="index.html#bayespy.plot.contour" title="bayespy.plot.contour"><code class="xref py py-func docutils literal"><span class="pre">contour()</span></code></a>: show probability density function of two-element vector</li>
<li><a class="reference internal" href="index.html#bayespy.plot.hinton" title="bayespy.plot.hinton"><code class="xref py py-func docutils literal"><span class="pre">hinton()</span></code></a>: show the Hinton diagram</li>
<li><a class="reference internal" href="index.html#bayespy.plot.plot" title="bayespy.plot.plot"><code class="xref py py-func docutils literal"><span class="pre">plot()</span></code></a>: show value as a function</li>
</ul>
<p>The probability density function of a scalar random variable can be plotted
using the function <a class="reference internal" href="index.html#bayespy.plot.pdf" title="bayespy.plot.pdf"><code class="xref py py-func docutils literal"><span class="pre">pdf()</span></code></a>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="s1">&#39;tau&#39;</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../user_guide/plot-1.py">Source code</a>, <a class="reference external" href="../user_guide/plot-1.png">png</a>, <a class="reference external" href="../user_guide/plot-1.hires.png">hires.png</a>, <a class="reference external" href="../user_guide/plot-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/plot-1.png" src="_images/plot-1.png" />
</div>
<p>The variable <code class="docutils literal"><span class="pre">tau</span></code> models the inverse variance of the noise, for which the
true value is <img class="math" src="_images/math/1df705ca706b921ca3678bd051b12cf678d2645b.svg" alt="0.1^{-2}=100"/>.  Thus, the posterior captures the true value
quite accurately.  Similarly, the function <a class="reference internal" href="index.html#bayespy.plot.contour" title="bayespy.plot.contour"><code class="xref py py-func docutils literal"><span class="pre">contour()</span></code></a> can be used to plot
the probability density function of a 2-dimensional variable, for instance:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
<span class="go">&lt;matplotlib.contour.QuadContourSet object at 0x...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../user_guide/plot-2.py">Source code</a>, <a class="reference external" href="../user_guide/plot-2.png">png</a>, <a class="reference external" href="../user_guide/plot-2.hires.png">hires.png</a>, <a class="reference external" href="../user_guide/plot-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/plot-2.png" src="_images/plot-2.png" />
</div>
<p>Both <a class="reference internal" href="index.html#bayespy.plot.pdf" title="bayespy.plot.pdf"><code class="xref py py-func docutils literal"><span class="pre">pdf()</span></code></a> and <a class="reference internal" href="index.html#bayespy.plot.contour" title="bayespy.plot.contour"><code class="xref py py-func docutils literal"><span class="pre">contour()</span></code></a> require that the user provides the grid on
which the probability density function is computed.  They also support several
keyword arguments for modifying the output, similarly as <code class="docutils literal"><span class="pre">plot</span></code> and
<code class="docutils literal"><span class="pre">contour</span></code> in <code class="docutils literal"><span class="pre">matplotlib.pyplot</span></code>.  These functions can be used only for
stochastic nodes.  A few other plot types are also available as built-in
functions.  A Hinton diagram can be plotted as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../user_guide/plot-3.py">Source code</a>, <a class="reference external" href="../user_guide/plot-3.png">png</a>, <a class="reference external" href="../user_guide/plot-3.hires.png">hires.png</a>, <a class="reference external" href="../user_guide/plot-3.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/plot-3.png" src="_images/plot-3.png" />
</div>
<p>The diagram shows the elements of the matrix <img class="math" src="_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.svg" alt="C"/>.  The size of the filled
rectangle corresponds to the absolute value of the element mean, and white and
black correspond to positive and negative values, respectively.  The non-filled
rectangle shows standard deviation.  From this diagram it is clear that the
third column of <img class="math" src="_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.svg" alt="C"/> has been pruned out and the rows that were missing in
the data have zero mean and column-specific variance.  The function
<a class="reference internal" href="index.html#bayespy.plot.hinton" title="bayespy.plot.hinton"><code class="xref py py-func docutils literal"><span class="pre">hinton()</span></code></a> is a simple wrapper for node-specific Hinton diagram plotters,
such as <code class="xref py py-func docutils literal"><span class="pre">gaussian_hinton()</span></code> and <code class="xref py py-func docutils literal"><span class="pre">dirichlet_hinton()</span></code>.  Thus, the keyword
arguments depend on the node which is plotted.</p>
<p>Another plotting function is <a class="reference internal" href="index.html#bayespy.plot.plot" title="bayespy.plot.plot"><code class="xref py py-func docutils literal"><span class="pre">plot()</span></code></a>, which just plots the values of the
node over one axis as a function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../user_guide/plot-4.py">Source code</a>, <a class="reference external" href="../user_guide/plot-4.png">png</a>, <a class="reference external" href="../user_guide/plot-4.hires.png">hires.png</a>, <a class="reference external" href="../user_guide/plot-4.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/plot-4.png" src="_images/plot-4.png" />
</div>
<p>Now, the <code class="docutils literal"><span class="pre">axis</span></code> is the second last axis which corresponds to
<img class="math" src="_images/math/c3c610e9f9483ade26c2a663e594459e16a73680.svg" alt="n=0,\ldots,N-1"/>.  As <img class="math" src="_images/math/5eb97dc131c6ba57993086e7547164b8e3eb5096.svg" alt="D=3"/>, there are three subplots.  For Gaussian
variables, the function shows the mean and two standard deviations.  The plot
shows that the third component has been pruned out, thus the method has been
able to recover the true dimensionality of the latent space.  It also has
similar keyword arguments to <code class="docutils literal"><span class="pre">plot</span></code> function in <code class="docutils literal"><span class="pre">matplotlib.pyplot</span></code>.  Again,
<a class="reference internal" href="index.html#bayespy.plot.plot" title="bayespy.plot.plot"><code class="xref py py-func docutils literal"><span class="pre">plot()</span></code></a> is a simple wrapper over node-specific plotting functions, thus it
supports only some node classes.</p>
</div>
<div class="section" id="monitoring-during-the-inference-algorithm">
<h4>Monitoring during the inference algorithm<a class="headerlink" href="#monitoring-during-the-inference-algorithm" title="Permalink to this headline">¶</a></h4>
<p>It is possible to plot the distribution of the nodes during the learning
algorithm.  This is useful when the user is interested to see how the
distributions evolve during learning and what is happening to the distributions.
In order to utilize monitoring, the user must set plotters for the nodes that he
or she wishes to monitor.  This can be done either when creating the node or
later at any time.</p>
<p>The plotters are set by creating a plotter object and providing this object to
the node.  The plotter is a wrapper of one of the plotting functions mentioned
above: <a class="reference internal" href="index.html#bayespy.plot.PDFPlotter" title="bayespy.plot.PDFPlotter"><code class="xref py py-class docutils literal"><span class="pre">PDFPlotter</span></code></a>, <a class="reference internal" href="index.html#bayespy.plot.ContourPlotter" title="bayespy.plot.ContourPlotter"><code class="xref py py-class docutils literal"><span class="pre">ContourPlotter</span></code></a>, <a class="reference internal" href="index.html#bayespy.plot.HintonPlotter" title="bayespy.plot.HintonPlotter"><code class="xref py py-class docutils literal"><span class="pre">HintonPlotter</span></code></a> or
<a class="reference internal" href="index.html#bayespy.plot.FunctionPlotter" title="bayespy.plot.FunctionPlotter"><code class="xref py py-class docutils literal"><span class="pre">FunctionPlotter</span></code></a>.  Thus, our example model could use the following
plotters:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">PDFPlotter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">HintonPlotter</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">FunctionPlotter</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>These could have been given at node creation as a keyword argument <code class="docutils literal"><span class="pre">plotter</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span>
<span class="gp">... </span>             <span class="n">plotter</span><span class="o">=</span><span class="n">bpplt</span><span class="o">.</span><span class="n">ContourPlotter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span>
<span class="gp">... </span>                                          <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
<p>When the plotter is set, one can use the <code class="docutils literal"><span class="pre">plot</span></code> method of the node to perform
plotting:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">V</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="go">&lt;matplotlib.contour.QuadContourSet object at 0x...&gt;</span>
</pre></div>
</div>
<p>Nodes can also be plotted using the <code class="docutils literal"><span class="pre">plot</span></code> method of the inference engine:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This method remembers the figure in which a node has been plotted and uses that
every time it plots the same node.  In order to monitor the nodes during
learning, it is possible to use the keyword argument <code class="docutils literal"><span class="pre">plot</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
<span class="go">Iteration 19: loglike=-1.221354e+02 (... seconds)</span>
<span class="go">Iteration 20: loglike=-1.221354e+02 (... seconds)</span>
<span class="go">Iteration 21: loglike=-1.221354e+02 (... seconds)</span>
<span class="go">Iteration 22: loglike=-1.221354e+02 (... seconds)</span>
<span class="go">Iteration 23: loglike=-1.221354e+02 (... seconds)</span>
</pre></div>
</div>
<p>Each node which has a plotter set will be plotted after it is updated.  Note
that this may slow down the inference significantly if the plotting operation is
time consuming.</p>
</div>
<div class="section" id="posterior-parameters-and-moments">
<h4>Posterior parameters and moments<a class="headerlink" href="#posterior-parameters-and-moments" title="Permalink to this headline">¶</a></h4>
<p>If the built-in plotting functions are not sufficient, it is possible to use
<code class="docutils literal"><span class="pre">matplotlib.pyplot</span></code> for custom plotting.  Each node has <code class="docutils literal"><span class="pre">get_moments</span></code> method
which returns the moments and they can be used for plotting.  Stochastic
exponential family nodes have natural parameter vectors which can also be used.
In addition to plotting, it is also possible to just print the moments or
parameters in the console.</p>
</div>
<div class="section" id="saving-and-loading-results">
<h4>Saving and loading results<a class="headerlink" href="#saving-and-loading-results" title="Permalink to this headline">¶</a></h4>
<p>The results of the inference engine can be easily saved and loaded using
<a class="reference internal" href="index.html#bayespy.inference.VB.save" title="bayespy.inference.VB.save"><code class="xref py py-func docutils literal"><span class="pre">VB.save()</span></code></a> and <a class="reference internal" href="index.html#bayespy.inference.VB.load" title="bayespy.inference.VB.load"><code class="xref py py-func docutils literal"><span class="pre">VB.load()</span></code></a> methods:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tempfile</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">filename</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkstemp</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.hdf5&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">filename</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
<p>The results are stored in a HDF5 file.  The user may set an autosave file in
which the results are automatically saved regularly.  Autosave filename can be
set at creation time by <code class="docutils literal"><span class="pre">autosave_filename</span></code> keyword argument or later using
<a class="reference internal" href="index.html#bayespy.inference.VB.set_autosave" title="bayespy.inference.VB.set_autosave"><code class="xref py py-func docutils literal"><span class="pre">VB.set_autosave()</span></code></a> method.  If autosave file has been set, the
<a class="reference internal" href="index.html#bayespy.inference.VB.save" title="bayespy.inference.VB.save"><code class="xref py py-func docutils literal"><span class="pre">VB.save()</span></code></a> and <a class="reference internal" href="index.html#bayespy.inference.VB.load" title="bayespy.inference.VB.load"><code class="xref py py-func docutils literal"><span class="pre">VB.load()</span></code></a> methods use that file by default.  In order
for the saving to work, all stochastic nodes must have been given (unique)
names.</p>
<p>However, note that these methods do <em>not</em> save nor load the node definitions.
It means that the user must create the nodes and the inference engine and then
use <a class="reference internal" href="index.html#bayespy.inference.VB.load" title="bayespy.inference.VB.load"><code class="xref py py-func docutils literal"><span class="pre">VB.load()</span></code></a> to set the state of the nodes and the inference engine.  If
there are any differences in the model that was saved and the one which is tried
to update using loading, then loading does not work.  Thus, the user should keep
the model construction unmodified in a Python file in order to be able to load
the results later.  Or if the user wishes to share the results, he or she must
share the model construction Python file with the HDF5 results file.</p>
</div>
</div>
<span id="document-user_guide/advanced"></span><div class="section" id="advanced-topics">
<h3>Advanced topics<a class="headerlink" href="#advanced-topics" title="Permalink to this headline">¶</a></h3>
<p>This section contains brief information on how to implement some advanced
methods in BayesPy.  These methods include Riemannian conjugate gradient
methods, pattern search, simulated annealing, collapsed variational inference
and stochastic variational inference.  In order to use these methods properly,
the user should understand them to some extent.  They are also considered
experimental, thus you may encounter bugs or unimplemented features.  In any
case, these methods may provide huge performance improvements easily compared to
the standard VB-EM algorithm.</p>
<div class="section" id="gradient-based-optimization">
<h4>Gradient-based optimization<a class="headerlink" href="#gradient-based-optimization" title="Permalink to this headline">¶</a></h4>
<p>Variational Bayesian learning basically means that the parameters of the
approximate posterior distributions are optimized to maximize the lower bound of
the marginal log likelihood <a class="reference internal" href="index.html#honkela-2010" id="id1">[3]</a>.  This optimization can be done
by using gradient-based optimization methods.  In order to improve the
gradient-based methods, it is recommended to take into account the information
geometry by using the Riemannian (a.k.a. natural) gradient.  In fact, the
standard VB-EM algorithm is equivalent to a gradient ascent method which uses
the Riemannian gradient and step length 1.  Thus, it is natural to try to
improve this method by using non-linear conjugate gradient methods instead of
gradient ascent.  These optimization methods are especially useful when the
VB-EM update equations are not available but one has to use fixed form
approximation.  But it is possible that the Riemannian conjugate gradient method
improve performance even when the VB-EM update equations are available.</p>
<p>The optimization algorithm in <a class="reference internal" href="index.html#bayespy.inference.VB.optimize" title="bayespy.inference.VB.optimize"><code class="xref py py-func docutils literal"><span class="pre">VB.optimize()</span></code></a> has a simple interface.
Instead of using the default Riemannian geometry, one can use the Euclidean
geometry by giving <code class="code docutils literal"><span class="pre">riemannian=False</span></code>.  It is also possible to choose the
optimization method from gradient ascent (<code class="code docutils literal"><span class="pre">method='gradient'</span></code>) or
conjugate gradient methods (only <code class="code docutils literal"><span class="pre">method='fletcher-reeves'</span></code> implemented at
the moment).  For instance, we could optimize nodes <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">X</span></code> jointly
using Euclidean gradient ascent as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">riemannian</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Note that this is very inefficient way of updating those nodes (bad geometry and
not using conjugate gradients).  Thus, one should understand the idea of these
optimization methods, otherwise one may do something extremely inefficient.
Most likely this method can be found useful in combination with the advanced
tricks in the following sections.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The Euclidean gradient has not been implemented for all nodes yet.  The
Euclidean gradient is required by the Euclidean geometry based optimization
but also by the conjugate gradient methods in the Riemannian geometry.  Thus,
the Riemannian conjugate gradient may not yet work for all models.</p>
</div>
<p>It is possible to construct custom optimization algorithms with the tools
provided by <a class="reference internal" href="index.html#bayespy.inference.VB" title="bayespy.inference.VB"><code class="xref py py-class docutils literal"><span class="pre">VB</span></code></a>.  For instance, <a class="reference internal" href="index.html#bayespy.inference.VB.get_parameters" title="bayespy.inference.VB.get_parameters"><code class="xref py py-func docutils literal"><span class="pre">VB.get_parameters()</span></code></a> and
<a class="reference internal" href="index.html#bayespy.inference.VB.set_parameters" title="bayespy.inference.VB.set_parameters"><code class="xref py py-func docutils literal"><span class="pre">VB.set_parameters()</span></code></a> can be used to handle the parameters of nodes.
<a class="reference internal" href="index.html#bayespy.inference.VB.get_gradients" title="bayespy.inference.VB.get_gradients"><code class="xref py py-func docutils literal"><span class="pre">VB.get_gradients()</span></code></a> is used for computing the gradients of nodes.  The
parameter and gradient objects are not numerical arrays but more complex nested
lists not meant to be accessed by the user.  Thus, for simple arithmetics with
the parameter and gradient objects, use functions <a class="reference internal" href="index.html#bayespy.inference.VB.add" title="bayespy.inference.VB.add"><code class="xref py py-func docutils literal"><span class="pre">VB.add()</span></code></a> and
<a class="reference internal" href="index.html#bayespy.inference.VB.dot" title="bayespy.inference.VB.dot"><code class="xref py py-func docutils literal"><span class="pre">VB.dot()</span></code></a>.  Finally, <a class="reference internal" href="index.html#bayespy.inference.VB.compute_lowerbound" title="bayespy.inference.VB.compute_lowerbound"><code class="xref py py-func docutils literal"><span class="pre">VB.compute_lowerbound()</span></code></a> and
<a class="reference internal" href="index.html#bayespy.inference.VB.has_converged" title="bayespy.inference.VB.has_converged"><code class="xref py py-func docutils literal"><span class="pre">VB.has_converged()</span></code></a> can be used to monitor the lower bound.</p>
</div>
<div class="section" id="collapsed-inference">
<h4>Collapsed inference<a class="headerlink" href="#collapsed-inference" title="Permalink to this headline">¶</a></h4>
<p>The optimization method can be used efficiently in such a way that some of the
variables are collapsed, that is, marginalized out <a class="reference internal" href="index.html#hensman-2012" id="id2">[1]</a>.  The
collapsed variables must be conditionally independent given the observations and
all other variables.  Probably, one also wants that the size of the marginalized
variables is large and the size of the optimized variables is small.  For
instance, in our PCA example, we could optimize as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">collapsed</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">])</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>The collapsed variables are given as a list.  This optimization does basically
the following: It first computes the gradients for <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">tau</span></code> and takes
an update step using the desired optimization method.  Then, it updates the
collapsed variables by using the standard VB-EM update equations.  These two
steps are taken in turns.  Effectively, this corresponds to collapsing the
variables <code class="docutils literal"><span class="pre">X</span></code> and <code class="docutils literal"><span class="pre">alpha</span></code> in a particular way.  The point of this method is
that the number of parameters in the optimization reduces significantly and the
collapsed variables are updated optimally.  For more details, see
<a class="reference internal" href="index.html#hensman-2012" id="id3">[1]</a>.</p>
<p>It is possible to use this method in such a way, that the collapsed variables
are not conditionally independent given the observations and all other
variables.  However, in that case, the method does not anymore correspond to
collapsing the variables but just using VB-EM updates after gradient-based
updates.  The method does not check for conditional independence, so the user is
free to do this.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Although the Riemannian conjugate gradient method has not yet been
implemented for all nodes, it may be possible to collapse those nodes and
optimize the other nodes for which the Euclidean gradient is already
implemented.</p>
</div>
</div>
<div class="section" id="pattern-search">
<h4>Pattern search<a class="headerlink" href="#pattern-search" title="Permalink to this headline">¶</a></h4>
<p>The pattern search method estimates the direction in which the approximate
posterior distributions are updating and performs a line search in that
direction <a class="reference internal" href="index.html#honkela-2003" id="id4">[4]</a>.  The search direction is based on the difference
in the VB parameters on successive updates (or several updates).  The idea is
that the VB-EM algorithm may be slow because it just zigzags and this can be
fixed by moving to the direction in which the VB-EM is slowly moving.</p>
<p>BayesPy offers a simple built-in pattern search method
<a class="reference internal" href="index.html#bayespy.inference.VB.pattern_search" title="bayespy.inference.VB.pattern_search"><code class="xref py py-func docutils literal"><span class="pre">VB.pattern_search()</span></code></a>.  The method updates the nodes twice, measures the
difference in the parameters and performs a line search with a small number of
function evaluations:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">pattern_search</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Similarly to the collapsed optimization, it is possible to collapse some of the
variables in the pattern search.  The same rules of conditional independence
apply as above.  The collapsed variables are given as list:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">pattern_search</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">collapsed</span><span class="o">=</span><span class="p">[</span><span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">])</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Also, a maximum number of iterations can be set by using <code class="docutils literal"><span class="pre">maxiter</span></code> keyword
argument.  It is not always obvious whether a pattern search will improve the
rate of convergence or not but if it seems that the convergence is slow because
of zigzagging, it may be worth a try.  Note that the computational cost of the
pattern search is quite high, thus it is not recommended to perform it after
every VB-EM update but every now and then, for instance, after every 10
iterations.  In addition, it is possible to write a more customized VB learning
algorithm which uses pattern searches by using the different methods of
<a class="reference internal" href="index.html#bayespy.inference.VB" title="bayespy.inference.VB"><code class="xref py py-class docutils literal"><span class="pre">VB</span></code></a> discussed above.</p>
</div>
<div class="section" id="deterministic-annealing">
<h4>Deterministic annealing<a class="headerlink" href="#deterministic-annealing" title="Permalink to this headline">¶</a></h4>
<p>The standard VB-EM algorithm converges to a local optimum which can often be
inferior to the global optimum and many other local optima.  Deterministic
annealing aims at finding a better local optimum, hopefully even the global
optimum <a class="reference internal" href="index.html#katahira-2008" id="id5">[5]</a>.  It does this by increasing the weight on the
entropy of the posterior approximation in the VB lower bound.  Effectively, the
annealed lower bound becomes closer to a uniform function instead of the
original multimodal lower bound.  The weight on the entropy is recovered slowly
and the optimization is much more robust to initialization.</p>
<p>In BayesPy, the annealing can be set by using <a class="reference internal" href="index.html#bayespy.inference.VB.set_annealing" title="bayespy.inference.VB.set_annealing"><code class="xref py py-func docutils literal"><span class="pre">VB.set_annealing()</span></code></a>.  The
given annealing should be in range <img class="math" src="_images/math/48c650ca6c0103c5c8c99a511de327d90f12302a.svg" alt="(0,1]"/> but this is not validated in
case the user wants to do something experimental.  If annealing is set to 1, the
original VB lower bound is recovered.  Annealing with 0 would lead to an
improper uniform distribution, thus it will lead to errors.  The entropy term is
weighted by the inverse of this annealing term.  An alternative view is that the
model probability density functions are raised to the power of the annealing
term.</p>
<p>Typically, the annealing is used in such a way that the annealing is small at
the beginning and increased after every convergence of the VB algorithm until
value 1 is reached.  After the annealing value is increased, the algorithm
continues from where it had just converged.  The annealing can be used for
instance as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">while</span> <span class="n">beta</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">beta</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">set_annealing</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>Here, the <code class="docutils literal"><span class="pre">tol</span></code> keyword argument is used to adjust the threshold for
convergence.  In this case, it is a bit larger than by default so the algorithm
does not need to converge perfectly but a rougher convergence is sufficient for
the next iteration with a new annealing value.</p>
</div>
<div class="section" id="stochastic-variational-inference">
<h4>Stochastic variational inference<a class="headerlink" href="#stochastic-variational-inference" title="Permalink to this headline">¶</a></h4>
<p>In stochastic variational inference <a class="reference internal" href="index.html#hoffman-2013" id="id6">[2]</a>, the idea is to use
mini-batches of large datasets to compute noisy gradients and learn the VB
distributions by using stochastic gradient ascent.  In order for it to be
useful, the model must be such that it can be divided into &#8220;intermediate&#8221; and
&#8220;global&#8221; variables.  The number of intermediate variables increases with the
data but the number of global variables remains fixed.  The global variables are
learnt in the stochastic optimization.</p>
<p>By denoting the data as <img class="math" src="_images/math/6540b80d4b3c27d2753d08a645de1dcf50e0c5e3.svg" alt="Y=[Y_1, \ldots, Y_N]"/>, the intermediate variables
as <img class="math" src="_images/math/301c97427ea7a35030315cbf71bad64e49c56c6f.svg" alt="Z=[Z_1, \ldots, Z_N]"/> and the global variables as <img class="math" src="_images/math/3be04d4207434584251f6921820c24ac9fa8c6f1.svg" alt="\theta"/>, the
model needs to have the following structure:</p>
<div class="math">
<p><img src="_images/math/8c327fb0f6658aaa92e08fbfebbfacfa2d833e6a.svg" alt="p(Y, Z, \theta) &amp;= p(\theta) \prod^N_{n=1} p(Y_n|Z_n,\theta) p(Z_n|\theta)"/></p>
</div><p>The algorithm consists of three steps which are iterated: 1) a random mini-batch
of the data is selected, 2) the corresponding intermediate variables are updated
by using normal VB update equations, and 3) the global variables are updated
with (stochastic) gradient ascent as if there was as many replications of the
mini-batch as needed to recover the original dataset size.</p>
<p>The learning rate for the gradient ascent must satisfy:</p>
<div class="math">
<p><img src="_images/math/7972121fd3a0e1d31642afa186de898149cdbe1d.svg" alt="\sum^\infty_{i=1} \alpha_i = \infty \qquad \text{and} \qquad
\sum^\infty_{i=1} \alpha^2 &lt; \infty,"/></p>
</div><p>where <img class="math" src="_images/math/df0deb143e5ac127f00bd248ee8001ecae572adc.svg" alt="i"/> is the iteration number.  An example of a valid learning
parameter is <img class="math" src="_images/math/826c5f242667ddb206b3098c47f9d274c3d48dae.svg" alt="\alpha_i = (\delta + i)^{-\gamma}"/>, where <img class="math" src="_images/math/29089282600e18d581bed2becac2ebc71a5a3eef.svg" alt="\delta \geq
0"/> is a delay and <img class="math" src="_images/math/6424aef5836d4f4f9c8fe8f4d96e7a01ce7d6ee7.svg" alt="\gamma\in (0.5, 1]"/> is a forgetting rate.</p>
<p>Stochastic variational inference is relatively easy to use in BayesPy.  The idea
is that the user creates a model for the size of a mini-batch and specifies a
multiplier for those plate axes that are replicated.  For the PCA example, the
mini-batch model can be costructed as follows.  We decide to use <code class="docutils literal"><span class="pre">X</span></code> as an
intermediate variable and the other variables are global.  The global variables
<code class="docutils literal"><span class="pre">alpha</span></code>, <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">tau</span></code> are constructed identically as before.  The
intermediate variable <code class="docutils literal"><span class="pre">X</span></code> is constructed as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">plates_multiplier</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the plates are <code class="docutils literal"><span class="pre">(1,5)</span></code> whereas they are <code class="docutils literal"><span class="pre">(1,100)</span></code> in the full
model.  Thus, we need to provide a plates multiplier <code class="docutils literal"><span class="pre">(1,20)</span></code> to define how
the plates are replicated to get the full dataset.  These multipliers do not
need to be integers, in this case the latter plate axis is multiplied by
<img class="math" src="_images/math/4ecbc3bd337ca174df049aa6d45dc93c3e10218a.svg" alt="100/5=20"/>.  The remaining variables are defined as before:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the plates of <code class="docutils literal"><span class="pre">Y</span></code> and <code class="docutils literal"><span class="pre">F</span></code> also correspond to the size of the
mini-batch and they also deduce the plate multipliers from their parents, thus
we do not need to specify the multiplier here explicitly (although it is ok to
do so).</p>
<p>Let us construct the inference engine for the new mini-batch model:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>Use random initialization for <code class="docutils literal"><span class="pre">C</span></code> to break the symmetry in <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">X</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>Then, stochastic variational inference algorithm could look as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">ignore_bound_checks</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span><span class="n">subset</span><span class="p">])</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">learning_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">gradient_step</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
<p>First, we ignore the bound checks because they are noisy.  Then, the loop
consists of three parts: 1) Draw a random mini-batch of the data (5 samples from
100).  2) Update the intermediate variable <code class="docutils literal"><span class="pre">X</span></code>.  3) Update global variables
with gradient ascent using a proper learning rate.</p>
</div>
<div class="section" id="black-box-variational-inference">
<h4>Black-box variational inference<a class="headerlink" href="#black-box-variational-inference" title="Permalink to this headline">¶</a></h4>
<p>NOT YET IMPLEMENTED.</p>
</div>
</div>
</div>
</div>
<span id="document-examples/examples"></span><div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-examples/multinomial"></span><div class="section" id="Multinomial-distribution:-bags-of-marbles">
<h3>Multinomial distribution: bags of marbles<a class="headerlink" href="#Multinomial-distribution:-bags-of-marbles" title="Permalink to this headline">¶</a></h3>
<p><em>Written by: Deebul Nair (2016)</em></p>
<p><em>Edited by: Jaakko Luttinen (2016)</em></p>
<p><em>Inspired by https://probmods.org/hierarchical-models.html</em></p>
<div class="section" id="Using-multinomial-distribution">
<h4>Using multinomial distribution<a class="headerlink" href="#Using-multinomial-distribution" title="Permalink to this headline">¶</a></h4>
<p>There are several bags of coloured marbles, each bag containing
different amounts of each color. Marbles are drawn at random with
replacement from the bags. The goal is to predict the distribution of
the marbles in each bag.</p>
<div class="section" id="Data-generation">
<h5>Data generation<a class="headerlink" href="#Data-generation" title="Permalink to this headline">¶</a></h5>
<p>Let us create a dataset. First, decide the number of bags, colors and
trials (i.e., draws):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>n_colors = 5  # number of possible colors
n_bags = 3    # number of bags
n_trials = 20 # number of draws from each bag
</pre></div>
</div>
</div>
<p>Generate randomly a color distribution for each bag:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy import nodes
import numpy as np

p_colors = nodes.Dirichlet(n_colors * [0.5], plates=(n_bags,)).random()
</pre></div>
</div>
</div>
<p>The concentration parameter
<img class="math" src="_images/math/4282e6d8fb3a03271895f1b4d5ac5c09a9f25a1a.svg" alt="\begin{bmatrix}0.5 &amp; \ldots &amp; 0.5\end{bmatrix}"/> makes the
distributions very non-uniform within each bag, that is, the amount of
each color can be very different. We can visualize the probability
distribution of the colors in each bag:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import bayespy.plot as bpplt
bpplt.hinton(p_colors)
bpplt.pyplot.title(&quot;Original probability distributions of colors in the bags&quot;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_multinomial_6_0.png" src="_images/examples_multinomial_6_0.png" />
</div>
</div>
<p>As one can see, the color distributions aren&#8217;t very uniform in any of
the bags because of the small concentration parameter. Next, make the
ball draws:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>marbles = nodes.Multinomial(n_trials, p_colors).random()
print(marbles)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[[ 0  9  2  0  9]
 [ 0 18  0  0  2]
 [ 5  2  1  3  9]]
</pre></div></div>
</div>
</div>
<div class="section" id="Model">
<h5>Model<a class="headerlink" href="#Model" title="Permalink to this headline">¶</a></h5>
<p>We will use the same generative model for estimating the color
distributions in the bags as we did for generating the data:</p>
<div class="math">
<p><img src="_images/math/45fa83f6f1869a9c31373e5f2b33195bc1fd2e50.svg" alt="\theta_i \sim \mathrm{Dirichlet}\left(\begin{bmatrix} 0.5 &amp; \ldots &amp; 0.5 \end{bmatrix}\right)"/></p>
</div><div class="math">
<p><img src="_images/math/bb0415fdcc35f65586de2d7522611903400790e5.svg" alt="y_i | \theta_i \sim \mathrm{Multinomial}(\theta_i)"/></p>
</div><p>The simple graphical model can be drawn as below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>%%tikz -f svg
\usetikzlibrary{bayesnet}
\node [latent] (theta) {$\theta$};
\node [below=of theta, obs] (y) {$y$};
\edge {theta} {y};
\plate {trials} {(y)} {trials};
\plate {bags} {(theta)(y)(trials)} {bags};
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_multinomial_10_0.svg" src="_images/examples_multinomial_10_0.svg" /></div>
</div>
<p>The model is constructed equivalently to the generative model (except we
don&#8217;t use the nodes to draw random samples):</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>theta = nodes.Dirichlet(n_colors * [0.5], plates=(n_bags,))
y = nodes.Multinomial(n_trials, theta)
</pre></div>
</div>
</div>
<p>Data is provided by using the <code class="docutils literal"><span class="pre">observe</span></code> method:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>y.observe(marbles)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Performing-Inference">
<h5>Performing Inference<a class="headerlink" href="#Performing-Inference" title="Permalink to this headline">¶</a></h5>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.inference import VB
Q = VB(y, theta)
Q.update(repeat=1000)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration 1: loglike=-2.617894e+01 (0.001 seconds)
Iteration 2: loglike=-2.617894e+01 (0.001 seconds)
Converged at iteration 2.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import bayespy.plot as bpplt
bpplt.hinton(theta)
bpplt.pyplot.title(&quot;Learned distribution of colors&quot;)
bpplt.pyplot.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_multinomial_17_0.png" src="_images/examples_multinomial_17_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Using-categorical-Distribution">
<h4>Using categorical Distribution<a class="headerlink" href="#Using-categorical-Distribution" title="Permalink to this headline">¶</a></h4>
<p>The same problem can be solved with categorical distirbution.
Categorical distribution is similar to the Multinomical distribution
expect for the output it produces.</p>
<p>Multinomial and Categorical infer the number of colors from the size of
the probability vector (p_theta) Categorical data is in a form where
the value tells the index of the color that was picked in a trial. so if
n_colors=5, Categorical data could be [4, 4, 0, 1, 1, 2, 4] if the
number of trials was 7.</p>
<p>multinomial data is such that you have a vector where each element tells
how many times that color was picked, for instance, [3, 0, 4] if you
have 7 trials.</p>
<p>So there is significant difference in Multinomial and Categorical data .
Depending on the data you have the choice of the Distribution has to be
made.</p>
<p>Now we can see an example of Hierarchical model usign categorical data
generator and model</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy import nodes
import numpy as np

#The marbles drawn based on the distribution for 10 trials
# Using same p_color distribution as in the above example
draw_marbles = nodes.Categorical(p_colors,
                                 plates=(n_trials, n_bags)).random()
</pre></div>
</div>
</div>
<div class="section" id="Model">
<h5>Model<a class="headerlink" href="#Model" title="Permalink to this headline">¶</a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy import nodes
import numpy as np

p_theta = nodes.Dirichlet(np.ones(n_colors),
                          plates=(n_bags,),
                          name=&#39;p_theta&#39;)

bag_model = nodes.Categorical(p_theta,
                        plates=(n_trials, n_bags),
                        name=&#39;bag_model&#39;)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Inference">
<h5>Inference<a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>bag_model.observe(draw_marbles)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.inference import VB
Q = VB(bag_model, p_theta)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>Q.update(repeat=1000)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration 1: loglike=-6.595923e+01 (0.001 seconds)
Iteration 2: loglike=-6.595923e+01 (0.001 seconds)
Converged at iteration 2.
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>%matplotlib inline
import bayespy.plot as bpplt
bpplt.hinton(p_theta)
bpplt.pyplot.tight_layout()
bpplt.pyplot.title(&quot;Learned Distribution of colors using Categorical Distribution&quot;)
bpplt.pyplot.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_multinomial_26_0.png" src="_images/examples_multinomial_26_0.png" />
</div>
</div>
</div>
</div>
</div>
<span id="document-examples/regression"></span><p><em>This example is a Jupyter notebook. You can download it or run it
interactively on mybinder.org.</em></p>
<div class="section" id="Linear-regression">
<h3>Linear regression<a class="headerlink" href="#Linear-regression" title="Permalink to this headline">¶</a></h3>
<div class="section" id="Data">
<h4>Data<a class="headerlink" href="#Data" title="Permalink to this headline">¶</a></h4>
<p>The true parameters of the linear regression:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import numpy as np
k = 2 # slope
c = 5 # bias
s = 2 # noise standard deviation
</pre></div>
</div>
</div>
<p>Generate data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>x = np.arange(10)
y = k*x + c + s*np.random.randn(10)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Model">
<h4>Model<a class="headerlink" href="#Model" title="Permalink to this headline">¶</a></h4>
<p>The regressors, that is, the input data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>X = np.vstack([x, np.ones(len(x))]).T
</pre></div>
</div>
</div>
<p>Note that we added a column of ones to the regressor matrix for the bias
term. We model the slope and the bias term in the same node so we do not
factorize between them:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.nodes import GaussianARD
B = GaussianARD(0, 1e-6, shape=(2,))
</pre></div>
</div>
</div>
<p>The first element is the slope which multiplies x and the second element
is the bias term which multiplies the constant ones. Now we compute the
dot product of X and B:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.nodes import SumMultiply
F = SumMultiply(&#39;i,i&#39;, B, X)
</pre></div>
</div>
</div>
<p>The noise parameter:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.nodes import Gamma
tau = Gamma(1e-3, 1e-3)
</pre></div>
</div>
</div>
<p>The noisy observations:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>Y = GaussianARD(F, tau)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Inference">
<h4>Inference<a class="headerlink" href="#Inference" title="Permalink to this headline">¶</a></h4>
<p>Observe the data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>Y.observe(y)
</pre></div>
</div>
</div>
<p>Construct the variational Bayesian (VB) inference engine by giving all
stochastic nodes:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.inference import VB
Q = VB(Y, B, tau)
</pre></div>
</div>
</div>
<p>Iterate until convergence:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>Q.update(repeat=1000)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration 1: loglike=-4.080970e+01 (0.003 seconds)
Iteration 2: loglike=-4.057132e+01 (0.003 seconds)
Iteration 3: loglike=-4.056572e+01 (0.003 seconds)
Iteration 4: loglike=-4.056551e+01 (0.003 seconds)
Converged at iteration 4.
</pre></div></div>
</div>
</div>
<div class="section" id="Results">
<h4>Results<a class="headerlink" href="#Results" title="Permalink to this headline">¶</a></h4>
<p>Create a simple predictive model for new inputs:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>xh = np.linspace(-5, 15, 100)
Xh = np.vstack([xh, np.ones(len(xh))]).T
Fh = SumMultiply(&#39;i,i&#39;, B, Xh)
</pre></div>
</div>
</div>
<p>Note that we use the learned node B but create a new regressor array for
predictions. Plot the predictive distribution of noiseless function
values:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import bayespy.plot as bpplt
bpplt.pyplot.figure()
bpplt.plot(Fh, x=xh, scale=2)
bpplt.plot(y, x=x, color=&#39;r&#39;, marker=&#39;x&#39;, linestyle=&#39;None&#39;)
bpplt.plot(k*xh+c, x=xh, color=&#39;r&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_regression_24_0.png" src="_images/examples_regression_24_0.png" />
</div>
</div>
<p>Note that the above plot shows two standard deviation of the posterior
of the noiseless function, thus the data points may lie well outside
this range. The red line shows the true linear function. Next, plot the
distribution of the noise parameter and the true value, 2−2=0.25:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>bpplt.pyplot.figure()
bpplt.pdf(tau, np.linspace(1e-6,1,100), color=&#39;k&#39;)
bpplt.pyplot.axvline(s**(-2), color=&#39;r&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_regression_26_0.png" src="_images/examples_regression_26_0.png" />
</div>
</div>
<p>The noise level is captured quite well, although the posterior has more
mass on larger noise levels (smaller precision parameter values).
Finally, plot the distribution of the regression parameters and mark the
true value:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>bpplt.pyplot.figure();
bpplt.contour(B, np.linspace(1,3,1000), np.linspace(1,9,1000),
              n=10, colors=&#39;k&#39;);
bpplt.plot(c, x=k, color=&#39;r&#39;, marker=&#39;x&#39;, linestyle=&#39;None&#39;,
           markersize=10, markeredgewidth=2)
bpplt.pyplot.xlabel(r&#39;$k$&#39;);
bpplt.pyplot.ylabel(r&#39;$c$&#39;);
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/examples_regression_28_0.png" src="_images/examples_regression_28_0.png" />
</div>
</div>
<p>In this case, the true parameters are captured well by the posterior
distribution.</p>
</div>
<div class="section" id="Improving-accuracy">
<h4>Improving accuracy<a class="headerlink" href="#Improving-accuracy" title="Permalink to this headline">¶</a></h4>
<p>The model can be improved by not factorizing between B and tau but
learning their joint posterior distribution. This requires a slight
modification to the model by using GaussianGammaISO node:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from bayespy.nodes import GaussianGamma
B_tau = GaussianGamma(np.zeros(2), 1e-6*np.identity(2), 1e-3, 1e-3)
</pre></div>
</div>
</div>
<p>This node contains both the regression parameter vector and the noise
parameter. We compute the dot product similarly as before:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>F_tau = SumMultiply(&#39;i,i&#39;, B_tau, X)
</pre></div>
</div>
</div>
<p>However, Y is constructed as follows:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>Y = GaussianARD(F_tau, 1)
</pre></div>
</div>
</div>
<p>Because the noise parameter is already in F_tau we can give a constant
one as the second argument. The total noise parameter for Y is the
product of the noise parameter in F_tau and one. Now, inference is run
similarly as before:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>Y.observe(y)
Q = VB(Y, B_tau)
Q.update(repeat=1000)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Iteration 1: loglike=-4.130423e+01 (0.002 seconds)
Iteration 2: loglike=-4.130423e+01 (0.002 seconds)
Converged at iteration 2.
</pre></div></div>
</div>
<p>Note that the method converges immediately. This happens because there
is only one unobserved stochastic node so there is no need for iteration
and the result is actually the exact true posterior distribution, not an
approximation. Currently, the main drawback of using this approach is
that BayesPy does not yet contain any plotting utilities for nodes that
contain both Gaussian and gamma variables jointly.</p>
</div>
<div class="section" id="Further-extensions">
<h4>Further extensions<a class="headerlink" href="#Further-extensions" title="Permalink to this headline">¶</a></h4>
<p>The approach discussed in this example can easily be extended to
non-linear regression and multivariate regression. For non-linear
regression, the inputs are first transformed by some known non-linear
functions and then linear regression is applied to this transformed
data. For multivariate regression, X and B are concatenated
appropriately: If there are more regressors, add more columns to both X
and B. If there are more output dimensions, add plates to B.</p>
</div>
</div>
<span id="document-examples/gmm"></span><div class="section" id="gaussian-mixture-model">
<h3>Gaussian mixture model<a class="headerlink" href="#gaussian-mixture-model" title="Permalink to this headline">¶</a></h3>
<p>This example demonstrates the use of Gaussian mixture model for flexible density
estimation, clustering or classification.</p>
<div class="section" id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h4>
<p>First, let us generate some artificial data for the analysis.  The data are
two-dimensional vectors from one of the four different Gaussian distributions:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]],</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">y3</span><span class="p">])</span>
</pre></div>
</div>
<p>Thus, there are 200 data vectors in total.  The data looks as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;rx&#39;</span><span class="p">)</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/gmm-1.py">Source code</a>, <a class="reference external" href="../examples/gmm-1.png">png</a>, <a class="reference external" href="../examples/gmm-1.hires.png">hires.png</a>, <a class="reference external" href="../examples/gmm-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/gmm-1.png" src="_images/gmm-1.png" />
</div>
</div>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>For clarity, let us denote the number of the data vectors with <code class="docutils literal"><span class="pre">N</span></code></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>
</div>
<p>and the dimensionality of the data vectors with <code class="docutils literal"><span class="pre">D</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p>We will use a &#8220;large enough&#8221; number of Gaussian clusters in our model:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>Cluster assignments <code class="docutils literal"><span class="pre">Z</span></code> and the prior for the cluster assignment probabilities
<code class="docutils literal"><span class="pre">alpha</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Dirichlet</span><span class="p">,</span> <span class="n">Categorical</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="mf">1e-5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">),</span>
<span class="gp">... </span>                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The mean vectors and the precision matrices of the clusters:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">Wishart</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="n">Gaussian</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">),</span> <span class="mf">1e-5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>              <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,),</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">Wishart</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="mf">1e-5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                 <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,),</span>
<span class="gp">... </span>                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Lambda&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If either the mean or precision should be shared between clusters, then that
node should not have plates, that is, <code class="docutils literal"><span class="pre">plates=()</span></code>.  The data vectors are from
a Gaussian mixture with cluster assignments <code class="docutils literal"><span class="pre">Z</span></code> and Gaussian component
parameters <code class="docutils literal"><span class="pre">mu</span></code> and <code class="docutils literal"><span class="pre">Lambda</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Mixture</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>Before running the inference algorithm, we provide the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, run VB iteration until convergence:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-1.402345e+03 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 61: loglike=-8.888464e+02 (... seconds)</span>
<span class="go">Converged at iteration 61.</span>
</pre></div>
</div>
<p>The algorithm converges very quickly.  Note that the default update order of the
nodes was such that <code class="docutils literal"><span class="pre">mu</span></code> and <code class="docutils literal"><span class="pre">Lambda</span></code> were updated before <code class="docutils literal"><span class="pre">Z</span></code>, which is
what we wanted because <code class="docutils literal"><span class="pre">Z</span></code> was initialized randomly.</p>
</div>
<div class="section" id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h4>
<p>For two-dimensional Gaussian mixtures, the mixture components can be plotted
using <a class="reference internal" href="index.html#bayespy.plot.gaussian_mixture_2d" title="bayespy.plot.gaussian_mixture_2d"><code class="xref py py-func docutils literal"><span class="pre">gaussian_mixture_2d()</span></code></a>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">gaussian_mixture_2d</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/gmm-2.py">Source code</a>, <a class="reference external" href="../examples/gmm-2.png">png</a>, <a class="reference external" href="../examples/gmm-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/gmm-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/gmm-2.png" src="_images/gmm-2.png" />
</div>
<p>The function is called with <code class="docutils literal"><span class="pre">scale=2</span></code> which means that each ellipse shows two
standard deviations.  From the ten cluster components, the model uses
effectively the correct number of clusters (4).  These clusters capture the true
density accurately.</p>
<p>In addition to clustering and density estimation, this model could also be used
for classification by setting the known class assignments as observed.</p>
</div>
<div class="section" id="advanced-next-steps">
<h4>Advanced next steps<a class="headerlink" href="#advanced-next-steps" title="Permalink to this headline">¶</a></h4>
<div class="section" id="joint-node-for-mean-and-precision">
<h5>Joint node for mean and precision<a class="headerlink" href="#joint-node-for-mean-and-precision" title="Permalink to this headline">¶</a></h5>
<p>The next step for improving the results could be to use <a class="reference internal" href="index.html#bayespy.nodes.GaussianWishart" title="bayespy.nodes.GaussianWishart"><code class="xref py py-class docutils literal"><span class="pre">GaussianWishart</span></code></a>
node for modelling the mean vectors <code class="docutils literal"><span class="pre">mu</span></code> and precision matrices <code class="docutils literal"><span class="pre">Lambda</span></code>
jointly without factorization.  This should improve the accuracy of the
posterior approximation and the speed of the VB estimation.  However, the
implementation is a bit more complex.</p>
</div>
<div class="section" id="fast-collapsed-inference">
<h5>Fast collapsed inference<a class="headerlink" href="#fast-collapsed-inference" title="Permalink to this headline">¶</a></h5>
</div>
</div>
</div>
<span id="document-examples/bmm"></span><div class="section" id="bernoulli-mixture-model">
<h3>Bernoulli mixture model<a class="headerlink" href="#bernoulli-mixture-model" title="Permalink to this headline">¶</a></h3>
<p>This example considers data generated from a Bernoulli mixture model.  One
simple example process could be a questionnaire for election candidates.  We
observe a set of binary vectors, where each vector represents a candidate in the
election and each element in these vectors correspond to a candidate&#8217;s answer to
a yes-or-no question.  The goal is to find groups of similar candidates and
analyze the answer patterns of these groups.</p>
<div class="section" id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h4>
<p>First, we generate artificial data to analyze.  Let us assume that the
questionnaire contains ten yes-or-no questions.  We assume that there are three
groups with similar opinions.  These groups could represent parties.  These
groups have the following answering patterns, which are represented by vectors
with probabilities of a candidate answering yes to the questions:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p1</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p2</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
</pre></div>
</div>
<p>Thus, the candidates in the first group are likely to answer no to questions 1,
3, 5, 7 and 9, and yes to questions 2, 4, 6, 8, 10.  The candidates in the
second group are likely to answer yes to the last five questions, whereas the
candidates in the third group are likely to answer yes to the first five
questions.  For convenience, we form a NumPy array of these vectors:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">])</span>
</pre></div>
</div>
<p>Next, we generate a hundred candidates.  First, we randomly select the group for
each candidate:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.utils</span> <span class="k">import</span> <span class="n">random</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">categorical</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the group patterns, we generate yes-or-no answers for the candidates:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="n">z</span><span class="p">])</span>
</pre></div>
</div>
<p>This is our simulated data to be analyzed.</p>
</div>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>Now, we construct a model for learning the structure in the data.  We have a
dataset of hundred 10-dimensional binary vectors:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>We will create a Bernoulli mixture model.  We assume that the true number of
groups is unknown to us, so we use a large enough number of clusters:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>We use the categorical distribution for the group assignments and give the group
assignment probabilities an uninformative Dirichlet prior:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">Dirichlet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">R</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="n">K</span><span class="o">*</span><span class="p">[</span><span class="mf">1e-5</span><span class="p">],</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;R&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">R</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Z&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Each group has a probability of a yes answer for each question.  These
probabilities are given beta priors:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Beta</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>         <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">K</span><span class="p">),</span>
<span class="gp">... </span>         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;P&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The answers of the candidates are modelled with the Bernoulli distribution:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Mixture</span><span class="p">,</span> <span class="n">Bernoulli</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Bernoulli</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
<p>Here, <code class="docutils literal"><span class="pre">Z</span></code> defines the group assignments and <code class="docutils literal"><span class="pre">P</span></code> the answering probability
patterns for each group.  Note how the plates of the nodes are matched: <code class="docutils literal"><span class="pre">Z</span></code>
has plates <code class="docutils literal"><span class="pre">(N,1)</span></code> and <code class="docutils literal"><span class="pre">P</span></code> has plates <code class="docutils literal"><span class="pre">(D,K)</span></code>, but in the mixture node the
last plate axis of <code class="docutils literal"><span class="pre">P</span></code> is discarded and thus the node broadcasts plates
<code class="docutils literal"><span class="pre">(N,1)</span></code> and <code class="docutils literal"><span class="pre">(D,)</span></code> resulting in plates <code class="docutils literal"><span class="pre">(N,D)</span></code> for <code class="docutils literal"><span class="pre">X</span></code>.</p>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>In order to infer the variables in our model, we construct a variational
Bayesian inference engine:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
<p>This also gives the default update order of the nodes.  In order to find
different groups, they must be initialized differently, thus we use random
initialization for the group probability patterns:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>We provide our simulated data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can run inference:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-6.872145e+02 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 17: loglike=-5.236921e+02 (... seconds)</span>
<span class="go">Converged at iteration 17.</span>
</pre></div>
</div>
<p>The algorithm converges in 17 iterations.</p>
</div>
<div class="section" id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h4>
<p>Now we can examine the approximate posterior distribution.  First, let us plot
the group assignment probabilities:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/bmm-1.py">Source code</a>, <a class="reference external" href="../examples/bmm-1.png">png</a>, <a class="reference external" href="../examples/bmm-1.hires.png">hires.png</a>, <a class="reference external" href="../examples/bmm-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/bmm-1.png" src="_images/bmm-1.png" />
</div>
<p>This plot shows that there are three dominant groups, which is equal to the true
number of groups used to generate the data.  However, there are still two
smaller groups as the data does not give enough evidence to prune them out.  The
yes-or-no answer probability patterns for the groups can be plotted as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/bmm-2.py">Source code</a>, <a class="reference external" href="../examples/bmm-2.png">png</a>, <a class="reference external" href="../examples/bmm-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/bmm-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/bmm-2.png" src="_images/bmm-2.png" />
</div>
<p>The three dominant groups have found the true patterns accurately.  The patterns
of the two minor groups some kind of mixtures of the three groups and they exist
because the generated data happened to contain a few samples giving evidence for
these groups.  Finally, we can plot the group assignment probabilities for the
candidates:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/bmm-3.py">Source code</a>, <a class="reference external" href="../examples/bmm-3.png">png</a>, <a class="reference external" href="../examples/bmm-3.hires.png">hires.png</a>, <a class="reference external" href="../examples/bmm-3.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/bmm-3.png" src="_images/bmm-3.png" />
</div>
<p>This plot shows the clustering of the candidates.  It is possible to use
<a class="reference internal" href="index.html#bayespy.plot.HintonPlotter" title="bayespy.plot.HintonPlotter"><code class="xref py py-class docutils literal"><span class="pre">HintonPlotter</span></code></a> to enable monitoring during the VB iteration by providing
<code class="docutils literal"><span class="pre">plotter=HintonPlotter()</span></code> for <code class="docutils literal"><span class="pre">Z</span></code>, <code class="docutils literal"><span class="pre">P</span></code> and <code class="docutils literal"><span class="pre">R</span></code> when creating the nodes.</p>
</div>
</div>
<span id="document-examples/hmm"></span><div class="section" id="hidden-markov-model">
<h3>Hidden Markov model<a class="headerlink" href="#hidden-markov-model" title="Permalink to this headline">¶</a></h3>
<p>In this example, we will demonstrate the use of hidden Markov model in the case
of known and unknown parameters.  We will also use two different emission
distributions to demonstrate the flexibility of the model construction.</p>
<div class="section" id="known-parameters">
<h4>Known parameters<a class="headerlink" href="#known-parameters" title="Permalink to this headline">¶</a></h4>
<p>This example follows the one presented in <a class="reference external" href="http://en.wikipedia.org/wiki/Hidden_Markov_model#A_concrete_example">Wikipedia</a>.</p>
<div class="section" id="model">
<h5>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h5>
<p>Each day, the state of the weather is either &#8216;rainy&#8217; or &#8216;sunny&#8217;. The weather
follows a first-order discrete Markov process.  It has the following initial
state probabilities</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span> <span class="c1"># p(rainy)=0.6, p(sunny)=0.4</span>
</pre></div>
</div>
<p>and state transition probabilities:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="c1"># p(rainy-&gt;rainy)=0.7, p(rainy-&gt;sunny)=0.3</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]</span> <span class="c1"># p(sunny-&gt;rainy)=0.4, p(sunny-&gt;sunny)=0.6</span>
</pre></div>
</div>
<p>We will be observing one hundred samples:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<p>The discrete first-order Markov chain is constructed as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">CategoricalMarkovChain</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">CategoricalMarkovChain</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
</pre></div>
</div>
<p>However, instead of observing this process directly, we observe whether Bob is
&#8216;walking&#8217;, &#8216;shopping&#8217; or &#8216;cleaning&#8217;. The probability of each activity depends on
the current weather as follows:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">P</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]]</span>
</pre></div>
</div>
<p>where the first row contains activity probabilities on a rainy weather and the
second row contains activity probabilities on a sunny weather.  Using these
emission probabilities, the observed process is constructed as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">Mixture</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="data">
<h5>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h5>
<p>In order to test our method, we&#8217;ll generate artificial data from the model
itself.  First, draw realization of the weather process:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">weather</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
<p>Then, using this weather, draw realizations of the activities:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activity</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">weather</span><span class="p">,</span> <span class="n">Categorical</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h5>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h5>
<p>Now, using this data, we set our variable <img class="math" src="_images/math/0062c26909b3e07ee8f5a6285b2563d69bc979ff.svg" alt="Y"/> to be observed:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">activity</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to run inference, we construct variational Bayesian inference engine:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that we need to give all random variables to <code class="docutils literal"><span class="pre">VB</span></code>. In this case, the only
random variables were <code class="docutils literal"><span class="pre">Y</span></code> and <code class="docutils literal"><span class="pre">Z</span></code>. Next we run the inference, that is,
compute our posterior distribution:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
<span class="go">Iteration 1: loglike=-1.095883e+02 (... seconds)</span>
</pre></div>
</div>
<p>In this case, because there is only one unobserved random variable, we
recover the exact posterior distribution and there is no need to iterate
more than one step.</p>
</div>
<div class="section" id="results">
<h5>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h5>
<p>One way to plot a 2-class categorical timeseries is to use the basic
<a class="reference internal" href="index.html#bayespy.plot.plot" title="bayespy.plot.plot"><code class="xref py py-func docutils literal"><span class="pre">plot()</span></code></a> function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">weather</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/hmm-1.py">Source code</a>, <a class="reference external" href="../examples/hmm-1.png">png</a>, <a class="reference external" href="../examples/hmm-1.hires.png">hires.png</a>, <a class="reference external" href="../examples/hmm-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/hmm-1.png" src="_images/hmm-1.png" />
</div>
<p>The black line shows the posterior probability of rain and the red line and
crosses show the true state.  Clearly, the method is not able to infer the
weather very accurately in this case because the activies do not give that much
information about the weather.</p>
</div>
</div>
<div class="section" id="unknown-parameters">
<h4>Unknown parameters<a class="headerlink" href="#unknown-parameters" title="Permalink to this headline">¶</a></h4>
<p>In this example, we consider unknown parameters for the Markov process and
different emission distribution.</p>
<div class="section" id="id1">
<h5>Data<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>We generate data from three 2-dimensional Gaussian distributions with different
mean vectors and common standard deviation:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">std</span> <span class="o">=</span> <span class="mf">2.0</span>
</pre></div>
</div>
<p>Thus, the number of clusters is three:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
<p>And the number of samples is 200:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>
</div>
<p>Each initial state is equally probable:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span>
</pre></div>
</div>
<p>State transition matrix is such that with probability 0.9 the process stays in
the same state.  The probability to move one of the other two states is 0.05 for
both of those states.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">K</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P</span> <span class="o">=</span> <span class="n">q</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="n">r</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>Simulate the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">z</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span>
<span class="gp">... </span>    <span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">std</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">mu</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
<span class="gp">... </span>    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>
</pre></div>
</div>
<p>Then, let us visualize the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="go">(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]][</span><span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="p">)]</span> <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">z</span> <span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="go">&lt;matplotlib.collections.PathCollection object at 0x...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/hmm-2.py">Source code</a>, <a class="reference external" href="../examples/hmm-2.png">png</a>, <a class="reference external" href="../examples/hmm-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/hmm-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/hmm-2.png" src="_images/hmm-2.png" />
</div>
<p>Consecutive states are connected by a solid black line and the dot color shows
the true class.</p>
</div>
<div class="section" id="id2">
<h5>Model<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>Now, assume that we do not know the parameters of the process (initial state
probability and state transition probabilities). We give these parameters quite
non-informative priors, but it is possible to provide more informative priors if
such information is available:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Dirichlet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a0</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="mf">1e-3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">Dirichlet</span><span class="p">(</span><span class="mf">1e-3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">K</span><span class="p">)))</span>
</pre></div>
</div>
<p>The discrete Markov chain is constructed as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Z</span> <span class="o">=</span> <span class="n">CategoricalMarkovChain</span><span class="p">(</span><span class="n">a0</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, instead of using categorical emission distribution as before, we&#8217;ll use
Gaussian distribution.  For simplicity, we use the true parameters of the
Gaussian distributions instead of giving priors and estimating them.  The known
standard deviation can be converted to a precision matrix as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">std</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Thus, the observed process is a Gaussian mixture with cluster assignments from
the hidden Markov process <code class="docutils literal"><span class="pre">Z</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">Gaussian</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">Gaussian</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal"><span class="pre">Lambda</span></code> does not have cluster plate axis because it is shared
between the clusters.</p>
</div>
<div class="section" id="id3">
<h5>Inference<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<p>Let us use the simulated data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Because <code class="docutils literal"><span class="pre">VB</span></code> takes all the random variables, we need to provide <code class="docutils literal"><span class="pre">A</span></code> and
<code class="docutils literal"><span class="pre">a0</span></code> also:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">a0</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, run VB iteration until convergence:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-9.963054e+02 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 8: loglike=-9.235053e+02 (... seconds)</span>
<span class="go">Converged at iteration 8.</span>
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h5>Results<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<p>Plot the classification of the data similarly as the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="go">(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">colors</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_moments</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k-&#39;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="go">&lt;matplotlib.collections.PathCollection object at 0x...&gt;</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/hmm-3.py">Source code</a>, <a class="reference external" href="../examples/hmm-3.png">png</a>, <a class="reference external" href="../examples/hmm-3.hires.png">hires.png</a>, <a class="reference external" href="../examples/hmm-3.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/hmm-3.png" src="_images/hmm-3.png" />
</div>
<p>The data has been classified quite correctly.  Even samples that are more in the
region of another cluster are classified correctly if the previous and next
sample provide enough evidence for the correct class.  We can also plot the
state transition matrix:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/hmm-4.py">Source code</a>, <a class="reference external" href="../examples/hmm-4.png">png</a>, <a class="reference external" href="../examples/hmm-4.hires.png">hires.png</a>, <a class="reference external" href="../examples/hmm-4.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/hmm-4.png" src="_images/hmm-4.png" />
</div>
<p>Clearly, the learned state transition matrix is close to the true matrix.  The
models described above could also be used for classification by providing the
known class assignments as observed data to <code class="docutils literal"><span class="pre">Z</span></code> and the unknown class
assignments as missing data.</p>
</div>
</div>
</div>
<span id="document-examples/pca"></span><div class="section" id="principal-component-analysis">
<h3>Principal component analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline">¶</a></h3>
<p>This example uses a simple principal component analysis to find a
two-dimensional latent subspace in a higher dimensional dataset.</p>
<div class="section" id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h4>
<p>Let us create a Gaussian dataset with latent space dimensionality two and some
observation noise:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="mi">20</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ik,jk-&gt;ij&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>We will use 10-dimensional latent space in our model and let it learn the true
dimensionality:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>Import relevant nodes:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">GaussianARD</span><span class="p">,</span> <span class="n">Gamma</span><span class="p">,</span> <span class="n">SumMultiply</span>
</pre></div>
</div>
<p>The latent states:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,))</span>
</pre></div>
</div>
<p>The loading matrix with automatic relevance determination (ARD) prior:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,))</span>
</pre></div>
</div>
<p>Compute the dot product of the latent states and the loading matrix:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">SumMultiply</span><span class="p">(</span><span class="s1">&#39;d,d-&gt;&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
<p>The observation noise:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
<p>The observed variable:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>Observe the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>We do not have missing data now, but they could be easily handled with <code class="docutils literal"><span class="pre">mask</span></code>
keyword argument.  Construct variational Bayesian (VB) inference engine:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>Initialize the latent subspace randomly, otherwise both <code class="docutils literal"><span class="pre">X</span></code> and <code class="docutils literal"><span class="pre">C</span></code> would
converge to zero:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>Now we could use <a class="reference internal" href="index.html#bayespy.inference.VB.update" title="bayespy.inference.VB.update"><code class="xref py py-func docutils literal"><span class="pre">VB.update()</span></code></a> to run the inference.  However, let us first
create a parameter expansion to speed up the inference.  The expansion is based
on rotating the latent subspace optimally.  This is optional but will usually
improve the speed of the inference significantly, especially in high-dimensional
problems:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference.vmp.transformations</span> <span class="k">import</span> <span class="n">RotateGaussianARD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rot_X</span> <span class="o">=</span> <span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rot_C</span> <span class="o">=</span> <span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<p>By giving <code class="docutils literal"><span class="pre">alpha</span></code> for <code class="docutils literal"><span class="pre">rot_C</span></code>, the rotation will also optimize <code class="docutils literal"><span class="pre">alpha</span></code>
jointly with <code class="docutils literal"><span class="pre">C</span></code>.  Now that we have defined the rotations for our variables,
we need to construct an optimizer:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference.vmp.transformations</span> <span class="k">import</span> <span class="n">RotationOptimizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">R</span> <span class="o">=</span> <span class="n">RotationOptimizer</span><span class="p">(</span><span class="n">rot_X</span><span class="p">,</span> <span class="n">rot_C</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to use the rotations automatically, we need to set it as a callback
function:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">set_callback</span><span class="p">(</span><span class="n">R</span><span class="o">.</span><span class="n">rotate</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information about the rotation parameter expansion, see
<a class="reference internal" href="index.html#luttinen-2010" id="id1">[7]</a> and <a class="reference internal" href="index.html#luttinen-2013" id="id2">[6]</a>.  Now we can run the actual
inference until convergence:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-2.33...e+03 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration ...: loglike=6.500...e+02 (... seconds)</span>
<span class="go">Converged at iteration ...</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h4>
<p>The results can be visualized, for instance, by plotting the Hinton diagram of
the loading matrix:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/pca-1.py">Source code</a>, <a class="reference external" href="../examples/pca-1.png">png</a>, <a class="reference external" href="../examples/pca-1.hires.png">hires.png</a>, <a class="reference external" href="../examples/pca-1.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/pca-1.png" src="_images/pca-1.png" />
</div>
<p>The method has been able to prune out unnecessary latent dimensions and keep two
components, which is the true number of components.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/pca-2.py">Source code</a>, <a class="reference external" href="../examples/pca-2.png">png</a>, <a class="reference external" href="../examples/pca-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/pca-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/pca-2.png" src="_images/pca-2.png" />
</div>
<p>The reconstruction of the noiseless function values are practically perfect in
this simple example.  Larger noise variance, more latent space dimensions and
missing values would make this problem more difficult.  The model construction
could also be improved by having, for instance, <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">tau</span></code> in the same
node without factorizing between them in the posterior approximation.  This can
be achieved by using <code class="xref py py-class docutils literal"><span class="pre">GaussianGammaISO</span></code> node.</p>
</div>
</div>
<span id="document-examples/lssm"></span><div class="section" id="linear-state-space-model">
<h3>Linear state-space model<a class="headerlink" href="#linear-state-space-model" title="Permalink to this headline">¶</a></h3>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>In linear state-space models a sequence of <img class="math" src="_images/math/450a8e2c2320d77181e0d4fc68c947e9a5de8ecb.svg" alt="M"/>-dimensional observations
<img class="math" src="_images/math/c132005fc24663352e267edecabe964a4ada811c.svg" alt="\mathbf{Y}=(\mathbf{y}_1,\ldots,\mathbf{y}_N)"/> is assumed to be generated
from latent <img class="math" src="_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.svg" alt="D"/>-dimensional states
<img class="math" src="_images/math/b1f5fe9d32f5432a4f9af25792ecd8817f68c874.svg" alt="\mathbf{X}=(\mathbf{x}_1,\ldots,\mathbf{x}_N)"/> which follow a first-order
Markov process:</p>
<div class="math">
<p><img src="_images/math/0336e4362a8bace6713a60d0d38b8530dd6a8782.svg" alt="\begin{aligned}
\mathbf{x}_{n} &amp;= \mathbf{A}\mathbf{x}_{n-1} + \text{noise} \,,
\\
\mathbf{y}_{n} &amp;= \mathbf{C}\mathbf{x}_{n} + \text{noise} \,,
\end{aligned}"/></p>
</div><p>where the noise is Gaussian, <img class="math" src="_images/math/1b009ec88d176a0e31309bf4a536fcdeedb9f38e.svg" alt="\mathbf{A}"/> is the <img class="math" src="_images/math/9a902648ab1ed79231e73a1023c183e4186d2488.svg" alt="D\times D"/> state
dynamics matrix and <img class="math" src="_images/math/4b332c34a61a965d9977bc36f6619b062c4e239f.svg" alt="\mathbf{C}"/> is the <img class="math" src="_images/math/a5f6bcf6f09bf05ae9d5c5a1ad520e4b26d93368.svg" alt="M\times D"/> loading
matrix. Usually, the latent space dimensionality <img class="math" src="_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.svg" alt="D"/> is assumed to be much
smaller than the observation space dimensionality <img class="math" src="_images/math/450a8e2c2320d77181e0d4fc68c947e9a5de8ecb.svg" alt="M"/> in order to model
the dependencies of high-dimensional observations efficiently.</p>
<p>In order to construct the model in BayesPy, first import relevant nodes:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.nodes</span> <span class="k">import</span> <span class="n">GaussianARD</span><span class="p">,</span> <span class="n">GaussianMarkovChain</span><span class="p">,</span> <span class="n">Gamma</span><span class="p">,</span> <span class="n">Dot</span>
</pre></div>
</div>
<p>The data vectors will be 30-dimensional:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">M</span> <span class="o">=</span> <span class="mi">30</span>
</pre></div>
</div>
<p>There will be 400 data vectors:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">400</span>
</pre></div>
</div>
<p>Let us use 10-dimensional latent space:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">D</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>The state dynamics matrix <img class="math" src="_images/math/1b009ec88d176a0e31309bf4a536fcdeedb9f38e.svg" alt="\mathbf{A}"/> has ARD prior:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">alpha</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <img class="math" src="_images/math/1b009ec88d176a0e31309bf4a536fcdeedb9f38e.svg" alt="\mathbf{A}"/> is a <img class="math" src="_images/math/dc56312eb2dee9ad0ccec7223f45caf181d93c79.svg" alt="D\times{}D"/>-dimensional matrix.
However, in BayesPy it is modelled as a collection (<code class="docutils literal"><span class="pre">plates=(D,)</span></code>) of
<img class="math" src="_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.svg" alt="D"/>-dimensional vectors (<code class="docutils literal"><span class="pre">shape=(D,)</span></code>) because this is how the variables
factorize in the posterior approximation of the state dynamics matrix in
<a class="reference internal" href="index.html#bayespy.nodes.GaussianMarkovChain" title="bayespy.nodes.GaussianMarkovChain"><code class="xref py py-class docutils literal"><span class="pre">GaussianMarkovChain</span></code></a>.  The latent states are constructed as</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">GaussianMarkovChain</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="mf">1e-3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">A</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">D</span><span class="p">),</span>
<span class="gp">... </span>                        <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>where the first two arguments are the mean and precision matrix of the initial
state, the third argument is the state dynamics matrix and the fourth argument
is the diagonal elements of the precision matrix of the innovation noise.  The
node also needs the length of the chain given as the keyword argument <code class="docutils literal"><span class="pre">n=N</span></code>.
Thus, the shape of this node is <code class="docutils literal"><span class="pre">(N,D)</span></code>.</p>
<p>The linear mapping from the latent space to the observation space is modelled
with the loading matrix which has ARD prior:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>              <span class="n">name</span><span class="o">=</span><span class="s1">&#39;gamma&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">gamma</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">D</span><span class="p">,),</span>
<span class="gp">... </span>                <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the plates for <code class="docutils literal"><span class="pre">C</span></code> are <code class="docutils literal"><span class="pre">(M,1)</span></code>, thus the full shape of the node is
<code class="docutils literal"><span class="pre">(M,1,D)</span></code>.  The unit plate axis is added so that <code class="docutils literal"><span class="pre">C</span></code> broadcasts with <code class="docutils literal"><span class="pre">X</span></code>
when computing the dot product:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span> <span class="o">=</span> <span class="n">Dot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">X</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">name</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This dot product is computed over the <img class="math" src="_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.svg" alt="D"/>-dimensional latent space, thus
the result is a <img class="math" src="_images/math/d02b5a872ee117993cfd45e635ea76408a6b5af8.svg" alt="M\times{}N"/>-dimensional matrix which is now represented
with plates <code class="docutils literal"><span class="pre">(M,N)</span></code> in BayesPy:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">F</span><span class="o">.</span><span class="n">plates</span>
<span class="go">(30, 400)</span>
</pre></div>
</div>
<p>We also need to use random initialization either for <code class="docutils literal"><span class="pre">C</span></code> or <code class="docutils literal"><span class="pre">X</span></code> in order to
find non-zero latent space because by default both <code class="docutils literal"><span class="pre">C</span></code> and <code class="docutils literal"><span class="pre">X</span></code> are
initialized to zero because of their prior distributions.  We use random
initialization for <code class="docutils literal"><span class="pre">C</span></code> and then we must update <code class="docutils literal"><span class="pre">X</span></code> the first time before
updating <code class="docutils literal"><span class="pre">C</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>The precision of the observation noise is given gamma prior:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>            <span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>            <span class="n">name</span><span class="o">=</span><span class="s1">&#39;tau&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The observations are noisy versions of the dot products:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">GaussianARD</span><span class="p">(</span><span class="n">F</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">tau</span><span class="p">,</span>
<span class="gp">... </span>                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The variational Bayesian inference engine is then construced as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal"><span class="pre">X</span></code> is given before <code class="docutils literal"><span class="pre">C</span></code>, thus <code class="docutils literal"><span class="pre">X</span></code> is updated before <code class="docutils literal"><span class="pre">C</span></code> by
default.</p>
</div>
<div class="section" id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h4>
<p>Now, let us generate some toy data for our model.  Our true latent space is four
dimensional with two noisy oscillator components, one random walk component and
one white noise component.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">w</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">w</span><span class="p">),</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">0</span><span class="p">,</span>         <span class="mi">0</span><span class="p">,</span>          <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>              <span class="p">[</span><span class="mi">0</span><span class="p">,</span>         <span class="mi">0</span><span class="p">,</span>          <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<p>The true linear mapping is just random:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, generate the latent states and the observations using the model equations:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">M</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">M</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">f</span><span class="p">[:,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>    <span class="n">y</span><span class="p">[:,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">f</span><span class="p">[:,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
<p>We want to simulate missing values, thus we create a mask which randomly removes
80% of the data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.utils</span> <span class="k">import</span> <span class="n">random</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">mask</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>As we did not define plotters for our nodes when creating the model, it is done
now for some of the nodes:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">FunctionPlotter</span><span class="p">(</span><span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">A</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">HintonPlotter</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">C</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">HintonPlotter</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tau</span><span class="o">.</span><span class="n">set_plotter</span><span class="p">(</span><span class="n">bpplt</span><span class="o">.</span><span class="n">PDFPlotter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)))</span>
</pre></div>
</div>
<p>This enables plotting of the approximate posterior distributions during VB
learning.  The inference engine can be run using <code class="xref py py-func docutils literal"><span class="pre">VB.update()</span></code> method:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="go">Iteration 1: loglike=-1.439704e+05 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 10: loglike=-1.051441e+04 (... seconds)</span>
</pre></div>
</div>
<p>The iteration progresses a bit slowly, thus we&#8217;ll consider parameter expansion
to speed it up.</p>
<div class="section" id="parameter-expansion">
<h5>Parameter expansion<a class="headerlink" href="#parameter-expansion" title="Permalink to this headline">¶</a></h5>
<p>Section <a class="reference internal" href="index.html#sec-parameter-expansion"><span class="std std-ref">Parameter expansion</span></a> discusses parameter expansion for
state-space models to speed up inference.  It is based on a rotating the latent
space such that the posterior in the observation space is not affected:</p>
<div class="math">
<p><img src="_images/math/04c1be6495be9e4825c2cfd084d2aa79f85cd7e2.svg" alt="\mathbf{y}_n = \mathbf{C}\mathbf{x}_n =
(\mathbf{C}\mathbf{R}^{-1}) (\mathbf{R}\mathbf{x}_n) \,."/></p>
</div><p>Thus, the transformation is
<img class="math" src="_images/math/19d6470a3a4b3bc1b9492bf5c25e3c8540d07416.svg" alt="\mathbf{C}\rightarrow\mathbf{C}\mathbf{R}^{-1}"/> and
<img class="math" src="_images/math/dd1b6c26ac1eddbf8e2e4a88a99fbcb505dae14a.svg" alt="\mathbf{X}\rightarrow\mathbf{R}\mathbf{X}"/>.  In order to keep the
dynamics of the latent states unaffected by the transformation, the state
dynamics matrix <img class="math" src="_images/math/1b009ec88d176a0e31309bf4a536fcdeedb9f38e.svg" alt="\mathbf{A}"/> must be transformed accordingly:</p>
<div class="math">
<p><img src="_images/math/57c83c2ecd62626ae465608d401224e4df104314.svg" alt="\mathbf{R}\mathbf{x}_n = \mathbf{R}\mathbf{A}\mathbf{R}^{-1}
\mathbf{R}\mathbf{x}_{n-1} \,,"/></p>
</div><p>resulting in a transformation
<img class="math" src="_images/math/18cbc50b077ba7e76664b46e5bf82b1521473ce6.svg" alt="\mathbf{A}\rightarrow\mathbf{R}\mathbf{A}\mathbf{R}^{-1}"/>.  For more
details, refer to <a class="reference internal" href="index.html#luttinen-2013" id="id1">[6]</a> and <a class="reference internal" href="index.html#luttinen-2010" id="id2">[7]</a>.  In BayesPy,
the transformations are available in
<code class="xref py py-mod docutils literal"><span class="pre">bayespy.inference.vmp.transformations</span></code>:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference.vmp</span> <span class="k">import</span> <span class="n">transformations</span>
</pre></div>
</div>
<p>The rotation of the loading matrix along with the ARD parameters is defined as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rotC</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
<p>For rotating <code class="docutils literal"><span class="pre">X</span></code>, we first need to define the rotation of the state dynamics
matrix:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rotA</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianARD</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can define the rotation of the latent states:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rotX</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotateGaussianMarkovChain</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">rotA</span><span class="p">)</span>
</pre></div>
</div>
<p>The optimal rotation for all these variables is found using rotation optimizer:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">R</span> <span class="o">=</span> <span class="n">transformations</span><span class="o">.</span><span class="n">RotationOptimizer</span><span class="p">(</span><span class="n">rotX</span><span class="p">,</span> <span class="n">rotC</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</pre></div>
</div>
<p>Set the parameter expansion to be applied after each iteration:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">callback</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">rotate</span>
</pre></div>
</div>
<p>Now, run iterations until convergence:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration 11: loglike=-1.010...e+04 (... seconds)</span>
<span class="gp">...</span>
<span class="go">Iteration 58: loglike=-8.906...e+03 (... seconds)</span>
<span class="go">Converged at iteration ...</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h4>
<p>Because we have set the plotters, we can plot those nodes as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/lssm-1.py">Source code</a>)</p>
<div class="figure" id="id3">
<img alt="_images/lssm-1_00.png" src="_images/lssm-1_00.png" />
<p class="caption"><span class="caption-text">(<a class="reference external" href="../examples/lssm-1_00.png">png</a>, <a class="reference external" href="../examples/lssm-1_00.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_00.pdf">pdf</a>)</span></p>
</div>
<div class="figure" id="id4">
<img alt="_images/lssm-1_01.png" src="_images/lssm-1_01.png" />
<p class="caption"><span class="caption-text">(<a class="reference external" href="../examples/lssm-1_01.png">png</a>, <a class="reference external" href="../examples/lssm-1_01.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_01.pdf">pdf</a>)</span></p>
</div>
<div class="figure" id="id5">
<img alt="_images/lssm-1_02.png" src="_images/lssm-1_02.png" />
<p class="caption"><span class="caption-text">(<a class="reference external" href="../examples/lssm-1_02.png">png</a>, <a class="reference external" href="../examples/lssm-1_02.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_02.pdf">pdf</a>)</span></p>
</div>
<div class="figure" id="id6">
<img alt="_images/lssm-1_03.png" src="_images/lssm-1_03.png" />
<p class="caption"><span class="caption-text">(<a class="reference external" href="../examples/lssm-1_03.png">png</a>, <a class="reference external" href="../examples/lssm-1_03.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-1_03.pdf">pdf</a>)</span></p>
</div>
<p>There are clearly four effective components in <code class="docutils literal"><span class="pre">X</span></code>: random walk (component
number 1), random oscillation (7 and 10), and white noise (9).  These dynamics
are also visible in the state dynamics matrix Hinton diagram.  Note that the
white noise component does not have any dynamics.  Also <code class="docutils literal"><span class="pre">C</span></code> shows only four
effective components.  The posterior of <code class="docutils literal"><span class="pre">tau</span></code> captures the true value
<img class="math" src="_images/math/b3abb459f0e762d6f940fbb704fc615eb8618a59.svg" alt="3^{-2}\approx0.111"/> accurately.  We can also plot predictions in the
observation space:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>(<a class="reference external" href="../examples/lssm-2.py">Source code</a>, <a class="reference external" href="../examples/lssm-2.png">png</a>, <a class="reference external" href="../examples/lssm-2.hires.png">hires.png</a>, <a class="reference external" href="../examples/lssm-2.pdf">pdf</a>)</p>
<div class="figure">
<img alt="_images/lssm-2.png" src="_images/lssm-2.png" />
</div>
<p>We can also measure the performance numerically by computing root-mean-square
error (RMSE) of the missing values:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.utils</span> <span class="k">import</span> <span class="n">misc</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">misc</span><span class="o">.</span><span class="n">rmse</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">F</span><span class="o">.</span><span class="n">get_moments</span><span class="p">()[</span><span class="mi">0</span><span class="p">][</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
<span class="go">5.18...</span>
</pre></div>
</div>
<p>This is relatively close to the standard deviation of the noise (3), so the
predictions are quite good considering that only 20% of the data was used.</p>
</div>
</div>
<span id="document-examples/lda"></span><div class="section" id="latent-dirichlet-allocation">
<h3>Latent Dirichlet allocation<a class="headerlink" href="#latent-dirichlet-allocation" title="Permalink to this headline">¶</a></h3>
<p>Latent Dirichlet allocation is a widely used topic model.  The data is a
collection of documents which contain words.  The goal of the analysis is to
find topics (distribution of words in topics) and document topics (distribution
of topics in documents).</p>
<div class="section" id="data">
<h4>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h4>
<p>The data consists of two vectors of equal length.  The elements in these vectors
correspond to the words in all documents combined.  If there were <img class="math" src="_images/math/450a8e2c2320d77181e0d4fc68c947e9a5de8ecb.svg" alt="M"/>
documents and each document had <img class="math" src="_images/math/684381a21cd73ebbf43b63a087d3f7410ee99ce8.svg" alt="K"/> words, the vectors contain <img class="math" src="_images/math/46574264fb99d2fb7eb3d3dd613a4fc19edc9dcc.svg" alt="M
\cdot K"/> elements.  Let <img class="math" src="_images/math/450a8e2c2320d77181e0d4fc68c947e9a5de8ecb.svg" alt="M"/> be the number of documents in total.  The
first vector gives each word a document index <img class="math" src="_images/math/699a641b1213ba4977d780fc5c6345e8d826130e.svg" alt="i\in \{0,\ldots,M-1\}"/>
defining to which document the word belongs.  Let <img class="math" src="_images/math/f4170ed8938b79490d8923857962695514a8e4cb.svg" alt="N"/> be the size of the
whole available vocabulary.  The second vector gives each word a vocabulary
index <img class="math" src="_images/math/a424e8c93fdba3244004d1ce8cc93f7fb3c09a61.svg" alt="j\in \{0,\ldots,N-1\}"/> defining which word it is from the
vocabulary.</p>
<p>For this demo, we will just generate an artificial dataset for simplicity.  We
use the LDA model itself to generate the dataset.  First, import relevant
packages:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy</span> <span class="k">import</span> <span class="n">nodes</span>
</pre></div>
</div>
<p>Let us decide the number of documents and the number of words in those documents:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n_documents</span> <span class="o">=</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_words</span> <span class="o">=</span> <span class="mi">10000</span>
</pre></div>
</div>
<p>Randomly choose into which document each word belongs to:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">word_documents</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_documents</span><span class="p">)</span><span class="o">/</span><span class="n">n_documents</span><span class="p">,</span>
<span class="gp">... </span>                                   <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_words</span><span class="p">,))</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
<p>Let us also decide the size of our vocabulary:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n_vocabulary</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<p>Also, let us decide the true number of topics:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n_topics</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
<p>Generate some random distributions for the topics in each document:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_topic</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="mf">1e-1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_topics</span><span class="p">),</span>
<span class="gp">... </span>                          <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_documents</span><span class="p">,))</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
<p>Generate some random distributions for the words in each topic:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_word</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="mf">1e-1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="p">),</span>
<span class="gp">... </span>                         <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_topics</span><span class="p">,))</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
<p>Sample topic assignments for each word in each document:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">topic</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">p_topic</span><span class="p">[</span><span class="n">word_documents</span><span class="p">],</span>
<span class="gp">... </span>                          <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_words</span><span class="p">,))</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
<p>And finally, draw vocabulary indices for each word in all the documents:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">p_word</span><span class="p">[</span><span class="n">topic</span><span class="p">],</span>
<span class="gp">... </span>                           <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_words</span><span class="p">,))</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
</pre></div>
</div>
<p>Now, our dataset consists of <code class="docutils literal"><span class="pre">word_documents</span></code> and <code class="docutils literal"><span class="pre">corpus</span></code>, which define the
document and vocabulary indices for each word in our dataset.</p>
<div class="admonition-todo admonition" id="index-0">
<p class="first admonition-title">Todo</p>
<p class="last">Use some large real-world dataset, for instance, Wikipedia.</p>
</div>
</div>
<div class="section" id="model">
<h4>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h4>
<p>Variable for learning the topic distribution for each document:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_topic</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_topics</span><span class="p">),</span>
<span class="gp">... </span>                          <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_documents</span><span class="p">,),</span>
<span class="gp">... </span>                          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;p_topic&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Variable for learning the word distribution for each topic:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_word</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="p">),</span>
<span class="gp">... </span>                         <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_topics</span><span class="p">,),</span>
<span class="gp">... </span>                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;p_word&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The document indices for each word in the corpus:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference.vmp.nodes.categorical</span> <span class="k">import</span> <span class="n">CategoricalMoments</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">document_indices</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">CategoricalMoments</span><span class="p">(</span><span class="n">n_documents</span><span class="p">),</span> <span class="n">word_documents</span><span class="p">,</span>
<span class="gp">... </span>                                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;document_indices&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Variable for learning the topic assignments of each word in the corpus:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">topics</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">nodes</span><span class="o">.</span><span class="n">Gate</span><span class="p">(</span><span class="n">document_indices</span><span class="p">,</span> <span class="n">p_topic</span><span class="p">),</span>
<span class="gp">... </span>                           <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">),),</span>
<span class="gp">... </span>                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;topics&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The vocabulary indices for each word in the corpus:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">nodes</span><span class="o">.</span><span class="n">Gate</span><span class="p">(</span><span class="n">topics</span><span class="p">,</span> <span class="n">p_word</span><span class="p">),</span>
<span class="gp">... </span>                          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;words&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h4>
<p>Observe the corpus:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>Break symmetry by random initialization:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_topic</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p_word</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>Construct inference engine:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">p_word</span><span class="p">,</span> <span class="n">p_topic</span><span class="p">,</span> <span class="n">document_indices</span><span class="p">)</span>
</pre></div>
</div>
<p>Run the VB learning algorithm:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="go">Iteration ...</span>
</pre></div>
</div>
</div>
<div class="section" id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h4>
<p>Use <code class="docutils literal"><span class="pre">bayespy.plot</span></code> to plot the results:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">bayespy.plot</span> <span class="k">as</span> <span class="nn">bpplt</span>
</pre></div>
</div>
<p>Plot the topic distributions for each document:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="s1">&#39;p_topic&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior topic distribution for each document&quot;</span><span class="p">)</span>
<span class="go">&lt;matplotlib.text.Text object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Topics&quot;</span><span class="p">)</span>
<span class="go">&lt;matplotlib.text.Text object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Documents&quot;</span><span class="p">)</span>
<span class="go">&lt;matplotlib.text.Text object at 0x...&gt;</span>
</pre></div>
</div>
<p>Plot the word distributions for each topic:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">hinton</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="s1">&#39;p_word&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior word distributions for each topic&quot;</span><span class="p">)</span>
<span class="go">&lt;matplotlib.text.Text object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Words&quot;</span><span class="p">)</span>
<span class="go">&lt;matplotlib.text.Text object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Topics&quot;</span><span class="p">)</span>
<span class="go">&lt;matplotlib.text.Text object at 0x...&gt;</span>
</pre></div>
</div>
<div class="admonition-todo admonition" id="index-1">
<p class="first admonition-title">Todo</p>
<p class="last">Create more illustrative plots.</p>
</div>
</div>
<div class="section" id="stochastic-variational-inference">
<h4>Stochastic variational inference<a class="headerlink" href="#stochastic-variational-inference" title="Permalink to this headline">¶</a></h4>
<p>LDA is a popular example for stochastic variational inference (SVI).  Using SVI
for LDA is quite simple in BayesPy.  In SVI, only a subset of the dataset is
used at each iteration step but this subset is &#8220;repeated&#8221; to get the same size
as the original dataset.  Let us define a size for the subset:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">subset_size</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
<p>Thus, our subset will be repeat this many times:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">plates_multiplier</span> <span class="o">=</span> <span class="n">n_words</span> <span class="o">/</span> <span class="n">subset_size</span>
</pre></div>
</div>
<p>Note that this multiplier doesn&#8217;t need to be an integer.</p>
<p>Now, let us repeat the model construction with only one minor addition.  The
following variables are identical to previous:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_topic</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_topics</span><span class="p">),</span>
<span class="gp">... </span>                          <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_documents</span><span class="p">,),</span>
<span class="gp">... </span>                          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;p_topic&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p_word</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_vocabulary</span><span class="p">),</span>
<span class="gp">... </span>                         <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">n_topics</span><span class="p">,),</span>
<span class="gp">... </span>                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;p_word&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The document indices vector is now a bit shorter, using only a subset:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">document_indices</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">CategoricalMoments</span><span class="p">(</span><span class="n">n_documents</span><span class="p">),</span>
<span class="gp">... </span>                                  <span class="n">word_documents</span><span class="p">[:</span><span class="n">subset_size</span><span class="p">],</span>
<span class="gp">... </span>                                  <span class="n">name</span><span class="o">=</span><span class="s1">&#39;document_indices&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that at this point, it doesn&#8217;t matter which elements we chose for the
subset.  For the topic assignments of each word in the corpus we need to use
<code class="docutils literal"><span class="pre">plates_multiplier</span></code> because these topic assignments for the subset are
&#8220;repeated&#8221; to recover the full dataset:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">topics</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">nodes</span><span class="o">.</span><span class="n">Gate</span><span class="p">(</span><span class="n">document_indices</span><span class="p">,</span> <span class="n">p_topic</span><span class="p">),</span>
<span class="gp">... </span>                           <span class="n">plates</span><span class="o">=</span><span class="p">(</span><span class="n">subset_size</span><span class="p">,),</span>
<span class="gp">... </span>                           <span class="n">plates_multiplier</span><span class="o">=</span><span class="p">(</span><span class="n">plates_multiplier</span><span class="p">,),</span>
<span class="gp">... </span>                           <span class="n">name</span><span class="o">=</span><span class="s1">&#39;topics&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, the vocabulary indices for each word in the corpus are constructed as
before:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">words</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">nodes</span><span class="o">.</span><span class="n">Gate</span><span class="p">(</span><span class="n">topics</span><span class="p">,</span> <span class="n">p_word</span><span class="p">),</span>
<span class="gp">... </span>                          <span class="n">name</span><span class="o">=</span><span class="s1">&#39;words&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This node inherits the plates and multipliers from its parent <code class="docutils literal"><span class="pre">topics</span></code>, so
there is no need to define them here.  Again, break symmetry by random
initialization:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">p_topic</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">p_word</span><span class="o">.</span><span class="n">initialize_from_random</span><span class="p">()</span>
</pre></div>
</div>
<p>Construct inference engine:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayespy.inference</span> <span class="k">import</span> <span class="n">VB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span> <span class="o">=</span> <span class="n">VB</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">topics</span><span class="p">,</span> <span class="n">p_word</span><span class="p">,</span> <span class="n">p_topic</span><span class="p">,</span> <span class="n">document_indices</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to use SVI, we need to disable some lower bound checks, because the
lower bound doesn&#8217;t anymore necessarily increase at each iteration step:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Q</span><span class="o">.</span><span class="n">ignore_bound_checks</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>For the stochastic gradient ascent, we&#8217;ll define some learning parameters:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">delay</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forgetting_rate</span> <span class="o">=</span> <span class="mf">0.7</span>
</pre></div>
</div>
<p>Run the inference:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># Observe a random mini-batch</span>
<span class="gp">... </span>    <span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_words</span><span class="p">,</span> <span class="n">subset_size</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="p">[</span><span class="s1">&#39;words&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="n">subset</span><span class="p">])</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="p">[</span><span class="s1">&#39;document_indices&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">word_documents</span><span class="p">[</span><span class="n">subset</span><span class="p">])</span>
<span class="gp">... </span>    <span class="c1"># Learn intermediate variables</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s1">&#39;topics&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># Set step length</span>
<span class="gp">... </span>    <span class="n">step</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="n">delay</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="n">forgetting_rate</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># Stochastic gradient for the global variables</span>
<span class="gp">... </span>    <span class="n">Q</span><span class="o">.</span><span class="n">gradient_step</span><span class="p">(</span><span class="s1">&#39;p_topic&#39;</span><span class="p">,</span> <span class="s1">&#39;p_word&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
<span class="go">Iteration 1: ...</span>
</pre></div>
</div>
<p>If one is interested, the lower bound values during the SVI algorithm can be plotted as:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="go">&lt;matplotlib.figure.Figure object at 0x...&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bpplt</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">L</span><span class="p">)</span>
<span class="go">[&lt;matplotlib.lines.Line2D object at 0x...&gt;]</span>
</pre></div>
</div>
<p>The other results can be plotted as before.</p>
</div>
</div>
</div>
</div>
<span id="document-dev_guide/dev_guide"></span><div class="section" id="developer-guide">
<h2>Developer guide<a class="headerlink" href="#developer-guide" title="Permalink to this headline">¶</a></h2>
<p>This chapter provides basic information for developers about contributing, the
theoretical background and the core structure.  It is assumed that the reader
has read and is familiar with <a class="reference internal" href="index.html#sec-user-guide"><span class="std std-ref">User guide</span></a>.</p>
<div class="toctree-wrapper compound">
<span id="document-dev_guide/workflow"></span><div class="section" id="workflow">
<h3>Workflow<a class="headerlink" href="#workflow" title="Permalink to this headline">¶</a></h3>
<p>The main forum for BayesPy development is <a class="reference external" href="https://github.com/bayespy/bayespy">GitHub</a>.  Bugs and other issues can be reported
at <a class="reference external" href="https://github.com/bayespy/bayespy/issues">https://github.com/bayespy/bayespy/issues</a>.  Contributions to the code and
documentation are welcome and should be given as pull requests at
<a class="reference external" href="https://github.com/bayespy/bayespy/pulls">https://github.com/bayespy/bayespy/pulls</a>.  In order to create pull requests, it
is recommended to fork the git repository, make local changes and submit these
changes as a pull request.  The style guide for writing docstrings follows the
style guide of NumPy, available at
<a class="reference external" href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt">https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt</a>.  Detailed
instructions on development workflow can be read from NumPy guide, available at
<a class="reference external" href="http://docs.scipy.org/doc/numpy/dev/gitwash/development_workflow.html">http://docs.scipy.org/doc/numpy/dev/gitwash/development_workflow.html</a>.  BayesPy
uses the following acronyms to start the commit message:</p>
<blockquote>
<div><ul class="simple">
<li>API: an (incompatible) API change</li>
<li>BLD: change related to building numpy</li>
<li>BUG: bug fix</li>
<li>DEMO: modification in demo code</li>
<li>DEP: deprecate something, or remove a deprecated object</li>
<li>DEV: development tool or utility</li>
<li>DOC: documentation</li>
<li>ENH: enhancement</li>
<li>MAINT: maintenance commit (refactoring, typos, etc.)</li>
<li>REV: revert an earlier commit</li>
<li>STY: style fix (whitespace, PEP8)</li>
<li>TST: addition or modification of tests</li>
<li>REL: related to releasing</li>
</ul>
</div></blockquote>
<p>Since version 0.3.7, we have started following <a class="reference external" href="http://danielkummer.github.io/git-flow-cheatsheet/">Vincent Driessen&#8217;s branching
model</a> in how git is used.</p>
<div class="section" id="making-releases">
<h4>Making releases<a class="headerlink" href="#making-releases" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><ul class="simple">
<li>Commit any current changes to git.</li>
<li>Start a release branch: <code class="docutils literal"><span class="pre">git</span> <span class="pre">flow</span> <span class="pre">release</span> <span class="pre">start</span> <span class="pre">x.y.z</span></code></li>
<li>Edit version number in setup.py and commit.</li>
<li>Add changes to CHANGELOG.rst and commit.</li>
<li>Publish the release branch: <code class="docutils literal"><span class="pre">git</span> <span class="pre">flow</span> <span class="pre">release</span> <span class="pre">publish</span> <span class="pre">x.y.z</span></code></li>
<li>Finish the release: <code class="docutils literal"><span class="pre">git</span> <span class="pre">flow</span> <span class="pre">release</span> <span class="pre">finish</span> <span class="pre">x.y.z</span></code>. Write the following
commit message: <code class="docutils literal"><span class="pre">REL:</span> <span class="pre">Version</span> <span class="pre">x.y.z</span></code>.</li>
<li>Push to GitHub: <code class="docutils literal"><span class="pre">git</span> <span class="pre">push</span> <span class="pre">&amp;&amp;</span> <span class="pre">git</span> <span class="pre">push</span> <span class="pre">--tags</span></code></li>
<li>Download the release tarball from GitHub and use that in the phases below.
This avoids having local garbage in the release.</li>
<li>Publish in PyPI: <code class="docutils literal"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">release_pypi</span></code></li>
<li>Update the documentation web page: <code class="docutils literal"><span class="pre">cd</span> <span class="pre">doc</span> <span class="pre">&amp;&amp;</span> <span class="pre">make</span> <span class="pre">gh-pages</span></code></li>
<li>Publish in mloss.org.</li>
<li>Announcements to <a class="reference external" href="mailto:bayespy&#37;&#52;&#48;googlegroups&#46;com">bayespy<span>&#64;</span>googlegroups<span>&#46;</span>com</a>, <a class="reference external" href="mailto:scipy-user&#37;&#52;&#48;scipy&#46;org">scipy-user<span>&#64;</span>scipy<span>&#46;</span>org</a> and
<a class="reference external" href="mailto:numpy-discussion&#37;&#52;&#48;scipy&#46;org">numpy-discussion<span>&#64;</span>scipy<span>&#46;</span>org</a>.</li>
</ul>
</div></blockquote>
</div>
</div>
<span id="document-dev_guide/vmp"></span><div class="section" id="variational-message-passing">
<h3>Variational message passing<a class="headerlink" href="#variational-message-passing" title="Permalink to this headline">¶</a></h3>
<p>This section briefly describes the variational message passing (VMP) framework,
which is currently the only implemented inference engine in BayesPy.  The
variational Bayesian (VB) inference engine in BayesPy assumes that the posterior
approximation factorizes with respect to nodes and plates.  VMP is based on
updating one node at a time (the plates in one node can be updated
simultaneously) and iteratively updating all nodes in turns until convergence.</p>
<div class="section" id="standard-update-equation">
<h4>Standard update equation<a class="headerlink" href="#standard-update-equation" title="Permalink to this headline">¶</a></h4>
<p>The general update equation for the factorized approximation of node
<img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/> is the following:</p>
<div class="math" id="equation-vmp_general_update">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-vmp_general_update" title="Permalink to this equation">¶</a></span><img src="_images/math/0d0fcb4b1f0c470116e2849c0b8d5991f9011777.svg" alt="\log q(\boldsymbol{\theta})
&amp;=
\langle
  \log p\left( \boldsymbol{\theta} |
               \mathrm{pa}(\boldsymbol{\theta}) \right)
\rangle
+ \sum_{\mathbf{x} \in \mathrm{ch}(\boldsymbol{\theta})}
  \langle \log p(\mathbf{x}|\mathrm{pa}(\mathbf{x})) \rangle
+ \mathrm{const},"/></p>
</div><p>where <img class="math" src="_images/math/e45c64e9c3608bb339be645440a57defd62a38fd.svg" alt="\mathrm{pa}(\boldsymbol{\theta})"/> and
<img class="math" src="_images/math/d253d005ca8e018d01650e4ad731e49bca01ec22.svg" alt="\mathrm{ch}(\boldsymbol{\theta})"/> are the set of parents and children of
<img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/>, respectively.  Thus, the posterior approximation of
a node is updated by taking a sum of the expectations of all log densities in
which the node variable appears.  The expectations are over the approximate
distribution of all other variables than <img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/>.  Actually,
not all the variables are needed, because the non-constant part depends only on
the Markov blanket of <img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/>.  This leads to a local
optimization scheme, which uses messages from neighbouring nodes.</p>
<p>The messages are simple for conjugate exponential family models.  An exponential
family distribution has the following log probability density function:</p>
<div class="math" id="equation-likelihood">
<p><span class="eqno">(2)<a class="headerlink" href="#equation-likelihood" title="Permalink to this equation">¶</a></span><img src="_images/math/b1654995ef3cfff1914a89a82e8bd6a70510b55f.svg" alt="\log p(\mathbf{x}|\mathbf{\Theta})
&amp;=
\mathbf{u}_{\mathbf{x}}(\mathbf{x})^{\mathrm{T}}
\boldsymbol{\phi}_{\mathbf{x}}(\mathbf{\Theta})
+ g_{\mathbf{x}}(\mathbf{\Theta})
+ f_{\mathbf{x}}(\mathbf{x}),"/></p>
</div><p>where <img class="math" src="_images/math/4198d3d34c0ad725a752544558d79507b32ea943.svg" alt="\mathbf{\Theta}=\{\boldsymbol{\theta}_j\}"/> is the set of parents,
<img class="math" src="_images/math/2978d86319a8d5f6b72d697bce6ebfd0bcab6ca9.svg" alt="\mathbf{u}"/> is the sufficient statistic vector, <img class="math" src="_images/math/32dc4a30c91372385892f2c65d23b4fe3ac2670b.svg" alt="\boldsymbol{\phi}"/>
is the natural parameter vector, <img class="math" src="_images/math/307b3725cbb03398131f9ca542d79aff4933195f.svg" alt="g"/> is the negative log normalizer, and
<img class="math" src="_images/math/875eb40014526135383caa89fd500ae40a835f56.svg" alt="f"/> is the log base function.  Note that the log density is linear with
respect to the terms that are functions of <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>:
<img class="math" src="_images/math/2978d86319a8d5f6b72d697bce6ebfd0bcab6ca9.svg" alt="\mathbf{u}"/> and <img class="math" src="_images/math/875eb40014526135383caa89fd500ae40a835f56.svg" alt="f"/>.  If a parent has a conjugate prior,
<a class="reference internal" href="#equation-likelihood">(2)</a> is also linear with respect to the parent&#8217;s sufficient
statistic vector.  Thus, <a class="reference internal" href="#equation-likelihood">(2)</a> can be re-organized with respect to a
parent <img class="math" src="_images/math/d5fe89aba1d618b6042e3d9b3d6f75c45187153e.svg" alt="\boldsymbol{\theta}_j"/> as</p>
<div class="math">
<p><img src="_images/math/3c637a56a34f8f0a77f1bf2ee9d08463eca64cbb.svg" alt="\log p(\mathbf{x}|\mathbf{\Theta})
&amp;=
\mathbf{u}_{\boldsymbol{\theta}_j}(\boldsymbol{\theta}_j)^{\mathrm{T}}
\boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}_j}
(\mathbf{x}, \{\boldsymbol{\theta}_k\}_{k\neq j})
+ \mathrm{const},"/></p>
</div><p>where <img class="math" src="_images/math/4895867998ca726ab2a0df9ec9403b8312a5b7b3.svg" alt="\mathbf{u}_{\boldsymbol{\theta}_j}"/> is the sufficient statistic
vector of <img class="math" src="_images/math/d5fe89aba1d618b6042e3d9b3d6f75c45187153e.svg" alt="\boldsymbol{\theta}_j"/> and the constant part is constant with
respect to <img class="math" src="_images/math/d5fe89aba1d618b6042e3d9b3d6f75c45187153e.svg" alt="\boldsymbol{\theta}_j"/>.  Thus, the update equation
<a class="reference internal" href="#equation-vmp_general_update">(1)</a> for <img class="math" src="_images/math/d5fe89aba1d618b6042e3d9b3d6f75c45187153e.svg" alt="\boldsymbol{\theta}_j"/> can be written as</p>
<div class="math">
<p><img src="_images/math/0656085fa8ebc224c82d92c432d16b2b5bc439bf.svg" alt="\log q(\boldsymbol{\theta}_j)
&amp;=
\mathbf{u}_{\boldsymbol{\theta}_j}(\boldsymbol{\theta}_j)^{\mathrm{T}}
  \langle \boldsymbol{\phi}_{\boldsymbol{\theta}_j} \rangle
+ f_{\boldsymbol{\theta}_j}(\boldsymbol{\theta}_j)
+
\mathbf{u}_{\boldsymbol{\theta}_j}(\boldsymbol{\theta}_j)^{\mathrm{T}}
\sum_{\mathbf{x} \in \mathrm{ch}(\boldsymbol{\theta}_j)}
      \langle \boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}_j} \rangle
+ \mathrm{const},
\\
&amp;=
\mathbf{u}_{\boldsymbol{\theta}_j}(\boldsymbol{\theta}_j)^{\mathrm{T}}
\left(
  \langle \boldsymbol{\phi}_{\boldsymbol{\theta}_j} \rangle
  + \sum_{\mathbf{x} \in \mathrm{ch}(\boldsymbol{\theta}_j)}
      \langle \boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}_j} \rangle
\right)
+ f_{\boldsymbol{\theta}_j}(\boldsymbol{\theta}_j)
+ \mathrm{const},"/></p>
</div><p>where the summation is over all the child nodes of
<img class="math" src="_images/math/d5fe89aba1d618b6042e3d9b3d6f75c45187153e.svg" alt="\boldsymbol{\theta}_j"/>.  Because of the conjugacy,
<img class="math" src="_images/math/387ca07529261fe68cdf2790696f7d8dd004dd47.svg" alt="\langle\boldsymbol{\phi}_{\boldsymbol{\theta}_j}\rangle"/> depends
(multi)linearly on the parents&#8217; sufficient statistic vector.  Similarly,
<img class="math" src="_images/math/d639f74eb14e7d4b22f60122f3db4584a7b138f7.svg" alt="\langle \boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}_j}
\rangle"/> depends (multi)linearly on the expectations of the children&#8217;s and
co-parents&#8217; sufficient statistics.  This gives the following update equation for
the natural parameter vector of the posterior approximation
<img class="math" src="_images/math/30694af5f42d1510fad93bbe7ae245025c09629d.svg" alt="q(\boldsymbol{\phi}_j)"/>:</p>
<div class="math" id="equation-update_phi">
<p><span class="eqno">(3)<a class="headerlink" href="#equation-update_phi" title="Permalink to this equation">¶</a></span><img src="_images/math/de3e8c15006da9a1e60ec3551f93532c8092f740.svg" alt="\tilde{\boldsymbol{\phi}}_j &amp;= \langle \boldsymbol{\phi}_{\boldsymbol{\theta}_j} \rangle
  + \sum_{\mathbf{x} \in \mathrm{ch}(\boldsymbol{\theta}_j)} \langle
      \boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}_j} \rangle."/></p>
</div></div>
<div class="section" id="variational-messages">
<h4>Variational messages<a class="headerlink" href="#variational-messages" title="Permalink to this headline">¶</a></h4>
<p>The update equation <a class="reference internal" href="#equation-update_phi">(3)</a> leads to a message passing scheme: the term
<img class="math" src="_images/math/37d804529a77de4fadc1c29b1d380920b6e99430.svg" alt="\langle \boldsymbol{\phi}_{\boldsymbol{\theta}_j} \rangle"/> is a function
of the parents&#8217; sufficient statistic vector and the term <img class="math" src="_images/math/b4f6d9e56ba24532af6638b337c42c648dea41a0.svg" alt="\langle
\boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}_j} \rangle"/> can be
interpreted as a message from the child node <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>.  Thus, the
message from the child node <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/> to the parent node
<img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/> is</p>
<div class="math">
<p><img src="_images/math/81eedc878384aea844236bdbe24680c5c5907c01.svg" alt="\mathbf{m}_{\mathbf{x}\rightarrow\boldsymbol{\theta}}
&amp;\equiv
\langle \boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}} \rangle,"/></p>
</div><p>which can be computed as a function of the sufficient statistic vector of the
co-parent nodes of <img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/> and the sufficient statistic
vector of the child node <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>.  The message from the parent node
<img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/> to the child node <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/> is simply the
expectation of the sufficient statistic vector:</p>
<div class="math">
<p><img src="_images/math/1b5cdf86ace9bd4970ba1657b682d2c8231b0da3.svg" alt="\mathbf{m}_{\mathbf{\boldsymbol{\theta}}\rightarrow\mathbf{x}}
&amp;\equiv
\langle \mathbf{u}_{\boldsymbol{\theta}} \rangle."/></p>
</div><p>In order to compute the expectation of the sufficient statistic vector we need
to write <img class="math" src="_images/math/8b28a049acfd21bf275aab9d312649438f4fae01.svg" alt="q(\boldsymbol{\theta})"/> as</p>
<div class="math">
<p><img src="_images/math/c04587048131eefe24848d71e64f9c26e7696ba0.svg" alt="\log q(\boldsymbol{\theta}) &amp;=
\mathbf{u}(\boldsymbol{\theta})^{\mathrm{T}}
\tilde{\boldsymbol{\phi}}
+ \tilde{g}(\tilde{\boldsymbol{\phi}})
+ f(\boldsymbol{\theta}),"/></p>
</div><p>where <img class="math" src="_images/math/9f899bdc07b8b7921fb4c369580feab413f13771.svg" alt="\tilde{\boldsymbol{\phi}}"/> is the natural
parameter vector of <img class="math" src="_images/math/8b28a049acfd21bf275aab9d312649438f4fae01.svg" alt="q(\boldsymbol{\theta})"/>.  Now, the expectation of the
sufficient statistic vector is defined as</p>
<div class="math" id="equation-moments">
<p><span class="eqno">(4)<a class="headerlink" href="#equation-moments" title="Permalink to this equation">¶</a></span><img src="_images/math/43c542f2d69920910dc047f12016f4e66b76d33c.svg" alt="\langle \mathbf{u}_{\boldsymbol{\theta}} \rangle
&amp;= - \frac{\partial \tilde{g}}{\partial
\tilde{\boldsymbol{\phi}}_{\boldsymbol{\theta}}}
(\tilde{\boldsymbol{\phi}}_{\boldsymbol{\theta}})."/></p>
</div><p>We call this expectation of the sufficient statistic vector as the moments
vector.</p>
</div>
<div class="section" id="lower-bound">
<h4>Lower bound<a class="headerlink" href="#lower-bound" title="Permalink to this headline">¶</a></h4>
<p>Computing the VB lower bound is not necessary in order to find the posterior
approximation, although it is extremely useful in monitoring convergence and
possible bugs.  The VB lower bound can be written as</p>
<div class="math">
<p><img src="_images/math/ed0e713515e719445e9221191593ffea66b03361.svg" alt="\mathcal{L} = \langle \log p(\mathbf{Y}, \mathbf{X}) \rangle - \langle \log
q(\mathbf{X}) \rangle,"/></p>
</div><p>where <img class="math" src="_images/math/882e6721a5df0caf9a878f7d49deec991c5aacae.svg" alt="\mathbf{Y}"/> is the set of all observed variables and
<img class="math" src="_images/math/c20f7f9210a91b8aeba2e85d2bcb23e29bcab3f4.svg" alt="\mathbf{X}"/> is the set of all latent variables.  It can also be written as</p>
<div class="math">
<p><img src="_images/math/678c5d629699847a810a645bc6eec175774b2fe4.svg" alt="\mathcal{L} = \sum_{\mathbf{y} \in \mathbf{Y}} \langle \log p(\mathbf{y} |
\mathrm{pa}(\mathbf{y})) \rangle
+ \sum_{\mathbf{x} \in \mathbf{X}} \left[ \langle \log p(\mathbf{x} |
  \mathrm{pa}(\mathbf{x})) \rangle - \langle \log q(\mathbf{x}) \right],"/></p>
</div><p>which shows that observed and latent variables contribute differently to the
lower bound.  These contributions have simple forms for exponential family
nodes.  Observed exponential family nodes contribute to the lower bound as
follows:</p>
<div class="math">
<p><img src="_images/math/afe75610e929ec74f87ef8d653104393587b6a80.svg" alt="\langle \log p(\mathbf{y}|\mathrm{pa}(\mathbf{y})) \rangle &amp;=
\mathbf{u}(\mathbf{y})^T \langle \boldsymbol{\phi} \rangle
+ \langle g \rangle + f(\mathbf{x}),"/></p>
</div><p>where <img class="math" src="_images/math/5ecece3b7c1d1cdac478cc27046cd8b5d93f9895.svg" alt="\mathbf{y}"/> is the observed data.  On the other hand, latent
exponential family nodes contribute to the lower bound as follows:</p>
<div class="math">
<p><img src="_images/math/9b090cfdf5bf5262290b7aff145295d599aa82d5.svg" alt="\langle \log p(\mathbf{x}|\boldsymbol{\theta}) \rangle
- \langle \log q(\mathbf{x}) \rangle &amp;= \langle \mathbf{u} \rangle^T (\langle
\boldsymbol{\phi} \rangle - \tilde{\boldsymbol{\phi}} )
+ \langle g \rangle - \tilde{g}."/></p>
</div><p>If a node is partially observed and partially unobserved, these formulas are
applied plate-wise appropriately.</p>
</div>
<div class="section" id="terms">
<span id="sec-vmp-terms"></span><h4>Terms<a class="headerlink" href="#terms" title="Permalink to this headline">¶</a></h4>
<p>To summarize, implementing VMP requires one to write for each stochastic
exponential family node:</p>
<blockquote>
<div><p><img class="math" src="_images/math/d3cd87eb6aa8e60dff83c8a3965a91404d11d77c.svg" alt="\langle \boldsymbol{\phi} \rangle"/> : the expectation of the prior
natural parameter vector</p>
<blockquote>
<div>Computed as a function of the messages from parents.</div></blockquote>
<p><img class="math" src="_images/math/9f899bdc07b8b7921fb4c369580feab413f13771.svg" alt="\tilde{\boldsymbol{\phi}}"/> : natural parameter vector of the
posterior approximation</p>
<blockquote>
<div>Computed as a sum of <img class="math" src="_images/math/d3cd87eb6aa8e60dff83c8a3965a91404d11d77c.svg" alt="\langle \boldsymbol{\phi} \rangle"/> and the
messages from children.</div></blockquote>
<p><img class="math" src="_images/math/922c98a0a52c9d3b2d37b0d9277d34da9ecb8a41.svg" alt="\langle \mathbf{u} \rangle"/> : the posterior moments vector</p>
<blockquote>
<div>Computed as a function of <img class="math" src="_images/math/9f899bdc07b8b7921fb4c369580feab413f13771.svg" alt="\tilde{\boldsymbol{\phi}}"/> as defined
in <a class="reference internal" href="#equation-moments">(4)</a>.</div></blockquote>
<p><img class="math" src="_images/math/4d3a99b8c11b6e184c48212933ab0d5731c0e821.svg" alt="\mathbf{u}(\mathbf{x})"/> : the moments vector for given data</p>
<blockquote>
<div>Computed as a function of of the observed data <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>.</div></blockquote>
<p><img class="math" src="_images/math/5b5fb64b6a429f6c21e1232db17f2fdf5bc3b81e.svg" alt="\langle g \rangle"/> : the expectation of the negative log normalizer
of the prior</p>
<blockquote>
<div>Computed as a function of parent moments.</div></blockquote>
<p><img class="math" src="_images/math/3a7b9913dd69a1e368b744bf203423439144401e.svg" alt="\tilde{g}"/> : the negative log normalizer of the posterior
approximation</p>
<blockquote>
<div>Computed as a function of <img class="math" src="_images/math/9f899bdc07b8b7921fb4c369580feab413f13771.svg" alt="\tilde{\boldsymbol{\phi}}"/>.</div></blockquote>
<p><img class="math" src="_images/math/c2c862122b65b3160cd91e09d7712d2da703b3f4.svg" alt="f(\mathbf{x})"/> : the log base measure for given data</p>
<blockquote>
<div>Computed as a function of the observed data <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>.</div></blockquote>
<p><img class="math" src="_images/math/27c5f00d489885e5c73104f623f91f8439370795.svg" alt="\langle \boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}}
\rangle"/> : the message to parent <img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/></p>
<blockquote>
<div>Computed as a function of the moments of this node and the other
parents.</div></blockquote>
</div></blockquote>
<p>Deterministic nodes require only the following terms:</p>
<blockquote>
<div><p><img class="math" src="_images/math/922c98a0a52c9d3b2d37b0d9277d34da9ecb8a41.svg" alt="\langle \mathbf{u} \rangle"/> : the posterior moments vector</p>
<blockquote>
<div>Computed as a function of the messages from the parents.</div></blockquote>
<p><img class="math" src="_images/math/112d7222c72a9db4fef0be88db9ae311e5c02f54.svg" alt="\mathbf{m}"/> : the message to a parent</p>
<blockquote>
<div>Computed as a function of the messages from the other parents and all
children.</div></blockquote>
</div></blockquote>
</div>
</div>
<span id="document-dev_guide/engine"></span><div class="section" id="implementing-inference-engines">
<h3>Implementing inference engines<a class="headerlink" href="#implementing-inference-engines" title="Permalink to this headline">¶</a></h3>
<p>Currently, only variational Bayesian inference engine is implemented.  This
implementation is not very modular, that is, the inference engine is not well
separated from the model construction.  Thus, it is not straightforward to
implement other inference engines at the moment.  Improving the modularity of
the inference engine and model construction is future work with high priority.
In any case, BayesPy aims to be an efficient, simple and modular Bayesian
package for variational inference at least.</p>
</div>
<span id="document-dev_guide/writingnodes"></span><div class="section" id="implementing-nodes">
<h3>Implementing nodes<a class="headerlink" href="#implementing-nodes" title="Permalink to this headline">¶</a></h3>
<p>The main goal of BayesPy is to provide a package which enables easy and flexible
construction of simple and complex models with efficient inference.  However,
users may sometimes be unable to construct their models because the built-in
nodes do not implement some specific features.  Thus, one may need to implement
new nodes in order to construct the model.  BayesPy aims to make the
implementation of new nodes both simple and fast.  Probably, a large complex
model can be constructed almost completely with the built-in nodes and the user
needs to implement only a few nodes.</p>
<div class="section" id="moments">
<h4>Moments<a class="headerlink" href="#moments" title="Permalink to this headline">¶</a></h4>
<p>In order to implement nodes, it is important to understand the messaging
framework of the nodes.  A node is a unit of calculation which communicates to
its parent and child nodes using messages.  These messages have types that need
to match between nodes, that is, the child node needs to understand the messages
its parents are sending and vice versa.  Thus, a node defines which message type
it requires from each of its parents, and only nodes that have that type of
output message (i.e., the message to a child node) are valid parent nodes for
that node.</p>
<p>The message type is defined by the moments of the parent node.  The moments are
a collection of expectations: <img class="math" src="_images/math/eeccdebdbc19aec1c0fa153ef98750a769793756.svg" alt="\{ \langle f_1(X) \rangle, \ldots, \langle
f_N(X) \rangle \}"/>.  The functions <img class="math" src="_images/math/fce4f2cb2d686de9e0184a47b3ada6188241583b.svg" alt="f_1, \ldots, f_N"/> (and the number of
the functions) define the message type and they are the sufficient statistic as
discussed in the previous section.  Different message types are represented by
<a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.node.Moments" title="bayespy.inference.vmp.nodes.node.Moments"><code class="xref py py-class docutils literal"><span class="pre">Moments</span></code></a> class hierarchy.  For instance, <code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code>
represents a message type with parent moments <img class="math" src="_images/math/50ed760d9282deb72a3937f03c92cad0291bb4d7.svg" alt="\{\langle \mathbf{x}
\rangle, \langle \mathbf{xx}^T \rangle \}"/> and <code class="xref py py-class docutils literal"><span class="pre">WishartMoments</span></code> a message
type with parent moments <img class="math" src="_images/math/44d38a3a5f51c069134c57662e74f398beea91ee.svg" alt="\{\langle \mathbf{\Lambda} \rangle, \langle \log
|\mathbf{\Lambda}| \rangle\}"/>.</p>
<p>Let us give an example: <a class="reference internal" href="index.html#bayespy.nodes.Gaussian" title="bayespy.nodes.Gaussian"><code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code></a> node outputs <code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code>
messages and <a class="reference internal" href="index.html#bayespy.nodes.Wishart" title="bayespy.nodes.Wishart"><code class="xref py py-class docutils literal"><span class="pre">Wishart</span></code></a> node outputs <code class="xref py py-class docutils literal"><span class="pre">WishartMoments</span></code> messages.
<a class="reference internal" href="index.html#bayespy.nodes.Gaussian" title="bayespy.nodes.Gaussian"><code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code></a> node requires that it receives <code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code>
messages from the mean parent node and <code class="xref py py-class docutils literal"><span class="pre">WishartMoments</span></code> messages from the
precision parent node.  Thus, <a class="reference internal" href="index.html#bayespy.nodes.Gaussian" title="bayespy.nodes.Gaussian"><code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code></a> and <a class="reference internal" href="index.html#bayespy.nodes.Wishart" title="bayespy.nodes.Wishart"><code class="xref py py-class docutils literal"><span class="pre">Wishart</span></code></a> are valid
node classes as the mean and precision parent nodes of <a class="reference internal" href="index.html#bayespy.nodes.Gaussian" title="bayespy.nodes.Gaussian"><code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code></a> node.</p>
<p>Note that several nodes may have the same output message type and some message
types can be transformed to other message types using deterministic converter
nodes.  For instance, <a class="reference internal" href="index.html#bayespy.nodes.Gaussian" title="bayespy.nodes.Gaussian"><code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code></a> and <a class="reference internal" href="index.html#bayespy.nodes.GaussianARD" title="bayespy.nodes.GaussianARD"><code class="xref py py-class docutils literal"><span class="pre">GaussianARD</span></code></a> nodes both
output <code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code> messages, deterministic <a class="reference internal" href="index.html#bayespy.nodes.SumMultiply" title="bayespy.nodes.SumMultiply"><code class="xref py py-class docutils literal"><span class="pre">SumMultiply</span></code></a>
also outputs <code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code> messages, and deterministic converter
<code class="xref py py-class docutils literal"><span class="pre">_MarkovChainToGaussian</span></code> converts <code class="xref py py-class docutils literal"><span class="pre">GaussianMarkovChainMoments</span></code> to
<code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code>.</p>
<p>Each node specifies the message type requirements of its parents by
<code class="xref py py-attr docutils literal"><span class="pre">Node._parent_moments</span></code> attribute which is a list of <a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.node.Moments" title="bayespy.inference.vmp.nodes.node.Moments"><code class="xref py py-class docutils literal"><span class="pre">Moments</span></code></a>
sub-class instances.  These moments objects have a few purpose when creating the
node: 1) check that parents are sending proper messages; 2) if parents use
different message type, try to add a converter which converts the messages to
the correct type if possible; 3) if given parents are not nodes but numeric
arrays, convert them to constant nodes with correct output message type.</p>
<p>When implementing a new node, it is not always necessary to implement a new
moments class.  If another node has the same sufficient statistic vector, thus
the same moments, that moments class can be used.  Otherwise, one must implement
a simple moments class which has the following methods:</p>
<blockquote>
<div><ul>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.node.Moments.compute_fixed_moments" title="bayespy.inference.vmp.nodes.node.Moments.compute_fixed_moments"><code class="xref py py-func docutils literal"><span class="pre">Moments.compute_fixed_moments()</span></code></a></p>
<blockquote>
<div><p>Computes the moments for a known value.  This is used to compute the
moments of constant numeric arrays and wrap them into constant nodes.</p>
</div></blockquote>
</li>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">Moments.compute_dims_from_values()</span></code></p>
<blockquote>
<div><p>Given a known value of the variable, return the shape of the variable
dimensions in the moments.  This is used to solve the shape of the moments
array for constant nodes.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="distributions">
<h4>Distributions<a class="headerlink" href="#distributions" title="Permalink to this headline">¶</a></h4>
<p>In order to implement a stochastic exponential family node, one must first write
down the log probability density function of the node and derive the terms
discussed in section <a class="reference internal" href="index.html#sec-vmp-terms"><span class="std std-ref">Terms</span></a>.  These terms are implemented and
collected as a class which is a subclass of <code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code>.  The main
reason to implement these methods in another class instead of the node class
itself is that these methods can be used without creating a node, for instance,
in <code class="xref py py-class docutils literal"><span class="pre">Mixture</span></code> class.</p>
<p>For exponential family distributions, the distribution class is a subclass of
<a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution"><code class="xref py py-class docutils literal"><span class="pre">ExponentialFamilyDistribution</span></code></a>, and the relation between the terms in
section <a class="reference internal" href="index.html#sec-vmp-terms"><span class="std std-ref">Terms</span></a> and the methods is as follows:</p>
<blockquote>
<div><ul>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_phi_from_parents" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_phi_from_parents"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.compute_phi_from_parents()</span></code></a></p>
<blockquote>
<div><p>Computes the expectation of the natural parameters <img class="math" src="_images/math/033b619feafc7481b861c4941d4d023877ae7738.svg" alt="\langle
\boldsymbol{\phi} \rangle"/> in the prior distribution given the moments of
the parents.</p>
</div></blockquote>
</li>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_cgf_from_parents" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_cgf_from_parents"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.compute_cgf_from_parents()</span></code></a></p>
<blockquote>
<div><p>Computes the expectation of the negative log normalizer <img class="math" src="_images/math/a7319ba1e966424b0dffaef756fe6758faf8aeb4.svg" alt="\langle g
\rangle"/> of the prior distribution given the moments of the parents.</p>
</div></blockquote>
</li>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_moments_and_cgf" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_moments_and_cgf"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.compute_moments_and_cgf()</span></code></a></p>
<blockquote>
<div><p>Computes the moments <img class="math" src="_images/math/922c98a0a52c9d3b2d37b0d9277d34da9ecb8a41.svg" alt="\langle \mathbf{u} \rangle"/> and the negative
log normalizer <img class="math" src="_images/math/3a7b9913dd69a1e368b744bf203423439144401e.svg" alt="\tilde{g}"/> of the posterior distribution
given the natural parameters <img class="math" src="_images/math/9f899bdc07b8b7921fb4c369580feab413f13771.svg" alt="\tilde{\boldsymbol{\phi}}"/>.</p>
</div></blockquote>
</li>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_message_to_parent" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_message_to_parent"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.compute_message_to_parent()</span></code></a></p>
<blockquote>
<div><p>Computes the message <img class="math" src="_images/math/a44ba469ae873b4741820217ae905f4d40c51c0a.svg" alt="\langle
\boldsymbol{\phi}_{\mathbf{x}\rightarrow\boldsymbol{\theta}} \rangle"/> from
the node <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/> to its parent node <img class="math" src="_images/math/5c9b4d32c525826a9dd2f216375a14995a3e9fa9.svg" alt="\boldsymbol{\theta}"/>
given the moments of the node and the other parents.</p>
</div></blockquote>
</li>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_fixed_moments_and_f" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.compute_fixed_moments_and_f"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.compute_fixed_moments_and_f()</span></code></a></p>
<blockquote>
<div><p>Computes <img class="math" src="_images/math/4d3a99b8c11b6e184c48212933ab0d5731c0e821.svg" alt="\mathbf{u}(\mathbf{x})"/> and <img class="math" src="_images/math/c2c862122b65b3160cd91e09d7712d2da703b3f4.svg" alt="f(\mathbf{x})"/> for
given observed value <img class="math" src="_images/math/8cea9f6053c66e0078625d15b2b70d858ca7f1db.svg" alt="\mathbf{x}"/>.  Without this method, variables
from this distribution cannot be observed.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>For each stochastic exponential family node, one must write a distribution class
which implements these methods.  After that, the node class is basically a
simple wrapper and it also stores the moments and the natural parameters of the
current posterior approximation.  Note that the distribution classes do not
store node-specific information, they are more like static collections of
methods.  However, sometimes the implementations depend on some information,
such as the dimensionality of the variable, and this information must be
provided, if needed, when constructing the distribution object.</p>
<p>In addition to the methods listed above, it is necessary to implement a few more
methods in some cases.  This happens when the plates of the parent do not map to
the plates directly as discussed in section <a class="reference internal" href="index.html#sec-irregular-plates"><span class="std std-ref">Irregular plates</span></a>.  Then,
one must write methods that implement this plate mapping and apply the same
mapping to the mask array:</p>
<blockquote>
<div><ul>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.plates_from_parent" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.plates_from_parent"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.plates_from_parent()</span></code></a></p>
<blockquote>
<div><p>Given the plates of the parent, return the resulting plates of the child.</p>
</div></blockquote>
</li>
<li><p class="first"><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.plates_to_parent" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution.plates_to_parent"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.plates_to_parent()</span></code></a></p>
<blockquote>
<div><p>Given the plates of the child, return the plates of the parent that would
have resulted them.</p>
</div></blockquote>
</li>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">ExponentialFamilyDistribution.compute_mask_to_parent()</span></code></p>
<blockquote>
<div><p>Given the mask array of the child, apply the plate mapping.</p>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>It is important to understand when one must implement these methods, because the
default implementations in the base class will lead to errors or weird results.</p>
</div>
<div class="section" id="stochastic-exponential-family-nodes">
<h4>Stochastic exponential family nodes<a class="headerlink" href="#stochastic-exponential-family-nodes" title="Permalink to this headline">¶</a></h4>
<p>After implementing the distribution class, the next task is to implement the
node class.  First, we need to explain a few important attributes before we can
explain how to implement a node class.</p>
<p>Stochastic exponential family nodes have two attributes that store the state of
the posterior distribution:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="docutils literal"><span class="pre">phi</span></code></p>
<p>The natural parameter vector <img class="math" src="_images/math/9f899bdc07b8b7921fb4c369580feab413f13771.svg" alt="\tilde{\boldsymbol{\phi}}"/> of the
posterior approximation.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">u</span></code></p>
<p>The moments <img class="math" src="_images/math/922c98a0a52c9d3b2d37b0d9277d34da9ecb8a41.svg" alt="\langle \mathbf{u} \rangle"/> of the posterior
approximation.</p>
</li>
</ul>
</div></blockquote>
<p>Instead of storing these two variables as vectors (as in the mathematical
formulas), they are stored as lists of arrays with convenient shapes.  For
instance, <code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code> node stores the moments as a list consisting of a
vector <img class="math" src="_images/math/a9bf9ef91ba3d6e07782f68d3210c1778a630d19.svg" alt="\langle \mathbf{x} \rangle"/> and a matrix <img class="math" src="_images/math/4115f9269e45dd7cd359ac992f9b7a693dbb0c5a.svg" alt="\langle
\mathbf{xx}^T \rangle"/> instead of reshaping and concatenating these into a
single vector.  The same applies for the natural parameters <code class="docutils literal"><span class="pre">phi</span></code> because it
has the same shape as <code class="docutils literal"><span class="pre">u</span></code>.</p>
<p>The shapes of the arrays in the lists <code class="docutils literal"><span class="pre">u</span></code> and <code class="docutils literal"><span class="pre">phi</span></code> consist of the shape
caused by the plates and the shape caused by the variable itself.  For instance,
the moments of <code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code> node have shape <code class="docutils literal"><span class="pre">(D,)</span></code> and <code class="docutils literal"><span class="pre">(D,</span> <span class="pre">D)</span></code>, where
<code class="docutils literal"><span class="pre">D</span></code> is the dimensionality of the Gaussian vector.  In addition, if the node
has plates, they are added to these shapes.  Thus, for instance, if the
<code class="xref py py-class docutils literal"><span class="pre">Gaussian</span></code> node has plates <code class="docutils literal"><span class="pre">(3,</span> <span class="pre">7)</span></code> and <code class="docutils literal"><span class="pre">D</span></code> is 5, the shape of
<code class="docutils literal"><span class="pre">u[0]</span></code> and <code class="docutils literal"><span class="pre">phi[0]</span></code> would be <code class="docutils literal"><span class="pre">(3,</span> <span class="pre">7,</span> <span class="pre">5)</span></code> and the shape of <code class="docutils literal"><span class="pre">u[1]</span></code> and
<code class="docutils literal"><span class="pre">phi[1]</span></code> would be <code class="docutils literal"><span class="pre">(3,</span> <span class="pre">7,</span> <span class="pre">5,</span> <span class="pre">5)</span></code>.  This shape information is stored in the
following attributes:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="docutils literal"><span class="pre">plates</span></code> : a tuple</p>
<p>The plates of the node.  In our example, <code class="docutils literal"><span class="pre">(3,</span> <span class="pre">7)</span></code>.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">dims</span></code> : a list of tuples</p>
<p>The shape of each of the moments arrays (or natural parameter arrays) without
plates.  In our example, <code class="docutils literal"><span class="pre">[</span> <span class="pre">(5,),</span> <span class="pre">(5,</span> <span class="pre">5)</span> <span class="pre">]</span></code>.</p>
</li>
</ul>
</div></blockquote>
<p>Finally, three attributes define VMP for the node:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="docutils literal"><span class="pre">_moments</span></code> : <code class="xref py py-class docutils literal"><span class="pre">Moments</span></code> sub-class instance</p>
<p>An object defining the moments of the node.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">_parent_moments</span></code> : list of <code class="xref py py-class docutils literal"><span class="pre">Moments</span></code> sub-class instances</p>
<p>A list defining the moments requirements for each parent.</p>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">_distribution</span></code> : <code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code> sub-class instance</p>
<p>An object implementing the VMP formulas.</p>
</li>
</ul>
</div></blockquote>
<p>Basically, a node class is a collection of the above attributes.  When a node is
created, these attributes are defined.  The base class for exponential family
nodes, <a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamily" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamily"><code class="xref py py-class docutils literal"><span class="pre">ExponentialFamily</span></code></a>, provides a simple default constructor which
does not need to be overwritten if <code class="docutils literal"><span class="pre">dims</span></code>, <code class="docutils literal"><span class="pre">_moments</span></code>, <code class="docutils literal"><span class="pre">_parent_moments</span></code>
and <code class="docutils literal"><span class="pre">_distribution</span></code> can be provided as static class attributes.  For instance,
<code class="xref py py-class docutils literal"><span class="pre">Gamma</span></code> node defines these attributes statically.  However, usually at
least one of these attributes cannot be defined statically in the class.  In
that case, one must implement a class method which overloads
<code class="xref py py-func docutils literal"><span class="pre">ExponentialFamily._constructor()</span></code>.  The purpose of this method is to define
all the attributes given the parent nodes.  These are defined using a class
method instead of <code class="docutils literal"><span class="pre">__init__</span></code> method in order to be able to use the class
constructors statically, for instance, in <code class="xref py py-class docutils literal"><span class="pre">Mixture</span></code> class.  This
construction allows users to create mixtures of any exponential family
distribution with simple syntax.</p>
<p>The parents of a node must be converted so that they have a correct message
type, because the user may have provided numeric arrays or nodes with incorrect
message type.  Numeric arrays should be converted to constant nodes with correct
message type.  Incorrect message type nodes should be converted to correct
message type nodes if possible.  Thus, the constructor should use
<code class="docutils literal"><span class="pre">Node._ensure_moments</span></code> method to make sure the parent is a node with correct
message type.  Instead of calling this method for each parent node in the
constructor, one can use <code class="docutils literal"><span class="pre">ensureparents</span></code> decorator to do this automatically.
However, the decorator requires that <code class="docutils literal"><span class="pre">_parent_moments</span></code> attribute has already
been defined statically.  If this is not possible, the parent nodes must be
converted manually in the constructor, because one should never assume that the
parent nodes given to the constructor are nodes with correct message type or
even nodes at all.</p>
</div>
<div class="section" id="deterministic-nodes">
<h4>Deterministic nodes<a class="headerlink" href="#deterministic-nodes" title="Permalink to this headline">¶</a></h4>
<p>Deterministic nodes are nodes that do not correspond to any probability
distribution but rather a deterministic function.  It does not have any moments
or natural parameters to store.  A deterministic node is implemented as a
subclass of <code class="xref py py-class docutils literal"><span class="pre">Deterministic</span></code> base class.  The new node class must
implement the following methods:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">Deterministic._compute_moments()</span></code></p>
<p>Computes the moments given the moments of the parents.</p>
</li>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">Deterministic._compute_message_to_parent()</span></code></p>
<p>Computes the message to a parent node given the message from the children and
the moments of the other parents.  In some cases, one may want to implement
<code class="xref py py-func docutils literal"><span class="pre">Deterministic._compute_message_and_mask_to_parent()</span></code> or
<code class="xref py py-func docutils literal"><span class="pre">Deterministic._message_to_parent()</span></code> instead in order to gain more
control over efficient computation.</p>
</li>
</ul>
</div></blockquote>
<p>Similarly as in <code class="xref py py-class docutils literal"><span class="pre">Distribution</span></code> class, if the node handles plates
irregularly, it is important to implement the following methods:</p>
<blockquote>
<div><ul>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">Deterministic._plates_from_parent()</span></code></p>
<p>Given the plates of the parent, return the resulting plates of the child.</p>
</li>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">Deterministic._plates_to_parent()</span></code></p>
<p>Given the plates of the child, return the plates of the parent that would
have resulted them.</p>
</li>
<li><p class="first"><code class="xref py py-func docutils literal"><span class="pre">Deterministic._compute_weights_to_parent()</span></code></p>
<p>Given the mask array, convert it to a plate mask of the parent.</p>
</li>
</ul>
</div></blockquote>
<div class="section" id="converter-nodes">
<h5>Converter nodes<a class="headerlink" href="#converter-nodes" title="Permalink to this headline">¶</a></h5>
<p>Sometimes a node has incorrect message type but the message can be converted
into a correct type.  For instance, <code class="xref py py-class docutils literal"><span class="pre">GaussianMarkovChain</span></code> has
<code class="xref py py-class docutils literal"><span class="pre">GaussianMarkovChainMoments</span></code> message type, which means moments <img class="math" src="_images/math/b7279dd2c5feb4df928967c153ee367692fe21da.svg" alt="\{
\langle \mathbf{x}_n \rangle, \langle \mathbf{x}_n \mathbf{x}_n^T \rangle,
\langle \mathbf{x}_n \mathbf{x}_{n-1}^T \rangle \}^N_{n=1}"/>.  These moments can
be converted to <code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code> by ignoring the third element and
considering the time axis as a plate axis.  Thus, if a node requires
<code class="xref py py-class docutils literal"><span class="pre">GaussianMoments</span></code> message from its parent, <code class="xref py py-class docutils literal"><span class="pre">GaussianMarkovChain</span></code>
is a valid parent if its messages are modified properly.  This conversion is
implemented in <code class="xref py py-class docutils literal"><span class="pre">_MarkovChainToGaussian</span></code> converter class.  Converter nodes
are simple deterministic nodes that have one parent node and they convert the
messages to another message type.</p>
<p>For the user, it is not convenient if the exact message type has to be known and
an explicit converter node needs to be created.  Thus, the conversions are done
automatically and the user will be unaware of them.  In order to enable this
automatization, when writing a converter node, one should register the converter
to the moments class using <code class="xref py py-func docutils literal"><span class="pre">Moments.add_converter()</span></code>.  For instance, a class
<code class="docutils literal"><span class="pre">X</span></code> which converts moments <code class="docutils literal"><span class="pre">A</span></code> to moments <code class="docutils literal"><span class="pre">B</span></code> is registered as
<code class="docutils literal"><span class="pre">A.add_conveter(B,</span> <span class="pre">X)</span></code>.  After that, <code class="xref py py-func docutils literal"><span class="pre">Node._ensure_moments()</span></code> and
<code class="xref py py-func docutils literal"><span class="pre">Node._convert()</span></code> methods are used to perform the conversions automatically.
The conversion can consist of several consecutive converter nodes, and the least
number of conversions is used.</p>
</div>
</div>
</div>
</div>
</div>
<span id="document-user_api/user_api"></span><div class="section" id="user-api">
<span id="sec-user-api"></span><h2>User API<a class="headerlink" href="#user-api" title="Permalink to this headline">¶</a></h2>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="index.html#module-bayespy.nodes" title="bayespy.nodes"><code class="xref py py-obj docutils literal"><span class="pre">bayespy.nodes</span></code></a></td>
<td>Package for nodes used to construct the model.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#module-bayespy.inference" title="bayespy.inference"><code class="xref py py-obj docutils literal"><span class="pre">bayespy.inference</span></code></a></td>
<td>Package for Bayesian inference engines</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#module-bayespy.plot" title="bayespy.plot"><code class="xref py py-obj docutils literal"><span class="pre">bayespy.plot</span></code></a></td>
<td>Functions for plotting nodes.</td>
</tr>
</tbody>
</table>
</div>
<span id="document-dev_api/dev_api"></span><div class="section" id="developer-api">
<h2>Developer API<a class="headerlink" href="#developer-api" title="Permalink to this headline">¶</a></h2>
<p>This chapter contains API specifications which are relevant to BayesPy
developers and contributors.</p>
<div class="toctree-wrapper compound">
<span id="document-dev_api/nodes"></span><div class="section" id="developer-nodes">
<h3>Developer nodes<a class="headerlink" href="#developer-nodes" title="Permalink to this headline">¶</a></h3>
<p>The following base classes are useful if writing new nodes:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.node.Node" title="bayespy.inference.vmp.nodes.node.Node"><code class="xref py py-obj docutils literal"><span class="pre">node.Node</span></code></a>(*parents[,&nbsp;dims,&nbsp;plates,&nbsp;name,&nbsp;...])</td>
<td>Base class for all nodes.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.stochastic.Stochastic" title="bayespy.inference.vmp.nodes.stochastic.Stochastic"><code class="xref py py-obj docutils literal"><span class="pre">stochastic.Stochastic</span></code></a>(*args[,&nbsp;initialize,&nbsp;dims])</td>
<td>Base class for nodes that are stochastic.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamily" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamily"><code class="xref py py-obj docutils literal"><span class="pre">expfamily.ExponentialFamily</span></code></a>(*args,&nbsp;**kwargs)</td>
<td>A base class for nodes using natural parameterization <cite>phi</cite>.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.deterministic.Deterministic" title="bayespy.inference.vmp.nodes.deterministic.Deterministic"><code class="xref py py-obj docutils literal"><span class="pre">deterministic.Deterministic</span></code></a>(*args,&nbsp;**kwargs)</td>
<td>Base class for deterministic nodes.</td>
</tr>
</tbody>
</table>
<p>The following nodes are examples of special nodes that remain hidden for the
user although they are often implicitly used:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.constant.Constant" title="bayespy.inference.vmp.nodes.constant.Constant"><code class="xref py py-obj docutils literal"><span class="pre">constant.Constant</span></code></a>(moments,&nbsp;x,&nbsp;**kwargs)</td>
<td>Node for presenting constant values.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianToGaussianGamma" title="bayespy.inference.vmp.nodes.gaussian.GaussianToGaussianGamma"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianToGaussianGamma</span></code></a>(X,&nbsp;**kwargs)</td>
<td>Converter for Gaussian moments to Gaussian-gamma isotropic moments</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianGammaToGaussianWishart" title="bayespy.inference.vmp.nodes.gaussian.GaussianGammaToGaussianWishart"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianGammaToGaussianWishart</span></code></a>(...)</td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.WrapToGaussianGamma" title="bayespy.inference.vmp.nodes.gaussian.WrapToGaussianGamma"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.WrapToGaussianGamma</span></code></a>(X,&nbsp;alpha[,&nbsp;ndim])</td>
<td></td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.WrapToGaussianWishart" title="bayespy.inference.vmp.nodes.gaussian.WrapToGaussianWishart"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.WrapToGaussianWishart</span></code></a>(X,&nbsp;Lambda[,&nbsp;ndim])</td>
<td>Wraps Gaussian and Wishart nodes into a Gaussian-Wishart node.</td>
</tr>
</tbody>
</table>
</div>
<span id="document-dev_api/moments"></span><div class="section" id="moments">
<h3>Moments<a class="headerlink" href="#moments" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.node.Moments" title="bayespy.inference.vmp.nodes.node.Moments"><code class="xref py py-obj docutils literal"><span class="pre">node.Moments</span></code></a></td>
<td>Base class for defining the expectation of the sufficient statistics.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianMoments" title="bayespy.inference.vmp.nodes.gaussian.GaussianMoments"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianMoments</span></code></a>(shape)</td>
<td>Class for the moments of Gaussian variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian_markov_chain.GaussianMarkovChainMoments" title="bayespy.inference.vmp.nodes.gaussian_markov_chain.GaussianMarkovChainMoments"><code class="xref py py-obj docutils literal"><span class="pre">gaussian_markov_chain.GaussianMarkovChainMoments</span></code></a>(N,&nbsp;D)</td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianGammaMoments" title="bayespy.inference.vmp.nodes.gaussian.GaussianGammaMoments"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianGammaMoments</span></code></a>(shape)</td>
<td>Class for the moments of Gaussian-gamma-ISO variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianWishartMoments" title="bayespy.inference.vmp.nodes.gaussian.GaussianWishartMoments"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianWishartMoments</span></code></a>(shape)</td>
<td>Class for the moments of Gaussian-Wishart variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gamma.GammaMoments" title="bayespy.inference.vmp.nodes.gamma.GammaMoments"><code class="xref py py-obj docutils literal"><span class="pre">gamma.GammaMoments</span></code></a></td>
<td>Class for the moments of gamma variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.wishart.WishartMoments" title="bayespy.inference.vmp.nodes.wishart.WishartMoments"><code class="xref py py-obj docutils literal"><span class="pre">wishart.WishartMoments</span></code></a>(shape)</td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.beta.BetaMoments" title="bayespy.inference.vmp.nodes.beta.BetaMoments"><code class="xref py py-obj docutils literal"><span class="pre">beta.BetaMoments</span></code></a>()</td>
<td>Class for the moments of beta variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.dirichlet.DirichletMoments" title="bayespy.inference.vmp.nodes.dirichlet.DirichletMoments"><code class="xref py py-obj docutils literal"><span class="pre">dirichlet.DirichletMoments</span></code></a>(categories)</td>
<td>Class for the moments of Dirichlet variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.bernoulli.BernoulliMoments" title="bayespy.inference.vmp.nodes.bernoulli.BernoulliMoments"><code class="xref py py-obj docutils literal"><span class="pre">bernoulli.BernoulliMoments</span></code></a>()</td>
<td>Class for the moments of Bernoulli variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.binomial.BinomialMoments" title="bayespy.inference.vmp.nodes.binomial.BinomialMoments"><code class="xref py py-obj docutils literal"><span class="pre">binomial.BinomialMoments</span></code></a>(N)</td>
<td>Class for the moments of binomial variables</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.categorical.CategoricalMoments" title="bayespy.inference.vmp.nodes.categorical.CategoricalMoments"><code class="xref py py-obj docutils literal"><span class="pre">categorical.CategoricalMoments</span></code></a>(categories)</td>
<td>Class for the moments of categorical variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.categorical_markov_chain.CategoricalMarkovChainMoments" title="bayespy.inference.vmp.nodes.categorical_markov_chain.CategoricalMarkovChainMoments"><code class="xref py py-obj docutils literal"><span class="pre">categorical_markov_chain.CategoricalMarkovChainMoments</span></code></a>(...)</td>
<td>Class for the moments of categorical Markov chain variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.multinomial.MultinomialMoments" title="bayespy.inference.vmp.nodes.multinomial.MultinomialMoments"><code class="xref py py-obj docutils literal"><span class="pre">multinomial.MultinomialMoments</span></code></a>(categories)</td>
<td>Class for the moments of multinomial variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.poisson.PoissonMoments" title="bayespy.inference.vmp.nodes.poisson.PoissonMoments"><code class="xref py py-obj docutils literal"><span class="pre">poisson.PoissonMoments</span></code></a></td>
<td>Class for the moments of Poisson variables</td>
</tr>
</tbody>
</table>
</div>
<span id="document-dev_api/distributions"></span><div class="section" id="distributions">
<h3>Distributions<a class="headerlink" href="#distributions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.stochastic.Distribution" title="bayespy.inference.vmp.nodes.stochastic.Distribution"><code class="xref py py-obj docutils literal"><span class="pre">stochastic.Distribution</span></code></a></td>
<td>A base class for the VMP formulas of variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution" title="bayespy.inference.vmp.nodes.expfamily.ExponentialFamilyDistribution"><code class="xref py py-obj docutils literal"><span class="pre">expfamily.ExponentialFamilyDistribution</span></code></a></td>
<td>Sub-classes implement distribution specific computations.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianDistribution" title="bayespy.inference.vmp.nodes.gaussian.GaussianDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianDistribution</span></code></a>(shape)</td>
<td>Class for the VMP formulas of Gaussian variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianARDDistribution" title="bayespy.inference.vmp.nodes.gaussian.GaussianARDDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianARDDistribution</span></code></a>(shape)</td>
<td>...</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianGammaDistribution" title="bayespy.inference.vmp.nodes.gaussian.GaussianGammaDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianGammaDistribution</span></code></a>(shape)</td>
<td>Class for the VMP formulas of Gaussian-Gamma-ISO variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian.GaussianWishartDistribution" title="bayespy.inference.vmp.nodes.gaussian.GaussianWishartDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian.GaussianWishartDistribution</span></code></a></td>
<td>Class for the VMP formulas of Gaussian-Wishart variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian_markov_chain.GaussianMarkovChainDistribution" title="bayespy.inference.vmp.nodes.gaussian_markov_chain.GaussianMarkovChainDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian_markov_chain.GaussianMarkovChainDistribution</span></code></a>(N,&nbsp;D)</td>
<td>Implementation of VMP formulas for Gaussian Markov chain</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian_markov_chain.SwitchingGaussianMarkovChainDistribution" title="bayespy.inference.vmp.nodes.gaussian_markov_chain.SwitchingGaussianMarkovChainDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian_markov_chain.SwitchingGaussianMarkovChainDistribution</span></code></a>(N,&nbsp;D,&nbsp;K)</td>
<td>Sub-classes implement distribution specific computations.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gaussian_markov_chain.VaryingGaussianMarkovChainDistribution" title="bayespy.inference.vmp.nodes.gaussian_markov_chain.VaryingGaussianMarkovChainDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gaussian_markov_chain.VaryingGaussianMarkovChainDistribution</span></code></a>(N,&nbsp;D)</td>
<td>Sub-classes implement distribution specific computations.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.gamma.GammaDistribution" title="bayespy.inference.vmp.nodes.gamma.GammaDistribution"><code class="xref py py-obj docutils literal"><span class="pre">gamma.GammaDistribution</span></code></a></td>
<td>Class for the VMP formulas of gamma variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.wishart.WishartDistribution" title="bayespy.inference.vmp.nodes.wishart.WishartDistribution"><code class="xref py py-obj docutils literal"><span class="pre">wishart.WishartDistribution</span></code></a></td>
<td>Sub-classes implement distribution specific computations.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.beta.BetaDistribution" title="bayespy.inference.vmp.nodes.beta.BetaDistribution"><code class="xref py py-obj docutils literal"><span class="pre">beta.BetaDistribution</span></code></a></td>
<td>Class for the VMP formulas of beta variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.dirichlet.DirichletDistribution" title="bayespy.inference.vmp.nodes.dirichlet.DirichletDistribution"><code class="xref py py-obj docutils literal"><span class="pre">dirichlet.DirichletDistribution</span></code></a></td>
<td>Class for the VMP formulas of Dirichlet variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.bernoulli.BernoulliDistribution" title="bayespy.inference.vmp.nodes.bernoulli.BernoulliDistribution"><code class="xref py py-obj docutils literal"><span class="pre">bernoulli.BernoulliDistribution</span></code></a>()</td>
<td>Class for the VMP formulas of Bernoulli variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.binomial.BinomialDistribution" title="bayespy.inference.vmp.nodes.binomial.BinomialDistribution"><code class="xref py py-obj docutils literal"><span class="pre">binomial.BinomialDistribution</span></code></a>(N)</td>
<td>Class for the VMP formulas of binomial variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.categorical.CategoricalDistribution" title="bayespy.inference.vmp.nodes.categorical.CategoricalDistribution"><code class="xref py py-obj docutils literal"><span class="pre">categorical.CategoricalDistribution</span></code></a>(categories)</td>
<td>Class for the VMP formulas of categorical variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.categorical_markov_chain.CategoricalMarkovChainDistribution" title="bayespy.inference.vmp.nodes.categorical_markov_chain.CategoricalMarkovChainDistribution"><code class="xref py py-obj docutils literal"><span class="pre">categorical_markov_chain.CategoricalMarkovChainDistribution</span></code></a>(...)</td>
<td>Class for the VMP formulas of categorical Markov chain variables.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.multinomial.MultinomialDistribution" title="bayespy.inference.vmp.nodes.multinomial.MultinomialDistribution"><code class="xref py py-obj docutils literal"><span class="pre">multinomial.MultinomialDistribution</span></code></a>(trials)</td>
<td>Class for the VMP formulas of multinomial variables.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#bayespy.inference.vmp.nodes.poisson.PoissonDistribution" title="bayespy.inference.vmp.nodes.poisson.PoissonDistribution"><code class="xref py py-obj docutils literal"><span class="pre">poisson.PoissonDistribution</span></code></a></td>
<td>Class for the VMP formulas of Poisson variables.</td>
</tr>
</tbody>
</table>
</div>
<span id="document-dev_api/utils"></span><div class="section" id="utility-functions">
<h3>Utility functions<a class="headerlink" href="#utility-functions" title="Permalink to this headline">¶</a></h3>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="index.html#module-bayespy.utils.linalg" title="bayespy.utils.linalg"><code class="xref py py-obj docutils literal"><span class="pre">linalg</span></code></a></td>
<td>General numerical functions and methods.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#module-bayespy.utils.random" title="bayespy.utils.random"><code class="xref py py-obj docutils literal"><span class="pre">random</span></code></a></td>
<td>General functions random sampling and distributions.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="index.html#module-bayespy.utils.optimize" title="bayespy.utils.optimize"><code class="xref py py-obj docutils literal"><span class="pre">optimize</span></code></a></td>
<td></td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="index.html#module-bayespy.utils.misc" title="bayespy.utils.misc"><code class="xref py py-obj docutils literal"><span class="pre">misc</span></code></a></td>
<td>General numerical functions and methods.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<span id="document-references"></span><p class="rubric">Bibliography</p>
<p id="bibtex-bibliography-references-0"><table class="docutils citation" frame="void" id="hensman-2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>James Hensman, Magnus Rattray, and Neil&nbsp;D. Lawrence. Fast variational inference in the conjugate exponential family. In F.&nbsp;Pereira, C.J.C. Burges, L.&nbsp;Bottou, and K.Q. Weinberger, editors, <em>Advances in Neural Information Processing Systems 25</em>, 2888–2896. Lake Tahoe, Nevada, USA, 2012.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hoffman-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>Matthew&nbsp;D. Hoffman, David&nbsp;M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. <em>Journal of Machine Learning Research</em>, 14:1303–47, 2013.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="honkela-2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[3]</td><td>Antti Honkela, Tapani Raiko, Mikael Kuusela, Matti Tornio, and Juha Karhunen. Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes. <em>Journal of Machine Learning Research</em>, 11:3235–3268, 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="honkela-2003" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[4]</td><td>Antti Honkela, Harri Valpola, and Juha Karhunen. Accelerating cyclic update algorithms for parameter estimation by pattern searches. <em>Neural Processing Letters</em>, 17(2):191–203, 2003. <a class="reference external" href="https://doi.org/10.1023/A:1023655202546">doi:10.1023/A:1023655202546</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="katahira-2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[5]</td><td>K.&nbsp;Katahira, K.&nbsp;Watanabe, and M.&nbsp;Okada. Deterministic annealing variant of variational Bayes method. <em>Journal of Physics: Conference Series</em>, 2008.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="luttinen-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td>Jaakko Luttinen. Fast variational Bayesian linear state-space model. In Hendrik Blockeel, Kristian Kersting, Siegfried Nijssen, and Filip Železný, editors, <em>Machine Learning and Knowledge Discovery in Databases</em>, volume 8188 of Lecture Notes in Computer Science, pages 305–320. Springer, 2013. <a class="reference external" href="https://doi.org/10.1007/978-3-642-40988-2_20">doi:10.1007/978-3-642-40988-2_20</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="luttinen-2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[7]</td><td>Jaakko Luttinen and Alexander Ilin. Transformations in variational Bayesian factor analysis to speed up learning. <em>Neurocomputing</em>, 73:1093–1102, 2010. <a class="reference external" href="https://doi.org/10.1016/j.neucom.2009.11.018">doi:10.1016/j.neucom.2009.11.018</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="luttinen-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[8]</td><td>Jaakko Luttinen, Tapani Raiko, and Alexander Ilin. Linear state-space model with time-varying dynamics. In Toon Calders, Floriana Esposito, Eyke Hüllermeier, and Rosa Meo, editors, <em>Machine Learning and Knowledge Discovery in Databases</em>, volume 8725 of Lecture Notes in Computer Science, pages 338–353. Springer, 2014. <a class="reference external" href="https://doi.org/10.1007/978-3-662-44851-9_22">doi:10.1007/978-3-662-44851-9_22</a>.</td></tr>
</tbody>
</table>
</p>
</div>
<ul class="simple">
<li><a class="reference internal" href="index.html#document-references"><span class="doc">Bibliography</span></a></li>
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2011-2017, Jaakko Luttinen and contributors.
      
        <span class="commit">
          Revision <code>1d2e2c2f</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      v: latest
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Versions</dt>
        
          <dd><a href="/en/latest/">latest</a></dd>
        
          <dd><a href="/en/stable/">stable</a></dd>
        
      </dl>
      <dl>
        <dt>Downloads</dt>
        
          <dd><a href="//readthedocs.org/projects/bayespy/downloads/htmlzip/latest/">htmlzip</a></dd>
        
      </dl>
      <dl>
        <dt>On Read the Docs</dt>
          <dd>
            <a href="//readthedocs.org/projects/bayespy/?fromdocs=bayespy">Project Home</a>
          </dd>
          <dd>
            <a href="//readthedocs.org/builds/bayespy/?fromdocs=bayespy">Builds</a>
          </dd>
      </dl>
      <hr/>
      Free document hosting provided by <a href="http://www.readthedocs.org">Read the Docs</a>.

    </div>
  </div>



  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.5.10+6.g1d2e2c2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>